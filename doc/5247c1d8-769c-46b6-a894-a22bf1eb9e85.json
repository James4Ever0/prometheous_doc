{
    "summary": "The code initializes an OpenAI language model, defines functions for executing chain and generating text, prints initialization parameters and query for debugging, and sets up LLM instance with optional temperature and GPT-4 parameters.",
    "details": [
        {
            "comment": "The code defines a class named \"LLM\" which initializes a Language Model Chain using the OpenAI API. It takes a prompt, temperature, and gpt_4 parameters. The maximum number of tokens is set depending on whether GPT-4 or Text-Davinci-003 is used. The show_init_config method displays the initialization configuration.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/llm.py\":0-35",
            "content": "# from langchain.prompts import Prompt\n# from langchain.chains import LLMChain\nfrom contextlib import contextmanager\nfrom langchain.llms import OpenAI\nimport tiktoken\ndef print_center(banner: str):\n    print(banner.center(50, \"=\"))\nclass LLM:\n    \"\"\"\n    A class for running a Language Model Chain.\n    \"\"\"\n    def __init__(self, prompt: str, temperature=0, gpt_4=False):\n        \"\"\"\n        Initializes the LLM class.\n        Args:\n            prompt (PromptTemplate): The prompt template to use.\n            temperature (int): The temperature to use for the model.\n            gpt_4 (bool): Whether to use GPT-4 or Text-Davinci-003.\n        Side Effects:\n            Sets the class attributes.\n        \"\"\"\n        self.prompt = prompt\n        self.prompt_size = self.number_of_tokens(prompt)\n        self.temperature = temperature\n        self.gpt_4 = gpt_4\n        self.model_name = \"gpt-4\" if self.gpt_4 else \"text-davinci-003\"\n        self.max_tokens = 4097 * 2 if self.gpt_4 else 4097\n        self.show_init_config()\n    def show_init_config(self):"
        },
        {
            "comment": "This code initializes an OpenAI language model with given parameters and defines a function `run` to execute the chain. The function takes a query as input, combines it with a prompt, and returns the generated text. The code also prints initialization parameters and the query for debugging purposes.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/llm.py\":36-64",
            "content": "        print_center(\"init params\")\n        print(f\"Model: {self.model_name}\")\n        print(f\"Max Tokens: {self.max_tokens}\")\n        print(f\"Prompt Size: {self.prompt_size}\")\n        print(f\"Temperature: {self.temperature}\")\n        print_center(\"init config\")\n        print(self.prompt)\n    def run(self, query):\n        \"\"\"\n        Runs the Language Model Chain.\n        Args:\n            code (str): The code to use for the chain.\n            **kwargs (dict): Additional keyword arguments.\n        Returns:\n            str: The generated text.\n        \"\"\"\n        llm = OpenAI(\n            temperature=self.temperature,\n            max_tokens=-1,\n            model_name=self.model_name,\n            disallowed_special=(),  # to suppress error when special tokens within the input text (encode special tokens as normal text)\n        )\n        # chain = LLMChain(llm=llm, prompt=self.prompt)\n        chunk_list = []\n        print_center(\"query\")\n        print(query)\n        print_center(\"response\")\n        _input = \"\\n\".join([self.prompt, query])"
        },
        {
            "comment": "The code defines a function `llm` that streams the output of an LLM model while counting tokens, and a `number_of_tokens` method. The `llm_context` context manager sets up an LLM instance with optional temperature and GPT-4 parameters.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/llm.py\":65-91",
            "content": "        for chunk in llm.stream(input=_input):\n            print(chunk, end=\"\", flush=True)\n            chunk_list.append(chunk)\n        print()\n        result = \"\".join(chunk_list)\n        return result\n    def number_of_tokens(self, text):\n        \"\"\"\n        Counts the number of tokens in a given text.\n        Args:\n            text (str): The text to count tokens for.\n        Returns:\n            int: The number of tokens in the text.\n        \"\"\"\n        encoding = tiktoken.encoding_for_model(\"gpt-4\")\n        return len(encoding.encode(text, disallowed_special=()))\n@contextmanager\ndef llm_context(prompt: str, temperature=0, gpt_4=False):\n    model = LLM(prompt, temperature=temperature, gpt_4=gpt_4)\n    try:\n        yield model\n    finally:\n        del model"
        }
    ]
}