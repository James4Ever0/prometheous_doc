{
    "summary": "This code initializes indexing and caching components for documents, sets up metadata for vector cache using HnswDocumentIndex and OllamaEmbeddings, updates comment index, searches comments/code with query and embedding, filters results, generates Markdown response, and repeats until interrupted.",
    "details": [
        {
            "comment": "This code sets up the necessary components for indexing and caching documents. It uses a RecursiveCharacterTextSplitter to split text into chunks, an argument parser to handle input directories, and defines FileSummary and FolderSummary classes for storing file and folder information. The code also imports modules from langchain, pydantic, and tinydb libraries.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py\":0-43",
            "content": "# responsible for turning everything into embeddings.\n# responsible for caching, index wirings.\n# code chunk with title, chunk description with title\n# code document chunks, folder document chunks\n# latest code chunks -> embeddings\n# folder/file summary hash -> latest document chunks -> document chunk hash -> embeddings\nimport os\nimport argparse\nos.environ[\"OPENAI_API_KEY\"] = \"any\"\nos.environ[\"OPENAI_API_BASE\"] = \"http://0.0.0.0:8000\"\nos.environ[\"BETTER_EXCEPTIONS\"] = \"1\"\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom pydantic import BaseModel\nfrom tinydb import TinyDB\nclass FileSummary(BaseModel):\n    file_hash: str\n    summary: str\nclass FolderSummary(BaseModel):  # this is not generated. do it now.\n    folder_hash: str\n    summary: str\ntextSpliter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=100,\n)\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-s\", \"--source_dir\", type=str, required=True)\nargs = parser.parse_args()\n# the only parameter.\nsource_dir = args.source_dir"
        },
        {
            "comment": "This code sets up directories, databases and loads metadata for vector cache initialization. It ensures required directories exist before creating them, loads file mapping from 'metadata.json' and split count, then loads project name and title split count from 'metadata_title.json'. Finally, it initializes empty dictionaries for data and title_data for later processing.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py\":45-81",
            "content": "assert os.path.exists(source_dir)\nassert os.path.isdir(source_dir)\nassert os.path.isabs(source_dir)\nimport json\nimport sys\nsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"../\"))\nfrom llm import llm_context\ncache_path_base = os.path.join(source_dir, \"vector_cache\")\ndocument_cache_path_bash = os.path.join(cache_path_base, \"document\")\nif not os.path.exists(cache_path_base):\n    os.mkdir(cache_path_base)\nif not os.path.exists(document_cache_path_bash):\n    os.mkdir(document_cache_path_bash)\nfolder_summary_db = TinyDB(os.path.join(cache_path_base, \"folder_summaries.json\"))\nmetadata = json.loads(open(os.path.join(source_dir, \"metadata.json\"), \"r\").read())\ntitle_metadata = json.loads(\n    open(os.path.join(source_dir, \"metadata_title.json\"), \"r\").read()\n)\nfile_mapping = metadata[\"file_mapping\"]\nsplit_count = metadata[\"split_count\"]\nproject_name = metadata[\"project_name\"]\ntitle_split_count = title_metadata[\"split_count\"]\ndata = {}\ntitle_data = {}\n# file_title_indices = []\nfile_summary_indices = [v[\"entry_id\"] + 1 for v in file_mapping.values()]"
        },
        {
            "comment": "The code reads JSON files and updates data dictionaries for code and title information. It strips quotes and calculates file hashes using MD5. The code then creates a list of code content, comments, and their locations.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py\":83-127",
            "content": "for i in range(split_count):\n    new_data = json.loads(open(os.path.join(source_dir, f\"data/{i}.json\"), \"r\").read())\n    data.update(new_data)\nfor i in range(title_split_count):\n    new_data = json.loads(\n        open(os.path.join(source_dir, f\"data/titles/{i}.json\"), \"r\").read()\n    )\n    title_data.update(new_data)\n# breakpoint()\ndef strip_quote(s: str):\n    if s[0] == s[-1]:\n        if s[0] in ['\"', \"'\"]:\n            return s[1:-1].strip()\n    return s.strip().strip(\".\")\nfile_summary_dict = {}\nfile_hash_dict = {}\nfile_chunk_comment_dict = {}\nfile_chunk_code_dict = {}\nimport hashlib\ndef hash_doc(enc: str):\n    hash_object = hashlib.md5(enc.encode())\n    return hash_object.hexdigest()\ncode_and_comment_list = []\nfor k, v in data.items():\n    if v[\"type\"] == \"code\":\n        code_elem = v\n        comment_elem = data[str(int(k) + 1)]\n        code_content = v[\"content\"]\n        comment_content = comment_elem[\"content\"]\n        location = v[\"location\"]\n        code_and_comment_list.append(\n            dict(code=code_content, comment=comment_content, location=location)"
        },
        {
            "comment": "Code snippet contains the implementation of a document index using HnswDocumentIndex for CodeCommentChunk, FileDocumentChunk, and FolderDocumentChunk. It imports necessary libraries, creates instances of HnswDocumentIndex for each type of document chunk, and sets the work directory paths. The code also initializes OllamaEmbeddings to extract embeddings from textual data and uses Python implementation for Protocol Buffers.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py\":128-173",
            "content": "        )\n        # chunk_hash = hash_doc(code_content)\nos.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\nfrom langchain.embeddings import OllamaEmbeddings\nollama_emb = OllamaEmbeddings(\n    model=\"openhermes2.5-mistral:latest\",\n    # model=\"llama:7b\",\n)\nfrom docarray import BaseDoc\nfrom docarray.index import HnswDocumentIndex  # type: ignore\nimport numpy as np\nfrom docarray.typing import NdArray\nclass CodeCommentChunk(BaseDoc):\n    code: str\n    comment: str\n    location: str\n    chunk_hash: str\n    embedding: NdArray[4096]  # type:ignore\nclass FileDocumentChunk(BaseDoc):\n    file_hash: str\n    chunk: str\n    embedding: NdArray[4096]  # type:ignore\nclass FolderDocumentChunk(BaseDoc):\n    folder_hash: str\n    chunk: str\n    embedding: NdArray[4096]  # type:ignore\n# create a Document Index\ncomment_index = HnswDocumentIndex[CodeCommentChunk](\n    work_dir=os.path.join(cache_path_base, \"comment\")\n)\nfile_document_index = HnswDocumentIndex[FileDocumentChunk](\n    work_dir=os.path.join(document_cache_path_bash, \"file\")"
        },
        {
            "comment": "This code snippet creates a `FolderDocumentChunk` instance from the `HnswDocumentIndex` class, using a specified work directory path. It then iterates over each item in the `code_and_comment_list` progress bar and performs the following operations: \n1. Computes the chunk hash of the code snippet.\n2. Retrieves corresponding location and doc ID from the database if the chunk hash is found.\n3. If a match is found, the document is cached and the iteration continues to the next item.\n4. If no match is found, it generates an embed representation for the code and comment combination.\n5. Creates a `CodeCommentChunk` instance with necessary information and embedding vector.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py\":174-211",
            "content": ")\nfolder_document_index = HnswDocumentIndex[FolderDocumentChunk](\n    work_dir=os.path.join(document_cache_path_bash, \"folder\")\n)\ncomment_index_ids = []\nimport progressbar\nfor it in progressbar.progressbar(code_and_comment_list, prefix=\"code and comments:\"):\n    chunk_hash = hash_doc(it[\"code\"])\n    comment_index._sqlite_cursor.execute(\n        \"SELECT location, doc_id FROM docs WHERE chunk_hash = ?\", (chunk_hash,)\n    )\n    rows = comment_index._sqlite_cursor.fetchall()\n    if len(rows) > 0:\n        cached = False\n        for row in rows:\n            if row[0] == it[\"location\"]:\n                cached = True\n                doc_id = row[1]\n                comment_index_ids.append(doc_id)\n                break\n        if cached:\n            print(\"document cached:\", str(it)[:50]+ '...}')\n            continue\n    code_and_comment = f\"\"\"Code:\n{it['code']}\nComment:\n{it['comment']}\n\"\"\"\n    embed = np.array(ollama_emb.embed_query(code_and_comment))\n    docObject = CodeCommentChunk(\n        **it, chunk_hash=chunk_hash, embedding=embed"
        },
        {
            "comment": "This code segment is updating the comment index with a new document and retrieving hashed IDs for documents containing the text \"hello\" from a SQLite database. It then calls a function called \"print_and_return\" passing it a string content, which returns the same string with an additional newline character appended. The code is part of a Python program likely working on a specific project and its related context.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py\":212-235",
            "content": "    )  # type:ignore\n    doc_id = comment_index._to_hashed_id(docObject.id)\n    comment_index_ids.append(doc_id)\n    comment_index.index(docObject)\n# comment_index._sqlite_cursor.execute(\"SELECT doc_id FROM docs WHERE text LIKE 'hello%'\")\n# rows = comment_index._sqlite_cursor.fetchall()\n# # print(rows)\n# hashed_ids = set(it[0] for it in rows)\n# # hashed_ids = set(str(it[0]) for it in rows)\n# print(hashed_ids)\ndef print_and_return(content: str):\n    return content + \"\\n\"\nif __name__ == \"__main__\":\n    # query for code & embedding index\n    init_prompt = \"\"\"You are a helpful assistant who can answer questions based on relevant context about a specific code project. Please answer the user query according to the context.\nAssume the reader does not know anything about how the project is strucuted or which folders/files are provided in the context.\nDo not reference the context in your answer. Instead use the context to inform your answer.\nIf you don't know the answer, just say \"Hmm, I'm not sure.\" Don't try to make up an answer."
        },
        {
            "comment": "This code repeatedly takes user input for a query, searches the comment index using both the query and its embedding, filters results to three documents, prints their details, and repeats until interrupted.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py\":236-260",
            "content": "Your answer should be at least 100 words and no more than 300 words.\nDo not include information that is not directly relevant to the question, even if the context includes it.\n\"\"\"\n    while True:\n        query = input(\"query for code & embedding index:\\n\")\n        ans = comment_index._search_and_filter(\n            np.array(ollama_emb.embed_query(query)).reshape(1, -1),\n            limit=3,\n            # limit=10,\n            search_field=\"embedding\",\n            hashed_ids=set(comment_index_ids),\n        )\n        print(\"ans:\", ans)\n        context = \"\"\n        for it in ans.documents[0]:\n            location = it.location\n            file_location = location.split(\":\")[0]\n            context += print_and_return(\"-\" * 10)\n            context += print_and_return(\"Location:\")\n            context += print_and_return(location)\n            # context += print_and_return(\"Title:\")\n            # context += print_and_return(title_data.get(location, title_data[file_location]))\n            context += print_and_return(\"Comment:\")"
        },
        {
            "comment": "This code retrieves the comment and code from an iterator (it) and appends them to the context. It then utilizes a language model (llm_context) with a specific prompt, temperature value of 0.2, and runs it on the constructed context and user query, asking for a response in Markdown format.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py\":261-268",
            "content": "            context += print_and_return(it.comment)\n            context += print_and_return(\"Code:\")\n            context += print_and_return(it.code)\n        with llm_context(init_prompt, temperature=0.2) as model:\n            model.run(\n                f\"Context:\\n{context}\\nUser query: {query}\\nRespond in Markdown format:\\n\"\n            )"
        }
    ]
}