{
    "summary": "The code initializes a language model chain with provided parameters and includes functions for running the chain, displaying initialization configuration, printing query and response information. It iterates through LLM model chunks, prints them, stores in a list, joins into a string, counts tokens, and handles LLM model instance within a context manager.",
    "details": [
        {
            "comment": "The code defines a class named \"LLM\" for running Language Model Chains. The class initializes with a prompt, temperature, and gpt_4 parameters. It sets class attributes such as the prompt size, model name (gpt-4 or text-davinci-003), and maximum tokens based on these inputs. It also displays initialization configuration.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/llm.py\":0-34",
            "content": "# from langchain.prompts import Prompt\n# from langchain.chains import LLMChain\nfrom contextlib import contextmanager\nfrom langchain.llms import OpenAI\nimport tiktoken\ndef print_center(banner: str):\n    print(banner.center(50, \"=\"))\nclass LLM:\n    \"\"\"\n    A class for running a Language Model Chain.\n    \"\"\"\n    def __init__(self, prompt: str, temperature=0, gpt_4=False):\n        \"\"\"\n        Initializes the LLM class.\n        Args:\n            prompt (PromptTemplate): The prompt template to use.\n            temperature (int): The temperature to use for the model.\n            gpt_4 (bool): Whether to use GPT-4 or Text-Davinci-003.\n        Side Effects:\n            Sets the class attributes.\n        \"\"\"\n        self.prompt = prompt\n        self.prompt_size = self.number_of_tokens(prompt)\n        self.temperature = temperature\n        self.gpt_4 = gpt_4\n        self.model_name = \"gpt-4\" if self.gpt_4 else \"text-davinci-003\"\n        self.max_tokens = 4097 * 2 if self.gpt_4 else 4097\n        self.show_init_config()\n    def show_init_config(self):"
        },
        {
            "comment": "This code initializes a language model chain and provides the necessary parameters for running it. It also includes a function to run the chain with a given query, and prints relevant information such as query and response.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/llm.py\":35-63",
            "content": "        print_center(\"init params\")\n        print(f\"Model: {self.model_name}\")\n        print(f\"Max Tokens: {self.max_tokens}\")\n        print(f\"Prompt Size: {self.prompt_size}\")\n        print(f\"Temperature: {self.temperature}\")\n        print_center(\"init config\")\n        print(self.prompt)\n    def run(self, query):\n        \"\"\"\n        Runs the Language Model Chain.\n        Args:\n            code (str): The code to use for the chain.\n            **kwargs (dict): Additional keyword arguments.\n        Returns:\n            str: The generated text.\n        \"\"\"\n        llm = OpenAI(\n            temperature=self.temperature,\n            max_tokens=-1,\n            model_name=self.model_name,\n            disallowed_special=(),  # to suppress error when special tokens within the input text (encode special tokens as normal text)\n        )\n        # chain = LLMChain(llm=llm, prompt=self.prompt)\n        chunk_list = []\n        print_center(\"query\")\n        print(query)\n        print_center(\"response\")\n        _input = \"\\n\".join([self.prompt, query])"
        },
        {
            "comment": "Code iterates through chunks of text from the LLM model, prints each chunk, and stores them in a list. After all chunks are printed, it joins them into a single string and returns it. The code also defines two functions: \"number_of_tokens\" for counting tokens in a given text and \"llm_context\" as a context manager to handle the LLM model instance.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/llm.py\":64-90",
            "content": "        for chunk in llm.stream(input=_input):\n            print(chunk, end=\"\", flush=True)\n            chunk_list.append(chunk)\n        print()\n        result = \"\".join(chunk_list)\n        return result\n    def number_of_tokens(self, text):\n        \"\"\"\n        Counts the number of tokens in a given text.\n        Args:\n            text (str): The text to count tokens for.\n        Returns:\n            int: The number of tokens in the text.\n        \"\"\"\n        encoding = tiktoken.encoding_for_model(\"gpt-4\")\n        return len(encoding.encode(text, disallowed_special=()))\n@contextmanager\ndef llm_context(prompt: str, temperature=0, gpt_4=False):\n    model = LLM(prompt, temperature=temperature, gpt_4=gpt_4)\n    try:\n        yield model\n    finally:\n        del model"
        }
    ]
}