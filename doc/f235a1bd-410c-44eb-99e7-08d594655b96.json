{
    "summary": "This code initializes a document index, checks for hash matches, embeds documents if needed, and uses embeddings to search comments. It retrieves relevant results based on user queries and constructs context variable for response generation using llm_context function.",
    "details": [
        {
            "comment": "This code initializes a RecursiveCharacterTextSplitter with specified chunk size and overlap, parses arguments from the command line using argparse, and sets source_dir as required input. This code is responsible for turning everything into embeddings by splitting text into chunks and generating file and folder summaries. It also handles caching, index wiring, and stores document chunks in folders and files.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py\":0-43",
            "content": "# responsible for turning everything into embeddings.\n# responsible for caching, index wirings.\n# code chunk with title, chunk description with title\n# code document chunks, folder document chunks\n# latest code chunks -> embeddings\n# folder/file summary hash -> latest document chunks -> document chunk hash -> embeddings\nimport os\nimport argparse\nos.environ[\"OPENAI_API_KEY\"] = \"any\"\nos.environ[\"OPENAI_API_BASE\"] = \"http://0.0.0.0:8000\"\nos.environ[\"BETTER_EXCEPTIONS\"] = \"1\"\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom pydantic import BaseModel\nfrom tinydb import TinyDB\nclass FileSummary(BaseModel):\n    file_hash: str\n    summary: str\nclass FolderSummary(BaseModel):  # this is not generated. do it now.\n    folder_hash: str\n    summary: str\ntextSpliter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=100,\n)\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-s\", \"--source_dir\", type=str, required=True)\nargs = parser.parse_args()\n# the only parameter.\nsource_dir = args.source_dir"
        },
        {
            "comment": "This code sets up the necessary paths and databases for vector indexing. It checks if directories exist, creates them if not, loads metadata from files, and initializes file and title data dictionaries. The file summary indices are populated while potentially skipping a title-based approach.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py\":45-81",
            "content": "assert os.path.exists(source_dir)\nassert os.path.isdir(source_dir)\nassert os.path.isabs(source_dir)\nimport json\nimport sys\nsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"../\"))\nfrom llm import llm_context\ncache_path_base = os.path.join(source_dir, \"vector_cache\")\ndocument_cache_path_bash = os.path.join(cache_path_base, \"document\")\nif not os.path.exists(cache_path_base):\n    os.mkdir(cache_path_base)\nif not os.path.exists(document_cache_path_bash):\n    os.mkdir(document_cache_path_bash)\nfolder_summary_db = TinyDB(os.path.join(cache_path_base, \"folder_summaries.json\"))\nmetadata = json.loads(open(os.path.join(source_dir, \"metadata.json\"), \"r\").read())\ntitle_metadata = json.loads(\n    open(os.path.join(source_dir, \"metadata_title.json\"), \"r\").read()\n)\nfile_mapping = metadata[\"file_mapping\"]\nsplit_count = metadata[\"split_count\"]\nproject_name = metadata[\"project_name\"]\ntitle_split_count = title_metadata[\"split_count\"]\ndata = {}\ntitle_data = {}\n# file_title_indices = []\nfile_summary_indices = [v[\"entry_id\"] + 1 for v in file_mapping.values()]"
        },
        {
            "comment": "The code reads JSON files, updates data and title_data dictionaries accordingly, and defines a function to strip quotes from strings. It also calculates hash values for documents, and initializes empty dictionaries for file summaries, hashes, chunk comments, and code. Then it iterates through the data dictionary, appending code elements along with their corresponding comments and locations into a list called 'code_and_comment_list'.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py\":83-127",
            "content": "for i in range(split_count):\n    new_data = json.loads(open(os.path.join(source_dir, f\"data/{i}.json\"), \"r\").read())\n    data.update(new_data)\nfor i in range(title_split_count):\n    new_data = json.loads(\n        open(os.path.join(source_dir, f\"data/titles/{i}.json\"), \"r\").read()\n    )\n    title_data.update(new_data)\n# breakpoint()\ndef strip_quote(s: str):\n    if s[0] == s[-1]:\n        if s[0] in ['\"', \"'\"]:\n            return s[1:-1].strip()\n    return s.strip().strip(\".\")\nfile_summary_dict = {}\nfile_hash_dict = {}\nfile_chunk_comment_dict = {}\nfile_chunk_code_dict = {}\nimport hashlib\ndef hash_doc(enc: str):\n    hash_object = hashlib.md5(enc.encode())\n    return hash_object.hexdigest()\ncode_and_comment_list = []\nfor k, v in data.items():\n    if v[\"type\"] == \"code\":\n        code_elem = v\n        comment_elem = data[str(int(k) + 1)]\n        code_content = v[\"content\"]\n        comment_content = comment_elem[\"content\"]\n        location = v[\"location\"]\n        code_and_comment_list.append(\n            dict(code=code_content, comment=comment_content, location=location)"
        },
        {
            "comment": "This code is creating a Document Index using HnswDocumentIndex for CodeCommentChunk, FileDocumentChunk, and FolderDocumentChunk types. It uses OllamaEmbeddings with \"openhermes2.5-mistral:latest\" model to get the document embeddings. The indexes are saved in respective directories under cache_path_base and document_cache_path_bash for future retrieval and search operations.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py\":128-173",
            "content": "        )\n        # chunk_hash = hash_doc(code_content)\nos.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\nfrom langchain.embeddings import OllamaEmbeddings\nollama_emb = OllamaEmbeddings(\n    model=\"openhermes2.5-mistral:latest\",\n    # model=\"llama:7b\",\n)\nfrom docarray import BaseDoc\nfrom docarray.index import HnswDocumentIndex  # type: ignore\nimport numpy as np\nfrom docarray.typing import NdArray\nclass CodeCommentChunk(BaseDoc):\n    code: str\n    comment: str\n    location: str\n    chunk_hash: str\n    embedding: NdArray[4096]  # type:ignore\nclass FileDocumentChunk(BaseDoc):\n    file_hash: str\n    chunk: str\n    embedding: NdArray[4096]  # type:ignore\nclass FolderDocumentChunk(BaseDoc):\n    folder_hash: str\n    chunk: str\n    embedding: NdArray[4096]  # type:ignore\n# create a Document Index\ncomment_index = HnswDocumentIndex[CodeCommentChunk](\n    work_dir=os.path.join(cache_path_base, \"comment\")\n)\nfile_document_index = HnswDocumentIndex[FileDocumentChunk](\n    work_dir=os.path.join(document_cache_path_bash, \"file\")"
        },
        {
            "comment": "This code chunk is initializing a folder document index using the HnswDocumentIndex class and connecting to a SQLite database. It then iterates through a list of code and comment pairs, checking if each document's hash exists in the database. If it does, the corresponding document ID is stored and the document is considered cached. Otherwise, it embeds the code and comment pair using the Ollama embedder.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py\":174-211",
            "content": ")\nfolder_document_index = HnswDocumentIndex[FolderDocumentChunk](\n    work_dir=os.path.join(document_cache_path_bash, \"folder\")\n)\ncomment_index_ids = []\nimport progressbar, random, time\nfor it in progressbar.progressbar(code_and_comment_list, prefix=\"code and comments:\"):\n    chunk_hash = hash_doc(it[\"code\"])\n    comment_index._sqlite_cursor.execute(\n        \"SELECT location, doc_id FROM docs WHERE chunk_hash = ?\", (chunk_hash,)\n    )\n    rows = comment_index._sqlite_cursor.fetchall()\n    if len(rows) > 0:\n        cached = False\n        for row in rows:\n            if row[0] == it[\"location\"]:\n                cached = True\n                doc_id = row[1]\n                comment_index_ids.append(doc_id)\n                break\n        if cached:\n            print(\"document cached:\", str(it)[:50] + \"...}\")\n            continue\n    code_and_comment = f\"\"\"Code:\n{it['code']}\nComment:\n{it['comment']}\n\"\"\"\n    while True:\n        embed_list = ollama_emb.embed_query(code_and_comment)\n        if embed_list is not None:\n            break"
        },
        {
            "comment": "The code is initializing a print statement with the text \"waiting for ollama service to be idle\" and then entering a sleep state for a random duration. It then converts a list of embeddings into a numpy array and creates a CodeCommentChunk object, assigning it an ID and embedding value. The function appends the hashed ID to the comment_index_ids list and indexes the CodeCommentChunk object in the comment_index. Finally, it prints the rows of documents where the text column matches 'hello%' from the SQLite cursor and returns a content string for printing and returning.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py\":212-243",
            "content": "        else:\n            print(\"waiting for ollama service to be idle\")\n            time.sleep(random.random())\n    embed = np.array(embed_list)  # it returns None. how comes?\n    # during LLM completion api?\n    # print(code_and_comment)\n    # print(embed.shape)\n    # breakpoint()\n    docObject = CodeCommentChunk(\n        **it, chunk_hash=chunk_hash, embedding=embed\n    )  # type:ignore\n    doc_id = comment_index._to_hashed_id(docObject.id)\n    comment_index_ids.append(doc_id)\n    comment_index.index(docObject)\n# comment_index._sqlite_cursor.execute(\"SELECT doc_id FROM docs WHERE text LIKE 'hello%'\")\n# rows = comment_index._sqlite_cursor.fetchall()\n# # print(rows)\n# hashed_ids = set(it[0] for it in rows)\n# # hashed_ids = set(str(it[0]) for it in rows)\n# print(hashed_ids)\ndef print_and_return(content: str):\n    return content + \"\\n\"\nif __name__ == \"__main__\":\n    # query for code & embedding index\n    init_prompt = \"\"\"You are a helpful assistant who can answer questions based on relevant context about a specific code project. Please answer the user query according to the context."
        },
        {
            "comment": "This code appears to be a part of an interactive application that allows the user to input queries and retrieve relevant documents based on their similarity in an embedding index. The embedded query is used to search for matches in the comment index, with a limit of 3 results (originally set to 10). The code then prints the answers and context by accessing the location information from the returned documents.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py\":244-266",
            "content": "Assume the reader does not know anything about how the project is strucuted or which folders/files are provided in the context.\nDo not reference the context in your answer. Instead use the context to inform your answer.\nIf you don't know the answer, just say \"Hmm, I'm not sure.\" Don't try to make up an answer.\nYour answer should be at least 100 words and no more than 300 words.\nDo not include information that is not directly relevant to the question, even if the context includes it.\n\"\"\"\n    while True:\n        query = input(\"query for code & embedding index:\\n\")\n        ans = comment_index._search_and_filter(\n            np.array(ollama_emb.embed_query(query)).reshape(1, -1),\n            limit=3,\n            # limit=10,\n            search_field=\"embedding\",\n            hashed_ids=set(comment_index_ids),\n        )\n        print(\"ans:\", ans)\n        context = \"\"\n        for it in ans.documents[0]:\n            location = it.location\n            file_location = location.split(\":\")[0]\n            context += print_and_return(\"-\" * 10)"
        },
        {
            "comment": "This code constructs a context variable by appending different prompts and their corresponding data, such as location, title, comment, and code. Then it uses the llm_context function with an init_prompt and a temperature of 0.2 to generate a response in Markdown format based on the constructed context and a user query.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py\":267-279",
            "content": "            context += print_and_return(\"Location:\")\n            context += print_and_return(location)\n            # context += print_and_return(\"Title:\")\n            # context += print_and_return(title_data.get(location, title_data[file_location]))\n            context += print_and_return(\"Comment:\")\n            context += print_and_return(it.comment)\n            context += print_and_return(\"Code:\")\n            context += print_and_return(it.code)\n        with llm_context(init_prompt, temperature=0.2) as model:\n            model.run(\n                f\"Context:\\n{context}\\nUser query: {query}\\nRespond in Markdown format:\\n\"\n            )"
        }
    ]
}