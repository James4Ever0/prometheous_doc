{
    "summary": "The code imports necessary libraries, sets up a vector store index and embeddings model, initializes an OpenLM model, and constructs a VectorStoreIndex using either the vector store or docstore. The vector store index is used as a retriever to retrieve \"hello\" and print the answer in a rich format.",
    "details": [
        {
            "comment": "This code imports necessary libraries and sets up a vector store for storing and retrieving documents using the llama_index package. The code utilizes DocArrayHnswVectorStore for efficient storage and retrieval, and also creates an OpenAIEmbedding model to generate embeddings for documents.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/vectorstore_embedding_chat_rag/llamaindex_test.py\":0-28",
            "content": "from llama_index import Document\nfrom llama_index.embeddings import OllamaEmbedding  # OpenAIEmbedding\nfrom llama_index.text_splitter import SentenceSplitter\nfrom llama_index.extractors import TitleExtractor\nfrom llama_index.ingestion import IngestionPipeline\nfrom llama_index.llms import Ollama\nfrom llama_index.storage.docstore import SimpleDocumentStore\nimport os\nos.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\nfrom llama_index import VectorStoreIndex, ServiceContext, LLMPredictor\nfrom llama_index.vector_stores import DocArrayHnswVectorStore\n# from llama_index.vector_stores import SimpleVectorStore\n# from llama_index.ingestion.cache import SimpleCache\nvector_store = DocArrayHnswVectorStore(\"storage\",dim = 4096)\n# create the pipeline with transformations\n# if os.path.exists(\"./storage\"):\n#     vector_store = SimpleVectorStore.from_persist_dir()\n# else:\n#     vector_store = SimpleVectorStore(stores_text=True)\n# vector_store.stores_text=True\nembed = OllamaEmbedding(model_name=\"openhermes2.5-mistral:latest\")"
        },
        {
            "comment": "This code initializes an OpenLM model, creates a pipeline for text processing with transformations like sentence splitting and title extraction, checks if a load path exists and loads the saved pipeline if so. It then runs the pipeline on example documents, prints the resulting nodes, persists the pipeline, creates a service context using the loaded LLM and embed models, and finally constructs a VectorStoreIndex from either the vector store or the documents in the docstore.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/vectorstore_embedding_chat_rag/llamaindex_test.py\":29-59",
            "content": "llm = Ollama(model=\"openhermes2.5-mistral:latest\")\npipeline = IngestionPipeline.construct(\n    transformations=[\n        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n        TitleExtractor(llm=llm),\n        embed,\n    ],\n    docstore=SimpleDocumentStore(),  # do it if you want load/persist to work properly.\n    vector_store=vector_store,\n    validate_arguments=False,\n)\nloadpath = \"./pipeline_storage\"\nif os.path.exists(loadpath):\n    pipeline.load(persist_dir=loadpath)\n# run the pipeline\nnodes = pipeline.run(documents=[Document.example()])  # return newly added nodes.\nimport rich\n# rich.print(nodes)\n# rich.print(pipeline.documents)\n# rich.print(pipeline.docstore.docs)\npipeline.persist(persist_dir=loadpath)  # not persisting document.\nserv_cont = ServiceContext.from_defaults(\n    llm_predictor=LLMPredictor(llm),\n    embed_model=embed,\n)\n# vsindex = VectorStoreIndex.from_vector_store(vectore_store)\nvsindex = VectorStoreIndex.from_vector_store(vector_store, service_context=serv_cont)\n# vsindex = VectorStoreIndex.from_documents(pipeline.docstore.docs.values(),service_context=serv_cont)"
        },
        {
            "comment": "This code initializes the vector store index (vsindex) as a retriever rather than a query engine, retrieves \"hello\" using the retriever, and prints the answer in a rich format. A qualified prompt is missing to enhance the functionality of the query or retrieve function.",
            "location": "\"/media/root/Toshiba XG3/works/prometheous_doc/src/document_agi_computer_control/vectorstore_embedding_chat_rag/llamaindex_test.py\":61-67",
            "content": "# this will do RAG. However do you have qualified prompt?\n# engine = vsindex.as_query_engine()\nengine = vsindex.as_retriever()\nans = engine.retrieve(\"hello\")\n# ans = engine.query(\"something interesting in the document\")\nrich.print(ans)"
        }
    ]
}