{
    "0": {
        "file_id": 0,
        "content": "/document_agi_computer_control/build_website.py",
        "type": "filepath"
    },
    "1": {
        "file_id": 0,
        "content": "The code defines a function `parse_arguments()` using the `argparse` module, expecting three arguments with an assertion on the `code_dir_path` being absolute. It then parses these arguments, loads a template file, generates HTML documents from the template at specified output paths, and writes these generated HTML documents to the output paths. Additionally, JSON data is loaded, summaries are extracted, and a dictionary of titles is created.",
        "type": "summary"
    },
    "2": {
        "file_id": 0,
        "content": "import argparse\nimport json\nimport os\nfrom beartype import beartype\nfrom jinja2 import Template\n# it is better structured like:\n# db: shared, unified mapping between filename, document json name (uuid) and document summary\n# search results again in selected documents, from entry level\n# search results in selected folder (you may click different part of the filepath to jump)\n# search results in detail of each document file (if clicked in)\n# or, just build a unified search index out of the entire repo.\ndef parse_arguments():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-f\",\n        \"--file\",\n        help=\"directory of code to process\",\n    )\n    parser.add_argument(\n        \"-d\",\n        \"--document\",\n        help=\"directory of document json to read\",\n    )\n    parser.add_argument(\"-o\", \"--output\", help=\"document output path\", default=\"\")\n    args = parser.parse_args()\n    code_dir_path = args.file\n    json_path = args.document\n    output_path = args.output\n    assert os.path.isabs(code_dir_path)",
        "type": "code",
        "location": "/document_agi_computer_control/build_website.py:1-37"
    },
    "3": {
        "file_id": 0,
        "content": "This code defines a function `parse_arguments()` that parses command-line arguments using the `argparse` module. It expects three arguments: `-f/--file`, `-d/--document`, and `-o/--output`. The function asserts that the provided `code_dir_path` is an absolute path.",
        "type": "comment"
    },
    "4": {
        "file_id": 0,
        "content": "    assert os.path.isabs(json_path)\n    assert os.path.isdir(code_dir_path)\n    assert os.path.isdir(json_path)\n    return output_path, code_dir_path, json_path\n@beartype\ndef generate_html_document(template: Template, data: dict):\n    html = template.render(**data)\n    return html\n@beartype\ndef load_template(template_path: str):\n    with open(template_path, \"r\") as f:\n        content = f.read()\n    template = Template(content)\n    return template\n@beartype\ndef generate_and_write_document(template: Template, data: dict, html_output_path: str):\n    content = generate_html_document(template, data)\n    with open(html_output_path, \"w+\") as f:\n        f.write(content)\nif __name__ == \"__main__\":\n    output_path, code_dir_path, json_path = parse_arguments()\n    template_path = \"website_template.html.j2\"\n    template = load_template(template_path)\n    datalist = []\n    html_output_path = os.path.join(output_path, \"index.html\")\n    for fpath in os.listdir(json_path):\n        json_abspath = os.path.join(json_path, fpath)\n        with open(json_abspath, \"r\") as f:",
        "type": "code",
        "location": "/document_agi_computer_control/build_website.py:38-78"
    },
    "5": {
        "file_id": 0,
        "content": "This code is parsing arguments, loading a template file, and generating HTML documents from the template using provided data. The HTML documents are then written to specified output paths.",
        "type": "comment"
    },
    "6": {
        "file_id": 0,
        "content": "            data = json.load(\n                f\n            )  # {\"summary\": summary, \"details\": [{\"comment\": comment, \"location\": location, \"content\": content}, ...]}\n            summary = data[\"summary\"]\n            datalist.append(dict(title=summary))\n    datadict = {index: content for index, content in enumerate(datalist)}\n    template_data = dict(datadict=datadict)\n    generate_and_write_document(template, template_data, html_output_path)",
        "type": "code",
        "location": "/document_agi_computer_control/build_website.py:79-86"
    },
    "7": {
        "file_id": 0,
        "content": "Loading JSON data, extracting summary and creating a dictionary of titles.",
        "type": "comment"
    },
    "8": {
        "file_id": 1,
        "content": "/document_agi_computer_control/cache_db_context.py",
        "type": "filepath"
    },
    "9": {
        "file_id": 1,
        "content": "The CacheManager class, using TinyDB, provides cache database functions and works with records. It defines functions for generating/verifying file paths/hashes. The test environment is set up in temporary directories, iterating through source/target files to generate test parameters and print \"test passed\" if successful.",
        "type": "summary"
    },
    "10": {
        "file_id": 1,
        "content": "import hashlib\nimport os\nfrom contextlib import contextmanager\nfrom typing import Any, Callable, Iterable, Optional, Tuple\nimport pydantic\nimport tinydb\nfrom beartype import beartype\nimport tempfile\nUTF8 = \"utf-8\"\n@beartype\ndef read_file_bytes(filename: str):\n    with open(filename, \"rb\") as f:\n        content = f.read()\n    return content\n@beartype\ndef hash_file(filename: str):\n    content = read_file_bytes(filename)\n    hash_obj = hashlib.md5(content)\n    ret = hash_obj.hexdigest()\n    return ret\n@beartype\nclass CacheManager:\n    class subkey:\n        path = \"path\"\n        hash = \"hash\"\n    class key:\n        source = \"source\"\n        target = \"target\"\n    def __init__(self, db_path: str):\n        self.init_db(db_path)\n        self.init_query()\n    def init_db(self, db_path: str):\n        self.db_path = db_path\n        self.db = tinydb.TinyDB(db_path)\n    def init_query(self):\n        self.query = tinydb.Query()\n        self.source_path_query, self.source_hash_query = self.construct_query_by_key(\n            self.key.source",
        "type": "code",
        "location": "/document_agi_computer_control/cache_db_context.py:1-51"
    },
    "11": {
        "file_id": 1,
        "content": "The code defines a `CacheManager` class that manages a cache database using the TinyDB library. It provides functions for reading file content in bytes, hashing file content using MD5, and initializing the database and query objects. The `CacheManager` class uses the `subkey` and `key` classes to define specific subkeys within the cache database.",
        "type": "comment"
    },
    "12": {
        "file_id": 1,
        "content": "        )\n        self.target_path_query, self.target_hash_query = self.construct_query_by_key(\n            self.key.target\n        )\n    def source_hash_eq(self, other: str):\n        return self.source_hash_query == other\n    def source_path_eq(self, other: str):\n        return self.source_path_query == other\n    def target_hash_eq(self, other: str):\n        return self.target_hash_query == other\n    def target_path_eq(self, other: str):\n        return self.target_path_query == other\n    def construct_query_by_key_and_subkey(self, key: str, subkey: str):\n        key_query = getattr(self.query, key)\n        subkey_query = getattr(key_query, subkey)\n        return subkey_query\n    def construct_query_by_key(self, key: str):\n        path_query = self.construct_query_by_key_and_subkey(key, self.subkey.path)\n        hash_query = self.construct_query_by_key_and_subkey(key, self.subkey.hash)\n        return path_query, hash_query\n    def get_record_by_computing_source_hash(self, source_path: str):\n        source_hash = hash_file(source_path)",
        "type": "code",
        "location": "/document_agi_computer_control/cache_db_context.py:52-80"
    },
    "13": {
        "file_id": 1,
        "content": "This code defines a class with methods for comparing hash and path values, constructing queries using key and subkey attributes, and computing the source hash of a file. The class also includes a method to retrieve records by computing the source hash.",
        "type": "comment"
    },
    "14": {
        "file_id": 1,
        "content": "        record = self.db.get(\n            self.source_hash_eq(source_hash)\n        )  # not necessarily directly pointing to the filepath\n        return record, source_hash\n    @classmethod\n    def get_record_file_path_and_hash(cls, record: dict, key: str) -> tuple[str, str]:\n        filepath = record[key][cls.subkey.path]\n        filehash = record[key][cls.subkey.hash]\n        return filepath, filehash\n    @classmethod\n    def get_record_source_path_and_hash(cls, record: dict):\n        return cls.get_record_file_path_and_hash(record, cls.key.source)\n    @classmethod\n    def get_record_target_path_and_hash(cls, record: dict):\n        return cls.get_record_file_path_and_hash(record, cls.key.target)\n    @classmethod\n    def verify_record_file_hash(cls, record: dict, key: str):\n        filepath, filehash = cls.get_record_file_path_and_hash(record, key)\n        verified = verify_filehash(filepath, filehash)\n        return verified\n    @classmethod\n    def verify_record_source_hash(cls, record: dict):\n        verified = cls.verify_record_file_hash(record, cls.key.source)",
        "type": "code",
        "location": "/document_agi_computer_control/cache_db_context.py:81-108"
    },
    "15": {
        "file_id": 1,
        "content": "This code defines a class with various methods for working with records in a database. It retrieves and verifies file paths and hashes for source and target files, as well as gets the record directly using a source hash. The methods interact with the database to fetch relevant information from stored records.",
        "type": "comment"
    },
    "16": {
        "file_id": 1,
        "content": "        return verified\n    @classmethod\n    def verify_record_target_hash(cls, record: dict):\n        verified = cls.verify_record_file_hash(record, cls.key.target)\n        return verified\n    @classmethod\n    def construct_upsert_data(\n        cls, source_path: str, source_hash: str, target_path: str, target_hash: str\n    ):\n        data = {\n            cls.key.source: {\n                cls.subkey.path: source_path,\n                cls.subkey.hash: source_hash,\n            },\n            cls.key.target: {\n                cls.subkey.path: target_path,\n                cls.subkey.hash: target_hash,\n            },\n        }\n        return data\n    def upsert_data(\n        self, source_path: str, source_hash: str, target_path: str, target_hash: str\n    ):\n        data = self.construct_upsert_data(\n            source_path, source_hash, target_path, target_hash\n        )\n        self.db.upsert(\n            data,\n            cond=self.source_path_eq(source_path),\n        )\n@contextmanager\ndef CacheContextManager(db_path: str):",
        "type": "code",
        "location": "/document_agi_computer_control/cache_db_context.py:109-145"
    },
    "17": {
        "file_id": 1,
        "content": "The code defines a class with methods to verify and store record hashes in a database. The `verify_record_target_hash` method checks the target hash of a record, while `construct_upsert_data` constructs data for upserting into the database. The `upsert_data` method actually performs the upsert operation. The `CacheContextManager` is a context manager used to interact with the cache database.",
        "type": "comment"
    },
    "18": {
        "file_id": 1,
        "content": "    manager = CacheManager(db_path)\n    try:\n        yield manager\n    finally:\n        del manager\n@beartype\ndef verify_filehash(filepath: str, filehash: str):\n    if os.path.exists(filepath):\n        current_hash = hash_file(filepath)\n        if current_hash == filehash:\n            return True\n    return False\nclass TargetGeneratorParameter(pydantic.BaseModel):\n    target_dir_path: str\n    source_path: str\n@beartype\ndef generate_and_hash_target(\n    param: TargetGeneratorParameter,\n    target_path_generator: Callable[[TargetGeneratorParameter], str],\n    target_file_geneator: Callable[[str, str], Any],\n):\n    target_path = target_path_generator(param)\n    _ = target_file_geneator(param.source_path, target_path)\n    target_hash = hash_file(target_path)\n    return target_path, target_hash\n@beartype\ndef verify_record_target(record: dict, manager: CacheManager):\n    record_target_path, record_target_hash = manager.get_record_target_path_and_hash(\n        record\n    )\n    target_verified = verify_filehash(record_target_path, record_target_hash)",
        "type": "code",
        "location": "/document_agi_computer_control/cache_db_context.py:146-184"
    },
    "19": {
        "file_id": 1,
        "content": "This code contains functions to generate and verify target file paths and hashes. It uses a CacheManager for retrieving record targets, verifies file hashes using the hash_file function, and provides beartype decorators for type checking. The generate_and_hash_target function generates and hashes a target file, while the verify_record_target function retrieves the record's target path and hash from the CacheManager and verifies it using verify_filehash.",
        "type": "comment"
    },
    "20": {
        "file_id": 1,
        "content": "    return target_verified, record_target_path, record_target_hash\n@beartype\ndef fix_record_if_source_filename_link_is_missing(\n    source_path: str,\n    source_hash: str,\n    record_target_path: str,\n    record_target_hash: str,\n    manager: CacheManager,\n):\n    pointed_record = manager.db.get(\n        manager.source_path_eq(source_path)\n        and manager.target_path_eq(record_target_path)\n    )\n    if pointed_record is None:\n        # insert record\n        manager.upsert_data(\n            source_path, source_hash, record_target_path, record_target_hash\n        )\n@beartype\ndef check_if_target_exists_with_source_in_record(\n    record: dict, source_path: str, source_hash: str, manager: CacheManager\n):\n    has_record = False\n    target_verified, record_target_path, record_target_hash = verify_record_target(\n        record, manager\n    )\n    if target_verified:\n        # we should check if we have source filename pointing to this target.\n        fix_record_if_source_filename_link_is_missing(\n            source_path, source_hash, record_target_path, record_target_hash, manager",
        "type": "code",
        "location": "/document_agi_computer_control/cache_db_context.py:185-218"
    },
    "21": {
        "file_id": 1,
        "content": "This code defines two functions. The first function, \"fix\\_record\\_if\\_source\\_filename\\_link\\_is\\_missing\", checks if the given source file path and hash match a record with the specified target file path and hash in the cache. If not found, it adds the new record to the cache. The second function, \"check\\_if\\_target\\_exists\\_with\\_source\\_in\\_record\", verifies if a target exists in the cache with a matching source file path and hash. It returns the verified status, target file path, and target file hash if successful.",
        "type": "comment"
    },
    "22": {
        "file_id": 1,
        "content": "        )\n        has_record = True\n    else:\n        manager.db.remove(manager.target_path_eq(record_target_path))\n    return has_record, record_target_path\n@beartype\ndef check_if_source_exists_in_record(\n    source_path: str, manager: CacheManager\n) -> Tuple[bool, str, Optional[str]]:\n    has_record = False\n    record_target_path = None\n    record, source_hash = manager.get_record_by_computing_source_hash(source_path)\n    if record:\n        has_record, record_target_path = check_if_target_exists_with_source_in_record(\n            record, source_path, source_hash, manager\n        )\n    return has_record, source_hash, record_target_path\nclass SourceIteratorAndTargetGeneratorParam(pydantic.BaseModel):\n    source_dir_path: str\n    target_dir_path: str\n    db_path: str\n@beartype\ndef iterate_source_dir_and_generate_to_target_dir(\n    param: SourceIteratorAndTargetGeneratorParam,\n    source_walker: Callable[[str], Iterable[tuple[Any, str]]],\n    target_path_generator: Callable[[TargetGeneratorParameter], str],\n    target_file_geneator: Callable[[str, str], Any],",
        "type": "code",
        "location": "/document_agi_computer_control/cache_db_context.py:219-251"
    },
    "23": {
        "file_id": 1,
        "content": "The code checks if a source file exists in a record and returns the target path if it does. It also defines classes for parameters used in iterating sources and generating targets.",
        "type": "comment"
    },
    "24": {
        "file_id": 1,
        "content": "    join_source_dir: bool = True,\n) -> list[str]:\n    @beartype\n    def process_source_and_return_target_path(\n        manager: CacheManager,\n        source_path: str,\n        source_hash: str,\n    ):\n        target_path, target_hash = generate_and_hash_target(\n            TargetGeneratorParameter(\n                target_dir_path=param.target_dir_path, source_path=source_path\n            ),\n            target_path_generator,\n            target_file_geneator,\n        )\n        manager.upsert_data(source_path, source_hash, target_path, target_hash)\n        return target_path\n    @beartype\n    def get_target_path_by_checking_manager_or_processing(\n        manager: CacheManager, source_path: str\n    ) -> str:\n        (\n            has_record,\n            source_hash,\n            record_target_path,\n        ) = check_if_source_exists_in_record(source_path, manager)\n        if not has_record or not isinstance(record_target_path, str):\n            target_path = process_source_and_return_target_path(\n                manager,",
        "type": "code",
        "location": "/document_agi_computer_control/cache_db_context.py:252-281"
    },
    "25": {
        "file_id": 1,
        "content": "This code defines two functions: `process_source_and_return_target_path` and `get_target_path_by_checking_manager_or_processing`. The former generates a target path and hash for a given source path and source hash, then inserts the data into the cache manager. The latter checks if the source path exists in the cache manager's records; if not, it calls `process_source_and_return_target_path` to generate the target path.",
        "type": "comment"
    },
    "26": {
        "file_id": 1,
        "content": "                source_path,\n                source_hash,\n            )\n        else:\n            target_path = record_target_path\n        return target_path\n    @beartype\n    def process_file_and_append_to_cache_paths(\n        manager: CacheManager, fpath: str, processed_cache_paths: list[str]\n    ):\n        source_path = (\n            os.path.join(param.source_dir_path, fpath) if join_source_dir else fpath\n        )\n        target_path = get_target_path_by_checking_manager_or_processing(\n            manager, source_path\n        )\n        processed_cache_paths.append(target_path)\n    @beartype\n    def get_processed_cache_paths():\n        processed_cache_paths: list[str] = []\n        with CacheContextManager(param.db_path) as manager:\n            # to make this accountable, we need to convert it into list.\n            items = list(source_walker(param.source_dir_path))\n            items_count = len(items)\n            print(f\"\\n>>>> PROCESSING PROGRESS: 0/{items_count}\")\n            for i, (_, fpath) in enumerate(items):",
        "type": "code",
        "location": "/document_agi_computer_control/cache_db_context.py:282-309"
    },
    "27": {
        "file_id": 1,
        "content": "Function 'process_file_and_append_to_cache_paths' takes in a file path and cache manager, checks if the file is already processed, and appends its target path to the list of processed cache paths.\n\nFunction 'get_processed_cache_paths' initializes an empty list, processes files using CacheContextManager, and appends their target paths to the list of processed cache paths.",
        "type": "comment"
    },
    "28": {
        "file_id": 1,
        "content": "                print(\"processing:\", fpath)\n                # if file_empty(fpath):\n                #     continue\n                process_file_and_append_to_cache_paths(\n                    manager, fpath, processed_cache_paths\n                )\n                print(f\"\\n>>>> PROCESSING PROGRESS: {i+1}/{items_count}\")\n        return processed_cache_paths\n    return get_processed_cache_paths()\n@beartype\ndef make_and_return_dir_path(base_dir: str, subdir: str):\n    dirpath = os.path.join(base_dir, subdir)\n    os.mkdir(dirpath)\n    return dirpath\n@beartype\ndef make_source_and_target_dirs(base_dir: str):\n    @beartype\n    def make_and_return_dir_path_under_base_dir(subdir: str):\n        return make_and_return_dir_path(base_dir, subdir)\n    source_dir = make_and_return_dir_path_under_base_dir(\"source\")\n    target_dir = make_and_return_dir_path_under_base_dir(\"target\")\n    return source_dir, target_dir\n@beartype\ndef read_file(fpath: str):\n    with open(fpath, \"r\", encoding=UTF8) as f:\n        return f.read()\n@beartype",
        "type": "code",
        "location": "/document_agi_computer_control/cache_db_context.py:310-346"
    },
    "29": {
        "file_id": 1,
        "content": "Function `make_and_return_dir_path` creates a new directory at the specified path and returns it. Function `make_source_and_target_dirs` uses `make_and_return_dir_path` to create source and target directories under the given base directory, then returns them as a tuple. Function `read_file` reads the content of a file at the specified path.",
        "type": "comment"
    },
    "30": {
        "file_id": 1,
        "content": "def write_file(fpath: str, content: str):\n    with open(fpath, \"w+\", encoding=UTF8) as f:\n        f.write(content)\ndef test_main():\n    test_file_basename = \"test_file.txt\"\n    test_db_basename = \"cache.db\"\n    test_source_content = \"test\"\n    @beartype\n    def test_target_file_generator(source_path: str, target_path: str):\n        content = read_file(source_path)\n        write_file(target_path, content)\n    @beartype\n    def join_dir_path_with_test_file_basename(dir_path: str):\n        ret = os.path.join(dir_path, test_file_basename)\n        return ret\n    @beartype\n    def test_target_path_generator(param: TargetGeneratorParameter):\n        ret = join_dir_path_with_test_file_basename(param.target_dir_path)\n        return ret\n    @beartype\n    def prepare_test_param(temp_dir: str):\n        source_dir, target_dir = make_source_and_target_dirs(temp_dir)\n        db_path = os.path.join(temp_dir, test_db_basename)\n        param = SourceIteratorAndTargetGeneratorParam(\n            source_dir_path=source_dir, target_dir_path=target_dir, db_path=db_path",
        "type": "code",
        "location": "/document_agi_computer_control/cache_db_context.py:347-377"
    },
    "31": {
        "file_id": 1,
        "content": "Function to write a file, function for generating test target file paths using provided parameters, and function for preparing test parameter.",
        "type": "comment"
    },
    "32": {
        "file_id": 1,
        "content": "        )\n        return param\n    @beartype\n    def generate_test_source_walker(source_dir: str):\n        @beartype\n        def test_source_walker(dirpath: str):\n            return [(dirpath, it) for it in os.listdir(source_dir)]\n        return test_source_walker\n    @beartype\n    def write_test_content_to_file(file_path: str):\n        write_file(file_path, test_source_content)\n    @beartype\n    def assert_file_content_as_test_content(file_path):\n        test_target_content = read_file(file_path)\n        assert test_target_content == test_source_content\n    @contextmanager\n    @beartype\n    def prepare_test_file_context(source_dir: str, target_dir: str):\n        test_source_path = join_dir_path_with_test_file_basename(source_dir)\n        test_target_path = join_dir_path_with_test_file_basename(target_dir)\n        write_test_content_to_file(test_source_path)\n        try:\n            yield\n        finally:\n            assert_file_content_as_test_content(test_target_path)\n    def test_and_assert(param: SourceIteratorAndTargetGeneratorParam):",
        "type": "code",
        "location": "/document_agi_computer_control/cache_db_context.py:378-409"
    },
    "33": {
        "file_id": 1,
        "content": "The code defines functions for generating a test source walker, writing test content to file, asserting file content as test content, and preparing a test file context. These functions work together to perform tests on source and target files with a common structure and content.",
        "type": "comment"
    },
    "34": {
        "file_id": 1,
        "content": "        with prepare_test_file_context(param.source_dir_path, param.target_dir_path):\n            test_source_walker = generate_test_source_walker(param.source_dir_path)\n            iterate_source_dir_and_generate_to_target_dir(\n                param,\n                test_source_walker,\n                test_target_path_generator,\n                test_target_file_generator,\n            )\n    def test_in_temporary_directory():\n        with tempfile.TemporaryDirectory() as temp_dir:\n            param = prepare_test_param(temp_dir)\n            test_and_assert(param)\n    test_in_temporary_directory()\n    print(\"test passed\")\nif __name__ == \"__main__\":\n    test_main()",
        "type": "code",
        "location": "/document_agi_computer_control/cache_db_context.py:410-429"
    },
    "35": {
        "file_id": 1,
        "content": "The code is setting up a test environment in temporary directories, generating test parameters, iterating through source and target directories, and then asserting the test results. It also includes a function to generate a test source walker and two generators for target paths and files. Finally, it prints \"test passed\" if the test is successful.",
        "type": "comment"
    },
    "36": {
        "file_id": 2,
        "content": "/document_agi_computer_control/code_view_demo.py",
        "type": "filepath"
    },
    "37": {
        "file_id": 2,
        "content": "This code defines a function called \"greet\" that takes a name as input and prints out a greeting message. The function is defined 15 times, each time with the same implementation, and called once with the argument \"World\".",
        "type": "summary"
    },
    "38": {
        "file_id": 2,
        "content": "def greet(name):\n    print(\"Hello, \" + name)\ngreet(\"World\")\ndef greet(name):\n    print(\"Hello, \" + name)\ngreet(\"World\")\ndef greet(name):\n    print(\"Hello, \" + name)\ngreet(\"World\")\ndef greet(name):\n    print(\"Hello, \" + name)\ngreet(\"World\")\ndef greet(name):\n    print(\"Hello, \" + name)\ngreet(\"World\")\ndef greet(name):\n    print(\"Hello, \" + name)\ngreet(\"World\")\ndef greet(name):\n    print(\"Hello, \" + name)\ngreet(\"World\")\ndef greet(name):\n    print(\"Hello, \" + name)\ngreet(\"World\")\ndef greet(name):\n    print(\"Hello, \" + name)\ngreet(\"World\")\ndef greet(name):\n    print(\"Hello, \" + name)\ngreet(\"World\")\ndef greet(name):\n    print(\"Hello, \" + name)\ngreet(\"World\")\ndef greet(name):\n    print(\"Hello, \" + name)\ngreet(\"World\")\ndef greet(name):\n    print(\"Hello, \" + name)\ngreet(\"World\")",
        "type": "code",
        "location": "/document_agi_computer_control/code_view_demo.py:2-63"
    },
    "39": {
        "file_id": 2,
        "content": "This code defines a function called \"greet\" that takes a name as input and prints out a greeting message. The function is defined 15 times, each time with the same implementation, and called once with the argument \"World\".",
        "type": "comment"
    },
    "40": {
        "file_id": 3,
        "content": "/document_agi_computer_control/codepiece_summarizer.py",
        "type": "filepath"
    },
    "41": {
        "file_id": 3,
        "content": "This code defines a comment summarizer function that uses an LLM to generate summaries by combining and summarizing pairs of comments. The function includes a helper function for combining and recursively calling itself until there's only one or no comment left.",
        "type": "summary"
    },
    "42": {
        "file_id": 3,
        "content": "from beartype import beartype\nfrom llm import LLM\n@beartype\ndef comment_summarizer(comments: list[str], word_limit: int = 30) -> str:\n    summary_prompt = \"\"\"You are a professional summarizer. You will be given a pair of comments and produce a concise summary.\n\"\"\"\n    summary_model = LLM(summary_prompt)\n    def combine_comments(comment1: str, comment2: str):\n        summary_query = f\"\"\"\n{comment1}\n{comment2}\nSummary in {word_limit} words (do not be verbose, just summarize):\n\"\"\"\n        ret = summary_model.run(summary_query)\n        return ret\n    def recursive_combine(comments_list: list[str]):\n        if len(comments_list) == 0:\n            raise Exception(\"No comments to combine\")\n        elif len(comments_list) == 1:\n            return comments_list[0]\n        elif len(comments_list) % 2 == 0:\n            combined = [\n                combine_comments(comments_list[i], comments_list[i + 1])\n                for i in range(0, len(comments_list), 2)\n            ]\n        else:\n            combined = [\n                combine_comments(comments_list[i], comments_list[i + 1])",
        "type": "code",
        "location": "/document_agi_computer_control/codepiece_summarizer.py:1-35"
    },
    "43": {
        "file_id": 3,
        "content": "This code defines a function `comment_summarizer` that takes a list of comments and returns a summary. It uses the LLM (Large Language Model) to generate summaries by combining and summarizing pairs of comments. The function also includes a helper function `combine_comments` to combine two comments and recursively call `recursive_combine` for a list of comments, unless there's only one or no comment remaining.",
        "type": "comment"
    },
    "44": {
        "file_id": 3,
        "content": "                for i in range(0, len(comments_list) - 1, 2)\n            ]\n            combined += [comments_list[-1]]\n        return recursive_combine(combined)\n    summary = recursive_combine(comments)\n    del summary_model\n    return summary",
        "type": "code",
        "location": "/document_agi_computer_control/codepiece_summarizer.py:36-43"
    },
    "45": {
        "file_id": 3,
        "content": "This code segment is implementing a recursive function to combine pairs of comments from a list, starting from the last element. It returns the combined summary after deleting the temporary summary_model.",
        "type": "comment"
    },
    "46": {
        "file_id": 4,
        "content": "/document_agi_computer_control/custom_doc_writer.py",
        "type": "filepath"
    },
    "47": {
        "file_id": 4,
        "content": "This code creates a custom document writer, initializes processing queues, and provides content processing with character/line limits. It iterates through content, handling files, language models, prompts generators, and code understanding prompts. It appears to be part of a larger program using the \"CodeWriter\" class for writing code files.",
        "type": "summary"
    },
    "48": {
        "file_id": 4,
        "content": "# will implement the doc writer myself.\n# first thing: visualize the progress.\n# TODO: specify location like: module -> filename -> block name (class/method) -> lineno\n# usually the line is not so long. but if it does, we cut it.\nfrom typing import Callable\nimport random\nimport argparse, os\nimport json\nfrom beartype import beartype\nfrom beartype.door import is_bearable\nfrom beartype.vale import Is\nfrom typing import Annotated, Optional  # <--------------- if Python ≥ 3.9.0\nfrom llm import LLM, llm_context  # type:ignore\nimport copy\nfrom codepiece_summarizer import comment_summarizer  # type:ignore\nfrom typing import TypedDict\nUTF8 = \"utf-8\"\nDEFAULT_LINE_LIMIT = 50\nDEFAULT_GRACE_PERIOD_CHAR_LIMIT = 100  # TODO: grace period support in line spliting\nclass CustomDocumentWriterParams(TypedDict):\n    location_prefix: Optional[str]\n    project_name: Optional[str]\nCUSTOM_DOC_WRITER_PARAMS = CustomDocumentWriterParams(\n    location_prefix=None, project_name=None\n)\nDEFAULT_CHAR_LIMIT = 1000\nNonEmptyString = Annotated[str, Is[lambda str_obj: len(str_obj.strip()) > 0]]",
        "type": "code",
        "location": "/document_agi_computer_control/custom_doc_writer.py:1-37"
    },
    "49": {
        "file_id": 4,
        "content": "The code is importing necessary modules and defining custom types for document writer parameters. It sets default values and ensures non-empty string inputs. The purpose is to implement a custom document writer, possibly for code or documentation generation or visualization.",
        "type": "comment"
    },
    "50": {
        "file_id": 4,
        "content": "class DocumentProcessingException(Exception):\n    ...  # placeholder\nclass UnableToCutByLineLimit(Exception):\n    ...\nclass ZeroCutIndex(Exception):\n    def __init__(self):\n        super().__init__(\"Unable to cut with zero cut index.\")\n@beartype\ndef commentProcessMethodFactory(\n    model: LLM, prompt_generator: Callable[[str, str, str], str]\n):\n    @beartype\n    def commentProcessMethod(\n        content: NonEmptyString,\n        location: NonEmptyString,\n        previous_comment: str = \"\",\n    ) -> tuple[bool, str]:\n        success = False\n        prompt = prompt_generator(content, location, previous_comment)\n        result = model.run(prompt)\n        success = True\n        return success, result\n    return commentProcessMethod\nclass DocProcessingItem(TypedDict):\n    comment: str\n    location: str\n    content: str\n@beartype\nclass DocProcessQueue:\n    def __init__(\n        self,\n        process_method: Callable[[str, str, str], tuple[bool, str]],\n        filepath: str,\n        char_limit: int = DEFAULT_CHAR_LIMIT,\n        line_limit: int = DEFAULT_LINE_LIMIT,",
        "type": "code",
        "location": "/document_agi_computer_control/custom_doc_writer.py:40-85"
    },
    "51": {
        "file_id": 4,
        "content": "The code defines a class `DocProcessQueue` that initializes with a processing method, file path, and optional character and line limits. The processing method takes content, location, and previous comment as inputs to generate a prompt for the LLM model to process. The processed result is returned as success and content tuple. If cut index is zero, it raises `ZeroCutIndex` exception.",
        "type": "comment"
    },
    "52": {
        "file_id": 4,
        "content": "        grace_period_char_limit: int = DEFAULT_GRACE_PERIOD_CHAR_LIMIT,\n        sample_size: Optional[int] = None,\n        use_previous_comment: bool = True,\n    ):\n        self.init_limits_and_counters(char_limit, line_limit, grace_period_char_limit)\n        self.init_storage()\n        self.init_sample(sample_size)\n        self.process_method = process_method\n        self.filepath = filepath\n        self.use_previous_comment = use_previous_comment\n    def init_sample(self, sample_size: Optional[int]):\n        self.sample_size = sample_size  # type: ignore\n        self.random_sample = self.sample_size is not None\n    def init_storage(self):\n        self.queue = []\n        self.locations = []\n        self.result_all: list[DocProcessingItem] = []\n        self.previous_comment = \"\"\n    def init_limits_and_counters(\n        self, char_limit: int, line_limit: int, grace_period_char_limit: int\n    ):\n        self.char_limit = char_limit\n        self.line_limit = line_limit\n        self.grace_period_char_limit = grace_period_char_limit",
        "type": "code",
        "location": "/document_agi_computer_control/custom_doc_writer.py:86-112"
    },
    "53": {
        "file_id": 4,
        "content": "This code snippet initializes the necessary attributes for a custom document writer. It takes parameters such as char_limit, line_limit, grace_period_char_limit, sample_size, process_method, and filepath. It sets up internal lists and variables, handles optional arguments, and defines methods for initializing sample size, storage, and limits/counters.",
        "type": "comment"
    },
    "54": {
        "file_id": 4,
        "content": "        self.char_count = 0\n        self.line_count = 0\n        # self.grace_period = False\n    def char_limit_exceeded(self):\n        return self.char_count > self.char_limit\n    def line_limit_exceeded(self):\n        return self.line_count > self.line_limit\n    def strip_storage_by_cut_index(self, cut_index: int):\n        self.queue = self.queue[cut_index:]\n        self.locations = self.locations[cut_index:]\n    def prepare_content_and_location(\n        self, cut_index: int, cut_content: Optional[str] = None\n    ):\n        from_lineno, to_lineno = self.locations[0], self.locations[cut_index - 1]\n        lines = self.queue[:cut_index]\n        if cut_content is not None:\n            lines[-1] = cut_content\n        content = \"\\n\".join(lines)\n        location = f'\"{self.filepath}\":{from_lineno}-{to_lineno}'\n        return content, location\n    def get_cut_and_remained_params_by_char_limit(self):\n        char_count = 0\n        remained_content = None\n        cut_content = None\n        remained_location = None\n        cut_index = None",
        "type": "code",
        "location": "/document_agi_computer_control/custom_doc_writer.py:114-145"
    },
    "55": {
        "file_id": 4,
        "content": "This code defines a class with properties for character and line limits. It includes methods to check if the character or line limit has been exceeded, strip storage by cut index, prepare content and location for cutting at a specified index, and get cut and remained parameters based on the character limit.",
        "type": "comment"
    },
    "56": {
        "file_id": 4,
        "content": "        for index, line in enumerate(self.queue):\n            char_count += len(line)\n            if char_count > self.char_limit:\n                if char_count < self.char_limit + self.grace_period_char_limit:\n                    cut_content = line\n                    remained_content = \"\"\n                    remained_location = self.locations[index]\n                    cut_index = index + 1\n                else:\n                    reverse_cut_point = char_count - self.char_limit\n                    cut_point = len(line) - reverse_cut_point\n                    cut_content = line[:cut_point]\n                    remained_content = line[cut_point:]\n                    remained_location = self.locations[index]\n                    cut_index = index + 1\n                break\n        return cut_index, cut_content, remained_content, remained_location\n    def process_by_char_limit(self):\n        (\n            cut_index,\n            cut_content,\n            remained_content,\n            remained_location,\n        ) = self.get_cut_and_remained_params_by_char_limit()",
        "type": "code",
        "location": "/document_agi_computer_control/custom_doc_writer.py:146-170"
    },
    "57": {
        "file_id": 4,
        "content": "This code processes text lines based on a character limit, cutting lines that exceed the limit. It keeps track of cut, remained, and location data for each line. The process_by_char_limit function calls get_cut_and_remained_params_by_char_limit to return these values.",
        "type": "comment"
    },
    "58": {
        "file_id": 4,
        "content": "        content, location = self.prepare_content_and_location(\n            cut_index, cut_content  # type:ignore\n        )\n        self.strip_storage_by_cut_index(cut_index)  # type:ignore\n        if remained_content:\n            self.queue.insert(0, remained_content)\n            self.locations.insert(0, remained_location)\n        return content, location\n    def update_counters(self):\n        self.char_count = len(\"\".join(self.queue))\n        self.line_count = len(self.queue)\n    def get_cut_params_by_line_limit(self, final: bool):\n        if self.line_count > self.line_limit:\n            cut_index = self.line_limit\n        elif final:\n            cut_index = self.line_count\n        else:\n            raise UnableToCutByLineLimit(\n                f\"Current line count {self.line_count} below limit {self.line_limit}\"\n            )\n        if cut_index == 0:\n            raise ZeroCutIndex()\n        cut_content = None\n        return cut_index, cut_content\n    def process_by_line_limit(self, final=False):\n        cut_index, cut_content = self.get_cut_params_by_line_limit(final)",
        "type": "code",
        "location": "/document_agi_computer_control/custom_doc_writer.py:171-200"
    },
    "59": {
        "file_id": 4,
        "content": "This code snippet is part of a custom document writer class. It includes methods for preparing content and location, stripping storage by cut index, updating counters based on the queue, getting cut parameters by line limit, and processing the document by line limit. The goal appears to be managing the document's contents while adhering to specified limits or conditions.",
        "type": "comment"
    },
    "60": {
        "file_id": 4,
        "content": "        content, location = self.prepare_content_and_location(cut_index, cut_content)\n        self.strip_storage_by_cut_index(cut_index)\n        return content, location\n    def process_by_limit(self, final=False):\n        processed = True\n        content = \"\"\n        location = \"\"\n        if self.char_limit_exceeded():\n            content, location = self.process_by_char_limit()  # here we use grace period\n        elif self.line_limit_exceeded() or final:\n            content, location = self.process_by_line_limit(final=final)\n        else:\n            processed = False\n        if processed:\n            self.update_counters()\n        return processed, content, location\n    def iterate_all_content_and_location_pairs(self):\n        while True:\n            try:\n                processed, content, location = self.process_by_limit(final=True)\n                if processed:\n                    yield content, location\n                else:\n                    break\n            except ZeroCutIndex:\n                break",
        "type": "code",
        "location": "/document_agi_computer_control/custom_doc_writer.py:201-229"
    },
    "61": {
        "file_id": 4,
        "content": "This code snippet defines a class with methods to process content based on character and line limits. It also provides an iterator to yield processed content and location pairs until a ZeroCutIndex exception is encountered.",
        "type": "comment"
    },
    "62": {
        "file_id": 4,
        "content": "    def process_content_and_location_pair(self, content: str, location: str):\n        success, result = self.process_method(\n            content,\n            location,\n            **(\n                {}\n                if not self.use_previous_comment\n                else dict(previous_comment=self.previous_comment)\n            ),  # type:ignore\n        )\n        if not success:\n            raise DocumentProcessingException(\"Failed to process code at:\", location)\n        self.previous_comment = result\n        ret = DocProcessingItem(\n            comment=result, location=location, content=content  # type:ignore\n        )\n        return ret\n    def collect_all_content_and_location_pairs(self):\n        ret = []\n        for content, location in self.iterate_all_content_and_location_pairs():\n            if is_bearable(content, NonEmptyString):\n                it = (content, location)\n                ret.append(it)\n        return ret\n    def sample_content_and_location_pairs(self):\n        pairs = self.collect_all_content_and_location_pairs()",
        "type": "code",
        "location": "/document_agi_computer_control/custom_doc_writer.py:231-258"
    },
    "63": {
        "file_id": 4,
        "content": "The code defines a class with two methods. The `process_content_and_location_pair` method takes a content and location, processes it using a specific method, and returns a DocProcessingItem object containing the result and original location. The `collect_all_content_and_location_pairs` method iterates over all pairs of content and location, filters out unsuitable ones, and returns a list of remaining pairs.",
        "type": "comment"
    },
    "64": {
        "file_id": 4,
        "content": "        pair_count = len(pairs)\n        if self.random_sample:\n            self.sample_size: int\n            if pair_count > self.sample_size:\n                pairs = random.sample(pairs, k=self.sample_size)\n        return pairs\n    def process_and_collect_all(self):\n        for content, location in self.sample_content_and_location_pairs():\n            it = self.process_content_and_location_pair(content, location)\n            self.result_all.append(it)\n        return copy.copy(self.result_all)\n    def update_counter_after_push(self, content: str):\n        self.char_count += len(content)\n        self.line_count += 1\n    def push(self, content: str, lineno: int):\n        if is_bearable(content, NonEmptyString):\n            self.queue.append(content)\n            self.locations.append(lineno)\n            self.update_counter_after_push(content)  # type:ignore\n@beartype\ndef split_by_line(content: str, newline=\"\\n\"):\n    return content.split(newline)\n@beartype\ndef process_content_and_get_result(process_queue: DocProcessQueue, content: str):",
        "type": "code",
        "location": "/document_agi_computer_control/custom_doc_writer.py:259-289"
    },
    "65": {
        "file_id": 4,
        "content": "Code handles document processing, sampling, and result collection. It splits content by lines, updates counter after push, processes content and gets results, and provides a method to process sample content and location pairs.",
        "type": "comment"
    },
    "66": {
        "file_id": 4,
        "content": "    @beartype\n    def iterate_and_push_line_to_process_queue(lines: list[str]):\n        for lineno, line in enumerate(lines):\n            process_queue.push(line, lineno)\n    def split_and_process_lines():\n        lines = split_by_line(content)\n        iterate_and_push_line_to_process_queue(lines)\n        return process_queue.process_and_collect_all()\n    return split_and_process_lines()\n@beartype\ndef assert_exists_as_absolute_directory(basepath: str):\n    assert os.path.isabs(basepath)\n    assert os.path.isdir(basepath)\n@beartype\ndef join_and_assert_exists_as_absolute_directory(basepath: str, name: str):\n    joined_path = os.path.join(basepath, name)\n    assert_exists_as_absolute_directory(joined_path)\n    return joined_path\ndef parse_arguments():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-d\",\n        \"--document_dir\",\n        help=f\"document directory, contains 'src' as source code directory, 'doc' as comment json directory\",\n    )\n    parser.add_argument(\n        \"-u\",\n        \"--repository_url\",",
        "type": "code",
        "location": "/document_agi_computer_control/custom_doc_writer.py:290-326"
    },
    "67": {
        "file_id": 4,
        "content": "The code defines two functions, `iterate_and_push_line_to_process_queue` and `split_and_process_lines`, which process lines of content by splitting them and pushing them to a queue for further processing. It also includes `assert_exists_as_absolute_directory` and `join_and_assert_exists_as_absolute_directory` functions that handle paths and directories, and `parse_arguments` function for command line arguments parsing using the argparse module.",
        "type": "comment"
    },
    "68": {
        "file_id": 4,
        "content": "        help=f\"url of source code repository\",\n    )\n    args = parser.parse_args()\n    document_dir = args.document_dir\n    repository_url = args.repository_url\n    assert_exists_as_absolute_directory(document_dir)\n    join_and_assert_exists_as_absolute_directory(document_dir, \"src\")\n    join_and_assert_exists_as_absolute_directory(document_dir, \"doc\")\n    return document_dir, repository_url\n@beartype\ndef summary_code_comment_return_value(ret: list[DocProcessingItem]):\n    comment_list = [elem[\"comment\"] for elem in ret]\n    summary = comment_summarizer(comment_list)\n    return summary\nclass DocProcessingResult(TypedDict):\n    summary: str\n    details: list[DocProcessingItem]\n@beartype\ndef process_content_and_return_result(\n    model: LLM,\n    prompt_generator: Callable[[str, str, str], str],\n    code_file_path: str,\n    content: str,\n    char_limit: int = DEFAULT_CHAR_LIMIT,\n    line_limit: int = DEFAULT_LINE_LIMIT,\n    sample_size: Optional[int] = None,\n    use_previous_comment: bool = True,\n) -> DocProcessingResult:",
        "type": "code",
        "location": "/document_agi_computer_control/custom_doc_writer.py:327-362"
    },
    "69": {
        "file_id": 4,
        "content": "This code defines a function `process_content_and_return_result` that takes in a language model (LLM), prompt generator, code file path, and content. It also includes optional parameters for character and line limits, sample size, and use of previous comment. The function processes the content by generating prompts and passing them to the language model. It returns a `DocProcessingResult` object containing a summary and details list.",
        "type": "comment"
    },
    "70": {
        "file_id": 4,
        "content": "    commentProcessMethod = commentProcessMethodFactory(model, prompt_generator)\n    process_queue = DocProcessQueue(\n        commentProcessMethod,\n        code_file_path,\n        char_limit=char_limit,\n        line_limit=line_limit,\n        sample_size=sample_size,\n        use_previous_comment=use_previous_comment,\n    )\n    result_all = process_content_and_get_result(process_queue, content)\n    if result_all == []:\n        raise Exception(\"Empty processed result for file: %s\" % code_file_path)\n    summary = summary_code_comment_return_value(result_all)\n    data = DocProcessingResult(summary=summary, details=result_all)\n    del process_queue\n    return data\n@beartype\ndef read_file(file_path: str, encoding=UTF8):\n    with open(file_path, \"r\", encoding=encoding) as f:\n        content = f.read()\n    return content\n@beartype\ndef serialize_dict_and_write_to_file(data_dict: dict, file_path: str, encoding=UTF8):\n    with open(file_path, \"w+\", encoding=encoding) as f:\n        f.write(json.dumps(data_dict, indent=4))\n@beartype",
        "type": "code",
        "location": "/document_agi_computer_control/custom_doc_writer.py:363-394"
    },
    "71": {
        "file_id": 4,
        "content": "This code defines a function that reads a file, processes its content using a custom document writer, and returns the processed data. It uses a method factory to create a comment processing method, manages a process queue for the document processing task, and handles exceptions for empty results. The code also includes helper functions for reading files (`read_file`) and writing serialized dictionaries to files (`serialize_dict_and_write_to_file`).",
        "type": "comment"
    },
    "72": {
        "file_id": 4,
        "content": "def process_code_and_write_result(\n    model: LLM,\n    prompt_generator: Callable[[str, str, str], str],\n    code_file_path: str,\n    output_path: str,\n    char_limit: int = DEFAULT_CHAR_LIMIT,\n    line_limit: int = DEFAULT_LINE_LIMIT,\n    sample_size: Optional[int] = None,\n    use_previous_comment=True,\n) -> DocProcessingResult:\n    content = read_file(code_file_path)\n    data = process_content_and_return_result(\n        model,\n        prompt_generator,\n        code_file_path,\n        content,\n        char_limit=char_limit,\n        line_limit=line_limit,\n        sample_size=sample_size,\n        use_previous_comment=use_previous_comment,\n    )\n    serialize_dict_and_write_to_file(data, output_path)  # type:ignore\n    return data\n@beartype\ndef filter_empty_elements(mlist: list):\n    return [elem for elem in mlist if elem]\n# TODO: check if is relative path only\n@beartype\ndef generate_location_component(location: str):\n    location_prefix = CUSTOM_DOC_WRITER_PARAMS.get(\"location_prefix\", None)\n    if isinstance(location_prefix, str):",
        "type": "code",
        "location": "/document_agi_computer_control/custom_doc_writer.py:395-429"
    },
    "73": {
        "file_id": 4,
        "content": "This code defines a function `process_code_and_write_result` that reads code from a file, processes it using the provided model and prompt generator, and writes the processed content to another file. The code also includes two decorator functions: `filter_empty_elements`, which removes empty elements from a list, and `generate_location_component`, which extracts a location prefix from a given string if it exists in the configuration.",
        "type": "comment"
    },
    "74": {
        "file_id": 4,
        "content": "        lp = '\"' + location_prefix + \"/src/\"\n        assert location.startswith(lp)\n        project_name = CUSTOM_DOC_WRITER_PARAMS[\"project_name\"]\n        location = (\n            '\"' + (project_name + \"/\" if project_name else \"\") + location[len(lp) :]\n        )\n    return f\"\"\"Storage location: {location}\"\"\"\n@beartype\ndef generate_previous_comment_component(previous_comment: str):\n    return (\n        f\"\"\"Previous code comment:\n{previous_comment}\"\"\"\n        if previous_comment\n        else \"\"\n    )\n@beartype\ndef generate_code_component(programming_language: str, code: str):\n    return f\"\"\"Code:\n```{programming_language}\n{code}\n```\"\"\"\ndef generate_comment_coponent():\n    # return \"\"\"Comment for code:\n    # to reduce verbosity\n    return \"\"\"\nComment for code in 30 words (do not repeat or rephrase the content, be concise and relevant):\n\"\"\"\n@beartype\ndef generate_prompt_components(\n    code: str, location: str, programming_language: str, previous_comment: str\n):\n    location_component = generate_location_component(location)",
        "type": "code",
        "location": "/document_agi_computer_control/custom_doc_writer.py:430-469"
    },
    "75": {
        "file_id": 4,
        "content": "This function generates the location component of the prompt, using the given location string.",
        "type": "comment"
    },
    "76": {
        "file_id": 4,
        "content": "    previous_comment_component = generate_previous_comment_component(previous_comment)\n    code_component = generate_code_component(programming_language, code)\n    comment_component = generate_comment_coponent()\n    components = [\n        location_component,\n        previous_comment_component,\n        code_component,\n        comment_component,\n    ]\n    return components\n@beartype\ndef assemble_prompt_components(components: list[str]):\n    components = filter_empty_elements(components)\n    ret = \"\\n\".join(components)\n    return ret\n@beartype\ndef generate_prompt_generator(programming_language: str):\n    @beartype\n    def prompt_generator(code: str, location: str, previous_comment: str = \"\"):\n        components = generate_prompt_components(\n            code, location, programming_language, previous_comment\n        )\n        ret = assemble_prompt_components(components)\n        return ret\n    return prompt_generator\n@beartype\ndef generate_prompt_base(word_limit: int):\n    return f\"\"\"You are reading code from codebase",
        "type": "code",
        "location": "/document_agi_computer_control/custom_doc_writer.py:470-504"
    },
    "77": {
        "file_id": 4,
        "content": "This code generates a prompt to help users read and understand code. It breaks down the code into components like location, previous comment, code snippet, and comment, then assembles them into a single formatted string. The function `generate_prompt_base` returns the base structure of the prompt message, while `generate_prompt_generator` creates a generator function that takes in the code, location, and optional previous comment to generate the final prompt.",
        "type": "comment"
    },
    "78": {
        "file_id": 4,
        "content": " in chunks. You would understand what the code is doing and return brief comments (under {word_limit} words).\"\"\"\n@beartype\ndef construct_llm_and_write_code_comment(\n    code_file_path: str,\n    output_path: str,\n    programming_language: str = \"\",\n    word_limit: int = 15,\n    use_previous_comment: bool = False,  # different from our blog summarizer.\n):\n    prompt_base = generate_prompt_base(word_limit)\n    prompt_generator = generate_prompt_generator(programming_language)\n    with llm_context(prompt_base) as model:\n        ret = process_code_and_write_result(\n            model,\n            prompt_generator,\n            code_file_path,\n            output_path,\n            use_previous_comment=use_previous_comment,\n        )\n    return ret\ndef main():\n    document_dir, repository_url = parse_arguments()\n    # programming_language, code_file_path, output_path = parse_arguments()\n    programming_language = \"\"\n    code_file_path = os.path.join(document_dir, \"src\")\n    output_path = \"doc\"\n    construct_llm_and_write_code_comment(",
        "type": "code",
        "location": "/document_agi_computer_control/custom_doc_writer.py:504-536"
    },
    "79": {
        "file_id": 4,
        "content": "This code defines a function \"construct_llm_and_write_code_comment\" that takes input parameters such as file paths, programming language, and word limit. It then generates a prompt using the input parameters, processes the code from the specified file path, and writes the result to the specified output path. The main() function is used for parsing arguments and calling the \"construct_llm_and_write_code_comment\" function.",
        "type": "comment"
    },
    "80": {
        "file_id": 4,
        "content": "        code_file_path, output_path, programming_language=programming_language\n    )\nif __name__ == \"__main__\":\n    main()",
        "type": "code",
        "location": "/document_agi_computer_control/custom_doc_writer.py:537-542"
    },
    "81": {
        "file_id": 4,
        "content": "This code appears to be part of a larger program. It creates an instance of a class (presumably named \"CodeWriter\") with arguments for code_file_path, output_path, and programming_language. Then it checks if the current file is the main module and calls the \"main\" function if so. This suggests that the main functionality of this program may be in the \"main\" function, and that the \"CodeWriter\" class is used to write code files.",
        "type": "comment"
    },
    "82": {
        "file_id": 5,
        "content": "/document_agi_computer_control/ensure_fdignore_with_doc.py",
        "type": "filepath"
    },
    "83": {
        "file_id": 5,
        "content": "Ensures the .fdignore file in the specified base directory contains \"docs\" entry.",
        "type": "summary"
    },
    "84": {
        "file_id": 5,
        "content": "# TODO: ensure \"docs\" in the .fdignore\nimport argparse\nimport os\nparser = argparse.ArgumentParser(\n    description='Make sure .fdignore file under base directory contains \"docs\" entry'\n)\nparser.add_argument(\"-b\", \"--basedir\", help=\"base directory\", required=True, type=str)\nargs = parser.parse_args()\nbasedir = args.basedir\nassert os.path.isdir(basedir)\nassert os.path.isabs(basedir)\nassert os.path.exists(basedir)\nfdignore_path = os.path.join(basedir, \".fdignore\")\nlines = []\nif os.path.exists(fdignore_path):\n    with open(fdignore_path, \"r\") as f:\n        content = f.read()\n        lines = content.split(\"\\n\")\nif \"docs\" not in lines:\n    lines.append(\"docs\")\n    with open(fdignore_path, \"w+\") as f:\n        f.write(\"\\n\".join(lines))",
        "type": "code",
        "location": "/document_agi_computer_control/ensure_fdignore_with_doc.py:1-27"
    },
    "85": {
        "file_id": 5,
        "content": "Ensures the .fdignore file in the specified base directory contains \"docs\" entry.",
        "type": "comment"
    },
    "86": {
        "file_id": 6,
        "content": "/document_agi_computer_control/estimate_utils.py",
        "type": "filepath"
    },
    "87": {
        "file_id": 6,
        "content": "Estimates time to process all files in a file list. Requires file list from a rule, potential .gitignore generation.",
        "type": "summary"
    },
    "88": {
        "file_id": 6,
        "content": "# estimate the time for processing all files under a file list.abs\n# you may need to obtain a file list from some rule.\n# is there anything for easy .gitignore generation?",
        "type": "code",
        "location": "/document_agi_computer_control/estimate_utils.py:1-3"
    },
    "89": {
        "file_id": 6,
        "content": "Estimates time to process all files in a file list. Requires file list from a rule, potential .gitignore generation.",
        "type": "comment"
    },
    "90": {
        "file_id": 7,
        "content": "/document_agi_computer_control/example.py",
        "type": "filepath"
    },
    "91": {
        "file_id": 7,
        "content": "Generates a \"hello world\" document and prints it.",
        "type": "summary"
    },
    "92": {
        "file_id": 7,
        "content": "# generate doc for me!\ndef hello_world():\n    print(\"hello world\")",
        "type": "code",
        "location": "/document_agi_computer_control/example.py:1-4"
    },
    "93": {
        "file_id": 7,
        "content": "Generates a \"hello world\" document and prints it.",
        "type": "comment"
    },
    "94": {
        "file_id": 8,
        "content": "/document_agi_computer_control/identify_utils.py",
        "type": "filepath"
    },
    "95": {
        "file_id": 8,
        "content": "This code imports a function from the identify module and uses it to determine the language of a given filename. If the filename contains \"text\", it will randomly select another candidate language from the identified tags if available.",
        "type": "summary"
    },
    "96": {
        "file_id": 8,
        "content": "from identify import identify\nfrom beartype import beartype\nimport random  # this is magic\nTEXT = \"text\"\n@beartype\ndef get_language_id_from_filename(filename: str) -> str:\n    language_id = TEXT # default language\n    tags = identify.tags_from_filename(filename)\n    if TEXT in tags:\n        candidates = [it for it in tags if it != TEXT]\n        if candidates:\n            language_id = random.choice(candidates)\n    return language_id\ndef test():\n    names = [\"test.bash\", \"test.py\", \"test.js\"]\n    for name in names:\n        language_id = get_language_id_from_filename(name)\n        print(f\"{name} -> {language_id}\")\nif __name__ == \"__main__\":\n    test()",
        "type": "code",
        "location": "/document_agi_computer_control/identify_utils.py:1-27"
    },
    "97": {
        "file_id": 8,
        "content": "This code imports a function from the identify module and uses it to determine the language of a given filename. If the filename contains \"text\", it will randomly select another candidate language from the identified tags if available.",
        "type": "comment"
    },
    "98": {
        "file_id": 9,
        "content": "/document_agi_computer_control/llm.py",
        "type": "filepath"
    },
    "99": {
        "file_id": 9,
        "content": "The code initializes an OpenAI language model, defines functions for executing chain and generating text, prints initialization parameters and query for debugging, and sets up LLM instance with optional temperature and GPT-4 parameters.",
        "type": "summary"
    }
}