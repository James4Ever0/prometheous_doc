{
    "100": {
        "file_id": 9,
        "content": "        print_center(\"init params\")\n        print(f\"Model: {self.model_name}\")\n        print(f\"Max Tokens: {self.max_tokens}\")\n        print(f\"Prompt Size: {self.prompt_size}\")\n        print(f\"Temperature: {self.temperature}\")\n        print_center(\"init config\")\n        print(self.prompt)\n    def run(self, query):\n        \"\"\"\n        Runs the Language Model Chain.\n        Args:\n            code (str): The code to use for the chain.\n            **kwargs (dict): Additional keyword arguments.\n        Returns:\n            str: The generated text.\n        \"\"\"\n        llm = OpenAI(\n            temperature=self.temperature,\n            max_tokens=-1,\n            model_name=self.model_name,\n            disallowed_special=(),  # to suppress error when special tokens within the input text (encode special tokens as normal text)\n        )\n        # chain = LLMChain(llm=llm, prompt=self.prompt)\n        chunk_list = []\n        print_center(\"query\")\n        print(query)\n        print_center(\"response\")\n        _input = \"\\n\".join([self.prompt, query])",
        "type": "code",
        "location": "/document_agi_computer_control/llm.py:37-65"
    },
    "101": {
        "file_id": 9,
        "content": "This code initializes an OpenAI language model with given parameters and defines a function `run` to execute the chain. The function takes a query as input, combines it with a prompt, and returns the generated text. The code also prints initialization parameters and the query for debugging purposes.",
        "type": "comment"
    },
    "102": {
        "file_id": 9,
        "content": "        for chunk in llm.stream(input=_input):\n            print(chunk, end=\"\", flush=True)\n            chunk_list.append(chunk)\n        print()\n        result = \"\".join(chunk_list)\n        return result\n    def number_of_tokens(self, text):\n        \"\"\"\n        Counts the number of tokens in a given text.\n        Args:\n            text (str): The text to count tokens for.\n        Returns:\n            int: The number of tokens in the text.\n        \"\"\"\n        encoding = tiktoken.encoding_for_model(\"gpt-4\")\n        return len(encoding.encode(text, disallowed_special=()))\n@contextmanager\ndef llm_context(prompt: str, temperature=0, gpt_4=False):\n    model = LLM(prompt, temperature=temperature, gpt_4=gpt_4)\n    try:\n        yield model\n    finally:\n        del model",
        "type": "code",
        "location": "/document_agi_computer_control/llm.py:66-92"
    },
    "103": {
        "file_id": 9,
        "content": "The code defines a function `llm` that streams the output of an LLM model while counting tokens, and a `number_of_tokens` method. The `llm_context` context manager sets up an LLM instance with optional temperature and GPT-4 parameters.",
        "type": "comment"
    },
    "104": {
        "file_id": 10,
        "content": "/document_agi_computer_control/path_select_utils.py",
        "type": "filepath"
    },
    "105": {
        "file_id": 10,
        "content": "This code selects paths under a directory for documentation purposes, works interactively with estimation utilities, caches results, and excludes processed files.",
        "type": "summary"
    },
    "106": {
        "file_id": 10,
        "content": "# select paths under a directory, for documentation purposes\n# shall be interactive, and work with estimation utils.\n# shall be cached. shall exclude processed files",
        "type": "code",
        "location": "/document_agi_computer_control/path_select_utils.py:1-3"
    },
    "107": {
        "file_id": 10,
        "content": "This code selects paths under a directory for documentation purposes, works interactively with estimation utilities, caches results, and excludes processed files.",
        "type": "comment"
    },
    "108": {
        "file_id": 11,
        "content": "/document_agi_computer_control/recursive_document_writer.py",
        "type": "filepath"
    },
    "109": {
        "file_id": 11,
        "content": "The code manages libraries, sets environment variables, and handles data processing and file management. It includes a `SearchIndexData` class for indexing and generates document webpages using Jinja2 templates. The directory is created, JSON files are processed, and the page is rendered recursively. A subprocess runs to view the page before exiting.",
        "type": "summary"
    },
    "110": {
        "file_id": 11,
        "content": "# os.environ[\"OPENAI_API_KEY\"] = \"any\"\n# os.environ[\"OPENAI_API_BASE\"] = \"http://0.0.0.0:8000\"\n# os.environ[\"BETTER_EXCEPTIONS\"] = \"1\"\nimport os\nfrom typing import Literal, Optional, Union\nimport uuid\nimport json\nfrom slice_utils import split_dict_into_chunks\nimport parse\nimport shutil\nimport custom_doc_writer\nCODE_LOCATION_FORMAT = '\"{code_path}\":{line_start:d}-{line_end:d}'\nDATA_SLICE_LENGTH = 100\nfrom beartype import beartype\nfrom cache_db_context import (\n    CacheContextManager,\n    CacheManager,\n    SourceIteratorAndTargetGeneratorParam,  # type:ignore\n    TargetGeneratorParameter,\n    iterate_source_dir_and_generate_to_target_dir,\n    read_file,\n    write_file,\n)\nfrom custom_doc_writer import (\n    construct_llm_and_write_code_comment,  # type:ignore\n    parse_arguments,\n)\nfrom identify_utils import get_language_id_from_filename\n@beartype\ndef dirpath_and_fpath_walker(dir_path: str):\n    for dirpath, _, filenames in os.walk(dir_path):\n        for filename in filenames:\n            fpath = os.path.join(dirpath, filename)",
        "type": "code",
        "location": "/document_agi_computer_control/recursive_document_writer.py:1-37"
    },
    "111": {
        "file_id": 11,
        "content": "This code imports various libraries and defines functions related to reading, writing, and processing files. It sets environment variables for OpenAI API, initializes a cache manager, and creates a file walker function for directories. The code also includes a beartype annotation for type hints, and a custom function for constructing LLM and writing code comments.",
        "type": "comment"
    },
    "112": {
        "file_id": 11,
        "content": "            yield dirpath, fpath\n@beartype\ndef get_source_iterator_and_target_generator_param_from_document_dir(\n    document_dir: str,\n    code_relpath: str = \"src\",\n    output_relpath: str = \"doc\",\n    db_relpath: str = \"cache_db.json\",\n):\n    source_dir_path = os.path.join(document_dir, code_relpath)\n    target_dir_path = os.path.join(document_dir, output_relpath)\n    db_path = os.path.join(document_dir, db_relpath)\n    param = SourceIteratorAndTargetGeneratorParam(\n        source_dir_path=source_dir_path,\n        target_dir_path=target_dir_path,\n        db_path=db_path,\n    )\n    return param\n@beartype\ndef generate_comment_path(param: TargetGeneratorParameter):\n    comment_rel_path = str(uuid.uuid4()) + \".json\"\n    comment_path = os.path.join(param.target_dir_path, comment_rel_path)\n    return comment_path\n@beartype\ndef scan_code_dir_and_write_to_comment_dir(document_dir: str):\n    param = get_source_iterator_and_target_generator_param_from_document_dir(\n        document_dir\n    )\n    iterate_source_dir_and_generate_to_target_dir(",
        "type": "code",
        "location": "/document_agi_computer_control/recursive_document_writer.py:38-72"
    },
    "113": {
        "file_id": 11,
        "content": "This code defines functions for generating a comment path, scanning a code directory, and writing the scan results to the generated comment directory. It utilizes relative paths and UUIDs for unique file names. The `get_source_iterator_and_target_generator_param_from_document_dir` function retrieves the source directory, target directory, and database path from a document directory, while the `generate_comment_path` function generates a unique comment file name relative to the target directory. Lastly, the `scan_code_dir_and_write_to_comment_dir` function initializes the parameter, iterates through the source directory, and generates the results to the corresponding comment directory.",
        "type": "comment"
    },
    "114": {
        "file_id": 11,
        "content": "        param,\n        dirpath_and_fpath_walker,\n        generate_comment_path,\n        construct_llm_and_write_code_comment,\n    )\n    return param\nfrom jinja2 import Environment, FileSystemLoader, Template\n@beartype\nclass SearchIndexData(dict):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.counter = 0\n        self.file_id: Optional[int] = None\n    def insert_filepath_and_summary(self, file_id: int, filepath: str, summary: str):\n        self.file_id = file_id\n        self.insert(\n            content=filepath,\n            type=\"filepath\",\n        )\n        self.insert(\n            content=summary,\n            type=\"summary\",\n        )\n    def insert_code_and_comment(self, code: str, comment: str, location: str):\n        self.insert(content=code, type=\"code\", location=location)\n        self.insert(\n            content=comment,\n            type=\"comment\",\n        )\n    def insert(\n        self,\n        content: str,\n        type: Literal[\"filepath\", \"summary\", \"comment\", \"code\"],\n        location: Optional[str] = None,",
        "type": "code",
        "location": "/document_agi_computer_control/recursive_document_writer.py:73-113"
    },
    "115": {
        "file_id": 11,
        "content": "The code defines a class `SearchIndexData` that extends the `dict` class. It has methods for inserting file paths, summaries, and code/comments along with their locations in the data dictionary. The `insert_filepath_and_summary`, `insert_code_and_comment`, and `insert` methods are provided to facilitate these operations.",
        "type": "comment"
    },
    "116": {
        "file_id": 11,
        "content": "    ):\n        assert isinstance(self.file_id, int)\n        self[self.counter] = dict(\n            file_id=self.file_id,\n            content=content,\n            type=type,\n            **(dict(location=location) if location else {}),\n        )\n        self.counter += 1\n@beartype\ndef render_document_webpage(\n    document_dir_path: str,\n    param: SourceIteratorAndTargetGeneratorParam,\n    repository_url: str,\n    template_dir: str = \".\",\n    template_filename: str = \"website_template.html.j2\",\n    output_filename: str = \"index.html\",\n    url_prefix: str = \"https://github.com/\",\n):\n    @beartype\n    def load_template() -> Template:\n        # Load the template from file\n        file_loader = FileSystemLoader(\n            template_dir\n        )  # Replace 'path_to_templates_directory' with the actual path\n        env = Environment(loader=file_loader)\n        template = env.get_template(\n            template_filename\n        )  # Replace 'sitemap_template.html' with the actual template file name\n        return template\n    @beartype",
        "type": "code",
        "location": "/document_agi_computer_control/recursive_document_writer.py:114-147"
    },
    "117": {
        "file_id": 11,
        "content": "The code defines a function `render_document_webpage` that takes various parameters such as document directory path, source iterator and target generator parameter, repository URL, template directory, template filename, output filename, and URL prefix. It uses the Jinja2 templating engine to load a template from a file and renders the document webpage based on the provided parameters.",
        "type": "comment"
    },
    "118": {
        "file_id": 11,
        "content": "    def write_to_output_path(content: str):\n        output_path = os.path.join(document_dir_path, output_filename)\n        write_file(output_path, content)\n    @beartype\n    def get_template_render_params() -> dict[str, Union[dict, str]]:\n        data = SearchIndexData()\n        file_mapping: dict[int, dict[str, Union[str, int]]] = {}\n        @beartype\n        def strip_path_prefix(path: str):\n            return path[len(param.source_dir_path) :]\n        @beartype\n        def strip_location(location: str):\n            result = parse.parse(CODE_LOCATION_FORMAT, location)\n            assert isinstance(result, parse.Result)\n            stripped_path = strip_path_prefix(result[\"code_path\"])\n            return f\"{stripped_path}:{result['line_start']+1}-{result['line_end']+1}\"\n        @beartype\n        def update_data_by_target_data(\n            target_data: dict, file_id: int, source_relative_path: str\n        ):\n            data.insert_filepath_and_summary(\n                file_id=file_id,\n                filepath=source_relative_path,",
        "type": "code",
        "location": "/document_agi_computer_control/recursive_document_writer.py:148-174"
    },
    "119": {
        "file_id": 11,
        "content": "This code defines several functions to handle data processing and file management. The `write_to_output_path` function writes content to the specified output path, `get_template_render_params` returns a dictionary of parameters for template rendering, `strip_path_prefix` removes a specific directory prefix from paths, `strip_location` parses code location format and returns stripped file paths with line numbers, and `update_data_by_target_data` updates the data object with file path and summary information.",
        "type": "comment"
    },
    "120": {
        "file_id": 11,
        "content": "                summary=target_data[\"summary\"],\n            )\n            for detail in target_data[\"details\"]:\n                data.insert_code_and_comment(\n                    code=detail[\"content\"],\n                    comment=detail[\"comment\"],\n                    location=strip_location(detail[\"location\"]),\n                )\n        @beartype\n        def update_data_and_file_mapping(\n            manager: CacheManager, record: dict, file_id: int, source_relative_path: str\n        ):\n            file_mapping[file_id] = dict(\n                filepath=source_relative_path,\n                entry_id=data.counter,\n                language_id=get_language_id_from_filename(source_relative_path),\n            )\n            target_path, _ = manager.get_record_target_path_and_hash(record)\n            target_data = json.loads(read_file(target_path))\n            update_data_by_target_data(target_data, file_id, source_relative_path)\n        def assemble_render_params():\n            partial_repository_url = repository_url.replace(url_prefix, \"\")",
        "type": "code",
        "location": "/document_agi_computer_control/recursive_document_writer.py:175-198"
    },
    "121": {
        "file_id": 11,
        "content": "The code defines a function update_data_and_file_mapping that takes in a CacheManager, record, file_id and source_relative_path as parameters. It adds a new entry to the file_mapping dictionary with the file id, source relative path, and language id derived from the filename. Then it retrieves target data from the record's target path and updates the data by calling update_data_by_target_data function. Lastly, it extracts partial repository URL and returns it. The code also defines assemble_render_params() function that extracts the partial repository URL from the repository URL by replacing the url_prefix with an empty string.",
        "type": "comment"
    },
    "122": {
        "file_id": 11,
        "content": "            render_params = dict(\n                datadict=data,\n                repository_url=repository_url,\n                file_mapping=file_mapping,\n                partial_repository_url=partial_repository_url,\n            )\n            return render_params\n        def iterate_source_dir_and_assemble_render_params():\n            # if only have one file, we should return one\n            with CacheContextManager(param.db_path) as manager:\n                for file_id, (_, source_path) in enumerate(\n                    dirpath_and_fpath_walker(param.source_dir_path)\n                ):\n                    source_relative_path = strip_path_prefix(source_path)\n                    record, _ = manager.get_record_by_computing_source_hash(source_path)\n                    if record:\n                        update_data_and_file_mapping(\n                            manager, record, file_id, source_relative_path\n                        )\n            return assemble_render_params()\n        return iterate_source_dir_and_assemble_render_params()",
        "type": "code",
        "location": "/document_agi_computer_control/recursive_document_writer.py:199-222"
    },
    "123": {
        "file_id": 11,
        "content": "This code defines a function that iterates through a source directory, retrieves file information from the database, and assembles render parameters. It checks if each file has corresponding records in the database and updates data and file mapping accordingly. Finally, it returns the assembled render parameters.",
        "type": "comment"
    },
    "124": {
        "file_id": 11,
        "content": "    def strip_quote(s: str):\n        s = s.strip()\n        if s[0] == s[-1]:\n            if s[0] in ['\"', \"'\"]:\n                return s[1:-1].strip()\n        return s.strip()\n    @beartype\n    def write_render_params(render_params: dict):\n        # TODO: mapping source file path to documentation json\n        # TODO: add mode of index to hide search bar and render single file left-right comparison only\n        datadict = render_params[\"datadict\"]\n        metadata = dict()\n        metadata[\"url\"] = dict(\n            full=render_params[\"repository_url\"],\n            partial=render_params[\"partial_repository_url\"],\n        )\n        metadata[\"file_mapping\"] = render_params[\"file_mapping\"]\n        metadata[\"project_name\"] = render_params[\"partial_repository_url\"].split(\"/\")[\n            -1\n        ]\n        split_count = 0\n        # datadict_split = {}\n        datadict = {\n            k: v\n            if (v[\"type\"] not in [\"comment\", \"summary\"])\n            else {\n                \"file_id\": v[\"file_id\"],\n                \"content\": strip_quote(v[\"content\"]),",
        "type": "code",
        "location": "/document_agi_computer_control/recursive_document_writer.py:224-252"
    },
    "125": {
        "file_id": 11,
        "content": "This code defines a function `strip_quote` that removes leading and trailing quotes from a string. The main function, `write_render_params`, takes a dictionary of render parameters and creates a metadata object containing the repository URL, file mapping, and project name. It then splits the data dictionary based on element type (excluding comments and summaries), and applies a strip quote transformation to the content of each item.",
        "type": "comment"
    },
    "126": {
        "file_id": 11,
        "content": "                \"type\": v[\"type\"],\n            }\n            for k, v in datadict.items()\n        }\n        data_dir = os.path.join(document_dir_path, \"data\")\n        if not os.path.exists(data_dir):\n            os.mkdir(data_dir)\n        for chunk in split_dict_into_chunks(datadict, DATA_SLICE_LENGTH):\n            write_file(\n                os.path.join(data_dir, f\"{split_count}.json\"),\n                json.dumps(chunk, indent=4, ensure_ascii=False),\n            )\n            split_count += 1\n        metadata[\"split_count\"] = split_count\n        write_file(\n            os.path.join(document_dir_path, \"metadata.json\"),\n            json.dumps(metadata, indent=4, ensure_ascii=False),\n        )\n    @beartype\n    def render_template(template: Template):\n        render_params = get_template_render_params()\n        write_render_params(render_params)\n        # do something else, like writing to files.\n        # ret = template.render(**render_params)\n        # return ret\n    def copy_static_pages():\n        script_base_dir = os.path.split(__file__)[0]",
        "type": "code",
        "location": "/document_agi_computer_control/recursive_document_writer.py:253-282"
    },
    "127": {
        "file_id": 11,
        "content": "This code creates a directory to store data and splits it into smaller chunks. It then writes each chunk to separate JSON files in the data directory, keeping track of the split count in a metadata file. The `render_template` function uses template rendering with parameters and potentially writes output to files. The `copy_static_pages` function copies static pages from a base script directory.",
        "type": "comment"
    },
    "128": {
        "file_id": 11,
        "content": "        static_pages_dir = os.path.join(script_base_dir, \"static_pages\")\n        for fname in os.listdir(static_pages_dir):\n            shutil.copy(os.path.join(static_pages_dir, fname), document_dir_path)\n    def write_gitignore():\n        with open(os.path.join(document_dir_path, \".gitignore\"), \"w+\") as f:\n            f.write(\"!.gitignore\\n!*\\n!*/*\\ncache_db.json\\ncache_tree.json\\n\")\n            # f.write(\"!.gitignore\\n!*\\n!*/*\\ncache_db.json\\n\")\n    def render_to_output_path():\n        template = load_template()\n        render_template(template)\n        copy_static_pages()\n        write_gitignore()\n        # content = render_template(template)\n        # write_to_output_path(content)\n    render_to_output_path()\nimport subprocess\ndef main():\n    (document_dir_path, repository_url) = parse_arguments()\n    project_name = repository_url.split(\"/\")[-1]\n    custom_doc_writer.CUSTOM_DOC_WRITER_PARAMS[\"location_prefix\"] = document_dir_path\n    custom_doc_writer.CUSTOM_DOC_WRITER_PARAMS[\"project_name\"] = project_name",
        "type": "code",
        "location": "/document_agi_computer_control/recursive_document_writer.py:283-310"
    },
    "129": {
        "file_id": 11,
        "content": "The code imports a module and defines functions for rendering templates to an output path. It sets custom document writer parameters, copies static pages, writes a gitignore file, and renders the template.",
        "type": "comment"
    },
    "130": {
        "file_id": 11,
        "content": "    param = scan_code_dir_and_write_to_comment_dir(document_dir_path)\n    # not done yet. we have to create the webpage.\n    render_document_webpage(document_dir_path, param, repository_url)\n    cli = f\"python3 -u tree_markdown_view_folder_hierarchy/main.py -s '{document_dir_path}'\"\n    # import time\n    print(cli)\n    excode = subprocess.check_call(cli, shell=True)\n    # time.sleep(10)\n    exit(excode)\nif __name__ == \"__main__\":\n    main()",
        "type": "code",
        "location": "/document_agi_computer_control/recursive_document_writer.py:311-323"
    },
    "131": {
        "file_id": 11,
        "content": "The code is creating a webpage using recursive document writing and then executing a subprocess to view the generated webpage in a separate application. It imports time but doesn't use it, and it exits with the exit code of the subprocess. The main function serves as the entry point for the script.",
        "type": "comment"
    },
    "132": {
        "file_id": 12,
        "content": "/document_agi_computer_control/slice_utils.py",
        "type": "filepath"
    },
    "133": {
        "file_id": 12,
        "content": "This code defines a function to split a dictionary into chunks of specified size using itertools and a generator function. The generator yields dictionaries, each containing at most `chunk_size` items.",
        "type": "summary"
    },
    "134": {
        "file_id": 12,
        "content": "from itertools import islice\nfrom beartype import beartype\n@beartype\ndef split_dict_into_chunks(dictionary:dict, chunk_size:int):\n    \"\"\"\n    Split a dictionary into chunks of specified size using itertools and a generator function.\n    Args:\n    - dictionary: The input dictionary to be split.\n    - chunk_size: The size of each chunk.\n    Returns:\n    - A generator that yields dictionaries, each containing at most `chunk_size` items.\n    \"\"\"\n    it = iter(dictionary.items())\n    while True:\n        chunk = dict(islice(it, chunk_size))\n        if not chunk:\n            break\n        yield chunk\ndef test():\n    # Example usage\n    input_dict = {str(i): i for i in range(1000)}  # Example input dictionary\n    chunked_dicts_generator = split_dict_into_chunks(input_dict, 100)  # Use the generator function to split the dictionary\n    # Iterate through the generator to get the chunks\n    for chunk in chunked_dicts_generator:\n        print(str(chunk)[:10]+\"...}\")\nif __name__ == \"__main__\":\n    test()",
        "type": "code",
        "location": "/document_agi_computer_control/slice_utils.py:1-31"
    },
    "135": {
        "file_id": 12,
        "content": "This code defines a function to split a dictionary into chunks of specified size using itertools and a generator function. The generator yields dictionaries, each containing at most `chunk_size` items.",
        "type": "comment"
    },
    "136": {
        "file_id": 13,
        "content": "/document_agi_computer_control/stdout_redirect_progress/main.py",
        "type": "filepath"
    },
    "137": {
        "file_id": 13,
        "content": "The code imports libraries, creates a progress bar class with logging widget for task status, and executes commands in unbuffered mode. It runs subprocesses asynchronously using asyncio, waits for their completion, retrieves return codes, and raises exceptions if non-zero.",
        "type": "summary"
    },
    "138": {
        "file_id": 13,
        "content": "# redirect stdout to some buffered output window, and show a progress bar below.\n# differentiate between \"cached\" file and \"processed\" file\n# you may retrieve \"cached\" file processing time from somewhere else.\n# if failed to retrieve stored processing time, use average one instead.\n# TODO: add this to recursive document generator.\n# TODO: before that, just use a simple timer for producing total processing time and count files, in size, count and lines.\nimport asyncio\nimport parse\nfrom textual.app import App, ComposeResult\nfrom textual.widgets import Log, ProgressBar\n# import textual\nfrom threading import Lock\nlock = Lock()\nINTERVAL = 0.1\nclass VisualIgnoreApp(App):\n    \"\"\"A Textual app to visualize\"\"\"\n    def __init__(self, error_container: list, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.mylog = Log()\n        self.prog = ProgressBar()\n        self.error_container = error_container\n        # self.prog.styles.width=\"100%\"\n        self.prog.styles.align_horizontal = \"center\"\n        self.prog.update(total=100, progress=0)",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main.py:1-33"
    },
    "139": {
        "file_id": 13,
        "content": "This code imports necessary libraries and defines a class called VisualIgnoreApp, which is an instance of Textual app. It initializes log and progress bar widgets and sets the progress to 0 initially.",
        "type": "comment"
    },
    "140": {
        "file_id": 13,
        "content": "    async def progress(self):\n        locked = lock.acquire(blocking=False)\n        if locked:\n            self.mylog.clear()\n            await main(self.mylog, self.prog, self.error_container)\n            await asyncio.sleep(2)\n            lock.release()\n    def compose(self) -> ComposeResult:\n        \"\"\"Create child widgets for the app.\"\"\"\n        return [self.mylog, self.prog]\n    def on_mount(self) -> None:\n        self.timer = self.set_interval(INTERVAL, self.progress)\n# mylog = textual.widgets.Log(max_lines = ...)\n# mybar = textual.widgets.ProgressBar(total=100, show_eta=...)\n# mylog.write()\n# TODO: run the document processor in a separate process.\n# TODO: parse the data received from the separate process, line by line.\n# TODO: if the data starts with something special, we would read and parse the whole line and update progress\n# this is sick.\n# cmd = [\"python3\", \"test.py\"]\n# cmd = [\"stdbuf\", \"-o0\", \"-e0\", \"bash\", \"-c\", \"python3 test.py 2>&1\"]\ncmd = [\"stdbuf\", \"-o0\", \"-e0\", \"python3\", \"test.py\"]\n# cmd = [\"bash\", \"-c\", \"python3 test.py 2>&1\"]",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main.py:35-62"
    },
    "141": {
        "file_id": 13,
        "content": "Code is creating a progress bar for displaying the status of a task being processed in a separate process. It uses a logging widget to output data received from the separate process and updates the progress based on the data received. The code also includes options for command execution with stdbuf for better error handling.",
        "type": "comment"
    },
    "142": {
        "file_id": 13,
        "content": "line_format = \"PROCESSING PROGRESS: {progress:d}%\"\ndef parse_line(line: str):\n    parsed = parse.parse(line_format, line)\n    if parsed:\n        return parsed[\"progress\"]\n    return None\nasync def read_stderr(proc, error_container):\n    while True:\n        mbyte = await proc.stderr.readline()  # type:ignore\n        # mbyte = await proc.stderr.read(1)  # type:ignore\n        error_container.append(mbyte.decode())\n        if mbyte == b\"\":\n            break\nasync def read_stdout(proc, mylog, prog):\n    # line_position = 0\n    # line_content = \"\"\n    # mtime = []\n    while True:\n        mbyte = await proc.stdout.readline()  # type:ignore\n        # mbyte = await proc.stdout.read(20)  # type:ignore\n        # mbyte = await proc.stdout.read(1)  # type:ignore\n        if mbyte == b\"\":\n            break\n        else:\n            line_content = mbyte.decode(\"utf-8\").rstrip()\n            # print(content)\n            mylog.write_line(line_content)\n            # mylog.refresh()\n            # continue\n            # if mbyte == b\"\\n\":",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main.py:64-99"
    },
    "143": {
        "file_id": 13,
        "content": "Code snippet is responsible for reading stdout and stderr streams from a process. It uses asynchronous reads to efficiently handle incoming data.\nIn the `read_stdout` function, it continuously reads lines from the stdout stream of the process, decodes them as UTF-8, trims trailing whitespace, and writes each line to a log. The loop breaks when no more data is received.\nThe `read_stderr` function follows similar logic for reading and handling data from the stderr stream.",
        "type": "comment"
    },
    "144": {
        "file_id": 13,
        "content": "            #     line_position = 0\n            #     # try to parse line content.\n            #     mylog.write(\"\\n\")\n            # mylog.write_line(line_content)\n            if line_content.startswith(\">>>> \"):\n                #     mtime.append(datetime.datetime.now())\n                mline = line_content[5:]\n                ret = parse_line(mline)\n                if ret is not None:\n                    steps = ret - prog.progress\n                    if steps > 0:\n                        prog.advance(steps)\n                mylog.write_line(\"parsed progress? \" + str(ret))\nasync def main(mylog, prog, error_container):\n    proc = await asyncio.create_subprocess_shell(\n        # proc = await asyncio.create_subprocess_exec(\n        # *cmd, stdout=asyncio.subprocess.PIPE\n        # UNBUFFERED FLAG: -u\n        \"bash -c 'python3 -u test_no_patch.py 2>&1'\",\n        stdout=asyncio.subprocess.PIPE,\n        stderr=asyncio.subprocess.PIPE\n        # \"python3 -u test_no_patch.py\", stdout=asyncio.subprocess.PIPE\n        # \"python3 test.py\", stdout=asyncio.subprocess.PIPE",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main.py:100-124"
    },
    "145": {
        "file_id": 13,
        "content": "The code is executing a subprocess using the `asyncio.create_subprocess_shell` or `asyncio.create_subprocess_exec` function with stdout and stderr piped to asyncio. It is likely that it is running a Python script in unbuffered mode, which means that the output will be written immediately without waiting for a full line of text to be complete. The code is also parsing the output for progress updates and updating a progress bar accordingly. If a progress update is found, it writes \"parsed progress?\" followed by the returned value to the log.",
        "type": "comment"
    },
    "146": {
        "file_id": 13,
        "content": "    )  # how to handle the stderr now? we may merge the altogether.\n    t1 = asyncio.create_task(read_stdout(proc, mylog, prog))\n    t2 = asyncio.create_task(read_stderr(proc, error_container))\n    # task1 = asyncio.create_task(read_stdout(proc, mylog))\n    # task2 = asyncio.create_task(read_stderr(proc))\n    await asyncio.gather(t1, t2)\n    # await asyncio.gather(task1, task2)\n    retcode = await proc.wait()\n    error_container.insert(0, retcode)\n    if retcode != 0:\n        print(f\"Error: subprocess returned {retcode}\")\n    else:\n        print(f\"Success: subprocess returned {retcode}\")\nif __name__ == \"__main__\":\n    error_container = []\n    app = VisualIgnoreApp(error_container)\n    app.run()\n    # breakpoint()\n    retcode = error_container[0]\n    if retcode != 0:\n        raise Exception(\n            \"\\n\".join(\n                [\"Error: subprocess returned\", str(retcode)] + error_container[1:]\n            )\n        )",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main.py:125-152"
    },
    "147": {
        "file_id": 13,
        "content": "This code runs a subprocess and handles its output and error streams asynchronously using asyncio. It waits for both the stdout and stderr tasks to complete, then retrieves the subprocess' return code. If the return code is not 0, it raises an exception with the error message.",
        "type": "comment"
    },
    "148": {
        "file_id": 14,
        "content": "/document_agi_computer_control/stdout_redirect_progress/main_once.py",
        "type": "filepath"
    },
    "149": {
        "file_id": 14,
        "content": "The code redirects stdout, handles cached and processed files, uses asyncio, initializes a progress bar class, tracks progress in separate process with stdbuf, and updates log file. It spawns subprocesses, redirects output to asyncio tasks, uses UNBUFFERED FLAG, prints success/error messages with execution time, displays remaining error text and total time taken if an error occurs.",
        "type": "summary"
    },
    "150": {
        "file_id": 14,
        "content": "# redirect stdout to some buffered output window, and show a progress bar below.\n# differentiate between \"cached\" file and \"processed\" file\n# you may retrieve \"cached\" file processing time from somewhere else.\n# if failed to retrieve stored processing time, use average one instead.\n# TODO: add this to recursive document generator.\n# TODO: before that, just use a simple timer for producing total processing time and count files, in size, count and lines.\nimport asyncio\nimport parse\nfrom textual.app import App, ComposeResult\nfrom textual.widgets import Log, ProgressBar\n# import textual\nfrom threading import Lock\nlock = Lock()\nINTERVAL = 0.1\nimport shutil\nimport textwrap\ndef wrap_text(text):\n    # Get the terminal width\n    terminal_width, _ = shutil.get_terminal_size()\n    tw = terminal_width - 8\n    if tw < 8:\n        tw = terminal_width\n    wrapped_text = textwrap.fill(text, width=tw)\n    return wrapped_text.rstrip()\nclass VisualIgnoreApp(App):\n    \"\"\"A Textual app to visualize\"\"\"\n    def __init__(self, error_container: list, program_args: list[str], *args, **kwargs):",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once.py:1-41"
    },
    "151": {
        "file_id": 14,
        "content": "This code is for redirecting stdout to a buffered output window, displaying a progress bar below. It distinguishes between \"cached\" and \"processed\" files, retrieves file processing time (either stored or average), and plans to integrate it with a recursive document generator. The code uses asyncio, parse, Log, ProgressBar, threading, and shutil modules, as well as Textual framework for creating a text-based user interface app.",
        "type": "comment"
    },
    "152": {
        "file_id": 14,
        "content": "        super().__init__(*args, **kwargs)\n        self.mylog = Log(max_lines=10000)\n        self.prog = ProgressBar()\n        self.program_args = program_args\n        self.error_container = error_container\n        # self.prog.styles.width=\"100%\"\n        self.prog.styles.align_horizontal = \"center\"\n        # self.prog.update(total=100, progress=0)\n    async def progress(self):\n        locked = lock.acquire(blocking=False)\n        if locked:\n            self.mylog.clear()\n            await main(self.mylog, self.prog, self.error_container, self.program_args)\n            self.exit()\n            # lock.release()\n    def compose(self) -> ComposeResult:\n        \"\"\"Create child widgets for the app.\"\"\"\n        return [self.mylog, self.prog]\n    def on_mount(self) -> None:\n        # await self.progress()\n        # self.exit()\n        self.timer = self.set_interval(INTERVAL, self.progress)\n# mylog = textual.widgets.Log(max_lines = ...)\n# mybar = textual.widgets.ProgressBar(total=100, show_eta=...)\n# mylog.write()\n# TODO: run the document processor in a separate process.",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once.py:42-73"
    },
    "153": {
        "file_id": 14,
        "content": "This code initializes a class with log, progress bar, and program arguments. It defines a 'progress' function to update the progress bar and logs, and a 'compose' method to create child widgets. The 'on_mount' method sets an interval to periodically call the 'progress' function. The TODO comment indicates that running the document processor in a separate process is planned for future implementation.",
        "type": "comment"
    },
    "154": {
        "file_id": 14,
        "content": "# TODO: parse the data received from the separate process, line by line.\n# TODO: if the data starts with something special, we would read and parse the whole line and update progress\n# this is sick.\n# cmd = [\"python3\", \"test.py\"]\n# cmd = [\"stdbuf\", \"-o0\", \"-e0\", \"bash\", \"-c\", \"python3 test.py 2>&1\"]\ncmd = [\"stdbuf\", \"-o0\", \"-e0\", \"python3\", \"test.py\"]\n# cmd = [\"bash\", \"-c\", \"python3 test.py 2>&1\"]\nline_format = \"PROCESSING PROGRESS: {progress:d}/{total:d}\"\ndef parse_line(line: str):\n    parsed = parse.parse(line_format, line)\n    if parsed:\n        return parsed[\"progress\"], parsed[\"total\"]\n    return None\nasync def read_stderr(proc, error_container):\n    while True:\n        mbyte = await proc.stderr.readline()  # type:ignore\n        # mbyte = await proc.stderr.read(1)  # type:ignore\n        error_container.append(mbyte.decode())\n        if mbyte == b\"\":\n            break\nasync def read_stdout(proc, mylog, prog):\n    # line_position = 0\n    # line_content = \"\"\n    # mtime = []\n    mtotal_count  =-1\n    while True:",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once.py:74-106"
    },
    "155": {
        "file_id": 14,
        "content": "This code defines a function to parse lines received from a separate process and reads the stderr and stdout of that process asynchronously. It uses \"stdbuf\" command to redirect stdout and stderr to file descriptors 1 and 2 respectively. The line format is defined as \"PROCESSING PROGRESS: {progress:d}/{total:d}\" and is used to parse the progress updates from the separate process.",
        "type": "comment"
    },
    "156": {
        "file_id": 14,
        "content": "        mbyte = await proc.stdout.readline()  # type:ignore\n        # mbyte = await proc.stdout.read(20)  # type:ignore\n        # mbyte = await proc.stdout.read(1)  # type:ignore\n        if mbyte == b\"\":\n            break\n        else:\n            line_content = mbyte.decode(\"utf-8\").rstrip()\n            # print(content)\n            mylog.write_line(wrap_text(line_content))\n            # mylog.refresh()\n            # continue\n            # if mbyte == b\"\\n\":\n            #     line_position = 0\n            #     # try to parse line content.\n            #     mylog.write(\"\\n\")\n            # mylog.write_line(line_content)\n            if line_content.startswith(\">>>> \"):\n                #     mtime.append(datetime.datetime.now())\n                mline = line_content[5:]\n                ret = parse_line(mline)\n                if ret is not None:\n                    ret_total, ret_prog = ret[1], ret[0]\n                    if ret_total != mtotal_count:\n                        mtotal_count = ret_total\n                        prog.update(total=ret_total, progress=ret_prog)",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once.py:107-131"
    },
    "157": {
        "file_id": 14,
        "content": "The code reads a line from the process's stdout, checks if it is an empty string to break the loop, and then decodes the line content. If not empty, it writes the content to mylog with text wrapping. It also handles cases where the line starts with \">>>>\" and updates progress accordingly by parsing the line using `parse_line()` function.",
        "type": "comment"
    },
    "158": {
        "file_id": 14,
        "content": "                        continue\n                    steps = ret_prog - prog.progress\n                    if steps > 0:\n                        prog.advance(steps)\n                    else:\n                        prog.update(total=ret_total, progress=ret_prog)\n                mylog.write_line(\"parsed progress? \" + str(ret))\nasync def main(mylog, prog, error_container, program_args):\n    # proc = await asyncio.create_subprocess_shell(\n    proc = await asyncio.create_subprocess_exec(\n        *program_args,  # stdout=asyncio.subprocess.PIPE\n        # UNBUFFERED FLAG: -u\n        # \"bash -c 'python3 -u test_no_patch.py 2>&1'\",\n        stdout=asyncio.subprocess.PIPE,\n        stderr=asyncio.subprocess.PIPE\n        # \"python3 -u test_no_patch.py\", stdout=asyncio.subprocess.PIPE\n        # \"python3 test.py\", stdout=asyncio.subprocess.PIPE\n    )  # how to handle the stderr now? we may merge the altogether.\n    t1 = asyncio.create_task(read_stdout(proc, mylog, prog))\n    t2 = asyncio.create_task(read_stderr(proc, error_container))",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once.py:132-153"
    },
    "159": {
        "file_id": 14,
        "content": "The code creates a subprocess with the specified command and redirects its stdout and stderr to separate asyncio tasks. The 'read_stdout' task updates a progress bar based on parsed lines, while the 'read_stderr' task handles errors. The UNBUFFERED FLAG (-u) is used to ensure immediate output from the subprocess.",
        "type": "comment"
    },
    "160": {
        "file_id": 14,
        "content": "    # task1 = asyncio.create_task(read_stdout(proc, mylog))\n    # task2 = asyncio.create_task(read_stderr(proc))\n    await asyncio.gather(t1, t2)\n    # await asyncio.gather(task1, task2)\n    retcode = await proc.wait()\n    error_container.insert(0, retcode)\n    if retcode != 0:\n        print(f\"Error: subprocess returned {retcode}\")\n    else:\n        print(f\"Success: subprocess returned {retcode}\")\nimport sys\nimport time\nimport humanize\nif __name__ == \"__main__\":\n    split_ind = sys.argv.index(\"--\")\n    args = sys.argv[split_ind + 1 :]\n    if \"python\" in args or \"python3\" in args:\n        assert \"-u\" in args, \"Python script must be run with -u flag (unbuffered)\"\n    error_container = []\n    app = VisualIgnoreApp(error_container, args)\n    start_time = time.time()\n    app.run()\n    end_time = time.time()\n    total_time = end_time - start_time\n    # breakpoint()\n    retcode = error_container[0]\n    if retcode != 0:\n        raise Exception(\n            \"\\n\".join(\n                [\"Error: subprocess returned\", str(retcode)]",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once.py:154-187"
    },
    "161": {
        "file_id": 14,
        "content": "This code is creating two asynchronous tasks, one for reading stdout and the other for reading stderr of a subprocess. It then awaits both tasks to complete and retrieves the return code from the subprocess. If the return code is not 0, it raises an exception with an error message. Finally, it calculates the total execution time of the script and prints a success or error message based on the return code. The script requires being run with the -u flag for unbuffered Python.",
        "type": "comment"
    },
    "162": {
        "file_id": 14,
        "content": "                + error_container[1:]\n                + [\"total time:\", humanize.naturaltime(total_time).split(\" ago\")[0]]\n            )\n        )\n    else:\n        print(\"exit successfully\")\n        print(\"total time:\", humanize.naturaltime(total_time).split(\" ago\")[0])",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once.py:188-194"
    },
    "163": {
        "file_id": 14,
        "content": "Code snippet checks if there is an error in the output. If there is, it adds a message to display the remaining error text and the total time taken. Otherwise, it simply prints the total time taken.",
        "type": "comment"
    },
    "164": {
        "file_id": 15,
        "content": "/document_agi_computer_control/stdout_redirect_progress/main_once_char_by_char.py",
        "type": "filepath"
    },
    "165": {
        "file_id": 15,
        "content": "The code is a progress bar text app that uses class variables and libraries for distinguishing cached and processed files. It logs updates, monitors output via stderr/stdout streams, and plans to integrate into a recursive document generator. It utilizes asyncio for subprocess running, handles stdout/stderr in separate tasks, checks return codes/errors, calculates total time taken, and displays it in a human-readable format upon successful execution.",
        "type": "summary"
    },
    "166": {
        "file_id": 15,
        "content": "# redirect stdout to some buffered output window, and show a progress bar below.\n# differentiate between \"cached\" file and \"processed\" file\n# you may retrieve \"cached\" file processing time from somewhere else.\n# if failed to retrieve stored processing time, use average one instead.\n# TODO: add this to recursive document generator.\n# TODO: before that, just use a simple timer for producing total processing time and count files, in size, count and lines.\nimport asyncio\nimport parse\nfrom textual.app import App, ComposeResult\nfrom textual.widgets import Log, ProgressBar\n# import textual\nfrom threading import Lock\nlock = Lock()\nINTERVAL = 0.1\nimport shutil\nimport textwrap\ndef wrap_text(text):\n    # Get the terminal width\n    terminal_width, _ = shutil.get_terminal_size()\n    tw = terminal_width - 8\n    if tw < 8:\n        tw = terminal_width\n    wrapped_text = textwrap.fill(text, width=tw)\n    return wrapped_text.rstrip()\nclass VisualIgnoreApp(App):\n    \"\"\"A Textual app to visualize\"\"\"\n    def __init__(self, error_container: list, program_args: list[str], *args, **kwargs):",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once_char_by_char.py:1-41"
    },
    "167": {
        "file_id": 15,
        "content": "This code is implementing a Textual app that redirects stdout to a buffered output window, displaying a progress bar. It differentiates between \"cached\" and \"processed\" files and retrieves the processing time from somewhere else or uses an average if not available. The code imports necessary libraries, defines a `VisualIgnoreApp` class, and includes some TODOs for future additions such as adding it to a recursive document generator and using a simple timer.",
        "type": "comment"
    },
    "168": {
        "file_id": 15,
        "content": "        super().__init__(*args, **kwargs)\n        self.mylog = Log(max_lines=10000)\n        self.prog = ProgressBar()\n        self.program_args = program_args\n        self.error_container = error_container\n        # self.prog.styles.width=\"100%\"\n        self.prog.styles.align_horizontal = \"center\"\n        # self.prog.update(total=100, progress=0)\n    async def progress(self):\n        locked = lock.acquire(blocking=False)\n        if locked:\n            self.mylog.clear()\n            await main(self.mylog, self.prog, self.error_container, self.program_args)\n            self.exit()\n            # lock.release()\n    def compose(self) -> ComposeResult:\n        \"\"\"Create child widgets for the app.\"\"\"\n        return [self.mylog, self.prog]\n    def on_mount(self) -> None:\n        # await self.progress()\n        # self.exit()\n        self.timer = self.set_interval(INTERVAL, self.progress)\n# mylog = textual.widgets.Log(max_lines = ...)\n# mybar = textual.widgets.ProgressBar(total=100, show_eta=...)\n# mylog.write()\n# TODO: run the document processor in a separate process.",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once_char_by_char.py:42-73"
    },
    "169": {
        "file_id": 15,
        "content": "This code initializes class variables and sets up a progress bar for monitoring the execution of a document processor. It also sets up a logger to clear its contents after a certain number of lines, and starts an interval-based function that updates the progress bar periodically until the task is complete. The comment indicates that in future, this should run in a separate process.",
        "type": "comment"
    },
    "170": {
        "file_id": 15,
        "content": "# TODO: parse the data received from the separate process, line by line.\n# TODO: if the data starts with something special, we would read and parse the whole line and update progress\n# this is sick.\n# cmd = [\"python3\", \"test.py\"]\n# cmd = [\"stdbuf\", \"-o0\", \"-e0\", \"bash\", \"-c\", \"python3 test.py 2>&1\"]\ncmd = [\"stdbuf\", \"-o0\", \"-e0\", \"python3\", \"test.py\"]\n# cmd = [\"bash\", \"-c\", \"python3 test.py 2>&1\"]\nline_format = \"PROCESSING PROGRESS: {progress:d}/{total:d}\"\ndef parse_line(line: str):\n    parsed = parse.parse(line_format, line)\n    if parsed:\n        return parsed[\"progress\"], parsed[\"total\"]\n    return None\nasync def read_stderr(proc, error_container):\n    while True:\n        # mbyte = await proc.stderr.readline()  # type:ignore\n        mbyte = await proc.stderr.read(100)  # type:ignore\n        error_container.append(mbyte)\n        if mbyte == b\"\":\n            break\nasync def read_stdout(proc, mylog, prog):\n    line_position = 0\n    line_content = \"\"\n    # mtime = []\n    init = False\n    while True:\n        # mbyte = await proc.stdout.readline()  # type:ignore",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once_char_by_char.py:74-107"
    },
    "171": {
        "file_id": 15,
        "content": "This code appears to be part of a larger system that involves running another process and monitoring its output. It uses the Python subprocess module to execute a separate process (presumably \"test.py\") and reads from its stdout and stderr streams.\n\nThe code defines a function, `parse_line`, which is used to parse lines from the stderr stream in a specific format. It also includes two asynchronous functions: `read_stderr` and `read_stdout`. The former reads data from the stderr stream until it encounters an empty byte string, while the latter reads data from the stdout stream, parsing each line using `parse_line` function.\n\nOverall, this code seems to be a part of a system that manages and monitors the output of another process in a structured manner.",
        "type": "comment"
    },
    "172": {
        "file_id": 15,
        "content": "        # mbyte = await proc.stdout.read(20)  # type:ignore\n        mbyte = await proc.stdout.read(1)  # type:ignore\n        if mbyte == b\"\":\n            break\n        else:\n            # line_content = mbyte.decode(\"utf-8\").rstrip()\n            # # print(content)\n            # mylog.write_line(wrap_text(line_content))\n            # mylog.refresh()\n            # continue\n            if mbyte == b\"\\n\":\n                line_position = 0\n                mylog.write(\"\\n\")\n                # try to parse line content.\n                if line_content.startswith(\">>>> \"):\n                    #     mtime.append(datetime.datetime.now())\n                    mline = line_content[5:]\n                    ret = parse_line(mline)\n                    if ret is not None:\n                        if not init:\n                            prog.update(total=ret[1], progress=0)\n                            init = True\n                        steps = ret[0] - prog.progress\n                        if steps > 0:\n                            prog.advance(steps)",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once_char_by_char.py:108-132"
    },
    "173": {
        "file_id": 15,
        "content": "Reading stdout one character at a time and checking for a newline to progress through the line. If a newline is found, reset line position and attempt to parse line content. If line starts with \">>>\" and there are no init values, update progress bar total value. Calculate steps needed to reach new progress value and advance progress if steps greater than zero.",
        "type": "comment"
    },
    "174": {
        "file_id": 15,
        "content": "                    mylog.write(\"parsed progress? \" + str(ret)+\"\\n\")\n                line_content = \"\"\n            else:\n                line_position +=1\n                line_content += mbyte.decode(\"utf-8\")\n                mylog.write( mbyte.decode(\"utf-8\"))\n            # mylog.write_line(line_content)\nasync def main(mylog, prog, error_container, program_args):\n    # proc = await asyncio.create_subprocess_shell(\n    proc = await asyncio.create_subprocess_exec(\n        *program_args,  # stdout=asyncio.subprocess.PIPE\n        # UNBUFFERED FLAG: -u\n        # \"bash -c 'python3 -u test_no_patch.py 2>&1'\",\n        stdout=asyncio.subprocess.PIPE,\n        stderr=asyncio.subprocess.PIPE\n        # \"python3 -u test_no_patch.py\", stdout=asyncio.subprocess.PIPE\n        # \"python3 test.py\", stdout=asyncio.subprocess.PIPE\n    )  # how to handle the stderr now? we may merge the altogether.\n    t1 = asyncio.create_task(read_stdout(proc, mylog, prog))\n    t2 = asyncio.create_task(read_stderr(proc, error_container))\n    # task1 = asyncio.create_task(read_stdout(proc, mylog))",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once_char_by_char.py:133-155"
    },
    "175": {
        "file_id": 15,
        "content": "The code creates a subprocess using `asyncio.create_subprocess_exec` and waits for its output. It uses separate tasks to handle stdout and stderr. The `main` function is an asynchronous function that takes a logger (mylog), progress object (prog), error container, and program arguments (program_args). It writes the parsed progress to the logger.",
        "type": "comment"
    },
    "176": {
        "file_id": 15,
        "content": "    # task2 = asyncio.create_task(read_stderr(proc))\n    await asyncio.gather(t1, t2)\n    # await asyncio.gather(task1, task2)\n    retcode = await proc.wait()\n    error_container.insert(0, retcode)\n    if retcode != 0:\n        print(f\"Error: subprocess returned {retcode}\")\n    else:\n        print(f\"Success: subprocess returned {retcode}\")\nimport sys\nimport time\nimport humanize\nif __name__ == \"__main__\":\n    split_ind = sys.argv.index(\"--\")\n    args = sys.argv[split_ind + 1 :]\n    if \"python\" in args or \"python3\" in args:\n        assert \"-u\" in args, \"Python script must be run with -u flag (unbuffered)\"\n    error_container = []\n    app = VisualIgnoreApp(error_container, args)\n    start_time = time.time()\n    app.run()\n    end_time = time.time()\n    total_time = end_time - start_time\n    # breakpoint()\n    retcode = error_container[0]\n    if retcode != 0:\n        error_info = b\"\\n\".join(error_container[1:])\n        sys.stderr.buffer.write(error_info)\n        raise Exception(\n            \"\\n\".join(\n                [\"Error: subprocess returned\", str(retcode)]",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once_char_by_char.py:156-190"
    },
    "177": {
        "file_id": 15,
        "content": "This code is part of a Python script that runs another subprocess and waits for its completion. It checks the return code of the subprocess and prints whether it was successful or encountered an error. If there's an error, it writes the error information to stderr. The code also includes time measurements and seems to be part of a larger application called VisualIgnoreApp.",
        "type": "comment"
    },
    "178": {
        "file_id": 15,
        "content": "                + [\"total time:\", humanize.naturaltime(total_time).split(\" ago\")[0]]\n            )\n        )\n    else:\n        print(\"exit successfully\")\n        print(\"total time:\", humanize.naturaltime(total_time).split(\" ago\")[0])",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once_char_by_char.py:191-196"
    },
    "179": {
        "file_id": 15,
        "content": "Code snippet calculates the total time taken by a process and prints it in a human-readable format. If the process exits successfully, it also displays \"exit successfully\" along with the total time taken.",
        "type": "comment"
    },
    "180": {
        "file_id": 16,
        "content": "/document_agi_computer_control/stdout_redirect_progress/test.py",
        "type": "filepath"
    },
    "181": {
        "file_id": 16,
        "content": "This code is overriding the built-in print function to default flush=True. It demonstrates progress bar printing and sleep functionality with a custom print function.",
        "type": "summary"
    },
    "182": {
        "file_id": 16,
        "content": "import builtins\nimport copy\nmyprint = copy.copy(builtins.print)\ndef custom_print(*args, **kwargs):\n    if \"flush\" not in kwargs:\n        kwargs[\"flush\"] = True\n    myprint(*args, **kwargs)\n# Override the built-in print function with the custom function\nbuiltins.print = custom_print\n# Now, when you use print, it will default to flush=True\nimport time\n# for _ in range(200):\n#     print(\">>>> PROCESSING PROGRESS: 30%\")\nprint(\"Hello, world\")\nSLEEP = 0.2\n# time.sleep(SLEEP)\nfor i in range(10000):\n    print(\n        f\">>>> PROCESSING PROGRESS: {i}%\"\n    )  # problem is here. how to set flush=True this as default?\n    print(\"hello world\")\n    time.sleep(SLEEP)",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/test.py:1-32"
    },
    "183": {
        "file_id": 16,
        "content": "This code is overriding the built-in print function to default flush=True. It demonstrates progress bar printing and sleep functionality with a custom print function.",
        "type": "comment"
    },
    "184": {
        "file_id": 17,
        "content": "/document_agi_computer_control/stdout_redirect_progress/test_no_patch.py",
        "type": "filepath"
    },
    "185": {
        "file_id": 17,
        "content": "This code simulates a progress bar with intermittent \"hello world\" printing, adjustable sleep time using SLEEP variable. The issue is setting flush=True as the default in print statements.",
        "type": "summary"
    },
    "186": {
        "file_id": 17,
        "content": "import time\n# for _ in range(200):\n#     print(\">>>> PROCESSING PROGRESS: 30%\")\nprint(\"Hello, world\")\nSLEEP = 0.1\n# SLEEP = 0.01\n# SLEEP = 1\n# time.sleep(SLEEP)\ntotal = 100\nfor i in range(total):\n    print(\n        f\">>>> PROCESSING PROGRESS: {i+1}/{total}\"\n    )  # problem is here. how to set flush=True this as default?\n    print(\"hello world\")\n    time.sleep(SLEEP)",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/test_no_patch.py:1-18"
    },
    "187": {
        "file_id": 17,
        "content": "This code simulates a progress bar with intermittent \"hello world\" printing, adjustable sleep time using SLEEP variable. The issue is setting flush=True as the default in print statements.",
        "type": "comment"
    },
    "188": {
        "file_id": 18,
        "content": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/high_level.py",
        "type": "filepath"
    },
    "189": {
        "file_id": 18,
        "content": "The code initializes a TinyDB, creates functions for brief generation based on summaries, generates hashes for summaries, and updates file and directory briefs using the generated hashes. It then iterates through file summaries and directories, updating their briefs if necessary.",
        "type": "summary"
    },
    "190": {
        "file_id": 18,
        "content": "import os\nimport hashlib\nfrom tinydb import TinyDB, Query\n# Initialize TinyDB\ndb = TinyDB('briefs_db.json')\ndef generate_file_summary_brief(filepath, summary):\n    # Generate a brief for the file based on its summary\n    # ...\ndef generate_directory_summary_brief(directory_path, children_briefs):\n    # Generate a brief for the directory based on its direct children's briefs\n    # ...\ndef hash_summary(summary):\n    # Generate a hash for the given summary\n    hash_object = hashlib.md5(summary.encode())\n    return hash_object.hexdigest()\ndef update_file_briefing(filepath, summary):\n    # Check if a matching briefing exists for the hash of the summary\n    # If not, update the briefing for the file\n    # ...\ndef update_directory_briefing(directory_path, children_briefs):\n    # Concatenate and sort the briefs of direct children before hashing\n    # Check if a matching briefing exists for the hash of the concatenated children briefs\n    # If not, update the briefing for the directory\n    # ...\n# Iterate through file summaries and update briefs",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/high_level.py:1-32"
    },
    "191": {
        "file_id": 18,
        "content": "This code initializes a TinyDB, defines functions to generate briefs for files and directories based on their summaries, generates hashes for summaries, updates file and directory briefings using the generated hashes. It then iterates through file summaries, updating briefs if necessary.",
        "type": "comment"
    },
    "192": {
        "file_id": 18,
        "content": "for filepath, summary in file_summaries.items():\n    update_file_briefing(filepath, summary)\n# Iterate through directories and their direct children to update briefs\nfor directory_path, children_briefs in directory_children_briefs.items():\n    update_directory_briefing(directory_path, children_briefs)",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/high_level.py:33-38"
    },
    "193": {
        "file_id": 18,
        "content": "Iterates through file summaries and updates briefs, followed by directories and their direct children's briefs.",
        "type": "comment"
    },
    "194": {
        "file_id": 19,
        "content": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main.py",
        "type": "filepath"
    },
    "195": {
        "file_id": 19,
        "content": "The Python script efficiently generates a Markdown-formatted, cacheable directory hierarchy with HTML styling and recursively produces concise directory summaries. The code uses Jinja2 templates and CSS to render the summary before copying it into the source directory upon completion.",
        "type": "summary"
    },
    "196": {
        "file_id": 19,
        "content": "# demo logic to generate filesystem hierarchy in markdown\n# TODO: show the total stage progress like [Stage 1/4], [Stage 2/4]\n# TODO: generate sitemap\n# TODO: modify all titles in all pages to contain full project name and project description (more informative titles)\n# TODO: print progress info during directory brief generation process\n# TODO: provide a brief view to file chunks.\n# TODO: provide an AST view (language specific) to file chunks.\n# TODO: make our prompt into json to formalize the input structure, and parse the output as json\n# language specific shall be built on language agnostic\nimport os\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-s\", \"--source_dir\", type=str, required=True)\nargs = parser.parse_args()\n# the only parameter.\nsource_dir = args.source_dir\nassert os.path.exists(source_dir)\nassert os.path.isdir(source_dir)\nassert os.path.isabs(source_dir)\nfrom collections import defaultdict\nimport json\nimport urllib.parse\nimport sys\nsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"../\"))",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main.py:1-34"
    },
    "197": {
        "file_id": 19,
        "content": "This Python script generates a filesystem hierarchy in markdown format. It requires the source directory as an argument and performs several tasks such as checking input, using collections and JSON libraries. There are also various TODO comments indicating potential future improvements or additions to the code.",
        "type": "comment"
    },
    "198": {
        "file_id": 19,
        "content": "from llm import llm_context\nmetadata = json.loads(open(os.path.join(source_dir, \"metadata.json\"), \"r\").read())\nfile_mapping = metadata[\"file_mapping\"]\nsplit_count = metadata[\"split_count\"]\nproject_name = metadata[\"project_name\"]\ndata = {}\nfor i in range(split_count):\n    new_data = json.loads(open(os.path.join(source_dir, f\"data/{i}.json\"), \"r\").read())\n    data.update(new_data)\ndef strip_quote(s: str):\n    if s[0] == s[-1]:\n        if s[0] in ['\"', \"'\"]:\n            return s[1:-1].strip()\n    return s.strip()\n# read metadata.json & data/*.json\n# create and read some cache_tree.json, which you may want to include in .gitignore\n# produce tree.json\n# copy tree.html\nimport hashlib\ndef hash_key(summary: str):\n    enc = summary.strip()\n    if enc:\n        # Generate a hash for the given summary\n        hash_object = hashlib.md5(enc.encode())\n        return hash_object.hexdigest()\nimport tinydb\ncache_tree = tinydb.TinyDB(os.path.join(source_dir, \"cache_tree.json\"))\ndef generate_file_summary_brief(filepath, summary):\n    # Generate a brief for the file based on its summary",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main.py:35-78"
    },
    "199": {
        "file_id": 19,
        "content": "Code reads metadata.json and data/*.json, creates and reads cache_tree.json, produces tree.json, and copies tree.html using llm_context from llm import, json manipulation, hashlib, and tinydb. Hash_key function generates a hash for the given summary and generate_file_summary_brief function generates briefs for files based on their summaries.",
        "type": "comment"
    }
}