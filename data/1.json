{
    "100": {
        "file_id": 9,
        "content": "        print_center(\"init params\")\n        print(f\"Model: {self.model_name}\")\n        print(f\"Max Tokens: {self.max_tokens}\")\n        print(f\"Prompt Size: {self.prompt_size}\")\n        print(f\"Temperature: {self.temperature}\")\n        print_center(\"init config\")\n        print(self.prompt)\n    def run(self, query):\n        \"\"\"\n        Runs the Language Model Chain.\n        Args:\n            code (str): The code to use for the chain.\n            **kwargs (dict): Additional keyword arguments.\n        Returns:\n            str: The generated text.\n        \"\"\"\n        llm = OpenAI(\n            temperature=self.temperature,\n            max_tokens=-1,\n            model_name=self.model_name,\n            disallowed_special=(),  # to suppress error when special tokens within the input text (encode special tokens as normal text)\n        )\n        # chain = LLMChain(llm=llm, prompt=self.prompt)\n        chunk_list = []\n        print_center(\"query\")\n        print(query)\n        print_center(\"response\")\n        _input = \"\\n\".join([self.prompt, query])",
        "type": "code",
        "location": "/document_agi_computer_control/llm.py:37-65"
    },
    "101": {
        "file_id": 9,
        "content": "This code initializes an OpenAI language model with given parameters and defines a function `run` to execute the chain. The function takes a query as input, combines it with a prompt, and returns the generated text. The code also prints initialization parameters and the query for debugging purposes.",
        "type": "comment"
    },
    "102": {
        "file_id": 9,
        "content": "        for chunk in llm.stream(input=_input):\n            print(chunk, end=\"\", flush=True)\n            chunk_list.append(chunk)\n        print()\n        result = \"\".join(chunk_list)\n        return result\n    def number_of_tokens(self, text):\n        \"\"\"\n        Counts the number of tokens in a given text.\n        Args:\n            text (str): The text to count tokens for.\n        Returns:\n            int: The number of tokens in the text.\n        \"\"\"\n        encoding = tiktoken.encoding_for_model(\"gpt-4\")\n        return len(encoding.encode(text, disallowed_special=()))\n@contextmanager\ndef llm_context(prompt: str, temperature=0, gpt_4=False):\n    model = LLM(prompt, temperature=temperature, gpt_4=gpt_4)\n    try:\n        yield model\n    finally:\n        del model",
        "type": "code",
        "location": "/document_agi_computer_control/llm.py:66-92"
    },
    "103": {
        "file_id": 9,
        "content": "The code defines a function `llm` that streams the output of an LLM model while counting tokens, and a `number_of_tokens` method. The `llm_context` context manager sets up an LLM instance with optional temperature and GPT-4 parameters.",
        "type": "comment"
    },
    "104": {
        "file_id": 10,
        "content": "/document_agi_computer_control/path_select_utils.py",
        "type": "filepath"
    },
    "105": {
        "file_id": 10,
        "content": "This code selects paths under a directory for documentation purposes, works interactively with estimation utilities, caches results, and excludes processed files.",
        "type": "summary"
    },
    "106": {
        "file_id": 10,
        "content": "# select paths under a directory, for documentation purposes\n# shall be interactive, and work with estimation utils.\n# shall be cached. shall exclude processed files",
        "type": "code",
        "location": "/document_agi_computer_control/path_select_utils.py:1-3"
    },
    "107": {
        "file_id": 10,
        "content": "This code selects paths under a directory for documentation purposes, works interactively with estimation utilities, caches results, and excludes processed files.",
        "type": "comment"
    },
    "108": {
        "file_id": 11,
        "content": "/document_agi_computer_control/recursive_document_writer.py",
        "type": "filepath"
    },
    "109": {
        "file_id": 11,
        "content": "This code uses OpenAI API, generates files, and writes comments for document management. It splits data into JSON files, tracks count, prepares projects for version control, and handles file operations.",
        "type": "summary"
    },
    "110": {
        "file_id": 11,
        "content": "# os.environ[\"OPENAI_API_KEY\"] = \"any\"\n# os.environ[\"OPENAI_API_BASE\"] = \"http://0.0.0.0:8000\"\n# os.environ[\"BETTER_EXCEPTIONS\"] = \"1\"\nimport os\nfrom typing import Literal, Optional, OrderedDict, Union\nimport uuid\nimport json\nfrom slice_utils import split_dict_into_chunks\nimport parse\nimport shutil\nimport custom_doc_writer\nCODE_LOCATION_FORMAT = '\"{code_path}\":{line_start:d}-{line_end:d}'\nDATA_SLICE_LENGTH = 100\nfrom beartype import beartype\nfrom cache_db_context import (\n    CacheContextManager,\n    CacheManager,\n    SourceIteratorAndTargetGeneratorParam,  # type:ignore\n    TargetGeneratorParameter,\n    iterate_source_dir_and_generate_to_target_dir,\n    read_file,\n    write_file,\n)\nfrom custom_doc_writer import (\n    construct_llm_and_write_code_comment,  # type:ignore\n    parse_arguments,\n)\nfrom identify_utils import get_language_id_from_filename\n@beartype\ndef dirpath_and_fpath_walker(dir_path: str):\n    for dirpath, _, filenames in os.walk(dir_path):\n        for filename in filenames:\n            fpath = os.path.join(dirpath, filename)",
        "type": "code",
        "location": "/document_agi_computer_control/recursive_document_writer.py:1-37"
    },
    "111": {
        "file_id": 11,
        "content": "The code imports necessary libraries and defines constants for OpenAI API, initializes cache management, and defines a function to walk through directories and files. This function is likely used for processing documents and writing custom LLM code comments.",
        "type": "comment"
    },
    "112": {
        "file_id": 11,
        "content": "            yield dirpath, fpath\n@beartype\ndef get_source_iterator_and_target_generator_param_from_document_dir(\n    document_dir: str,\n    code_relpath: str = \"src\",\n    output_relpath: str = \"doc\",\n    db_relpath: str = \"cache_db.json\",\n):\n    source_dir_path = os.path.join(document_dir, code_relpath)\n    target_dir_path = os.path.join(document_dir, output_relpath)\n    db_path = os.path.join(document_dir, db_relpath)\n    param = SourceIteratorAndTargetGeneratorParam(\n        source_dir_path=source_dir_path,\n        target_dir_path=target_dir_path,\n        db_path=db_path,\n    )\n    return param\n@beartype\ndef generate_comment_path(param: TargetGeneratorParameter):\n    comment_rel_path = str(uuid.uuid4()) + \".json\"\n    comment_path = os.path.join(param.target_dir_path, comment_rel_path)\n    return comment_path\n@beartype\ndef scan_code_dir_and_write_to_comment_dir(document_dir: str):\n    param = get_source_iterator_and_target_generator_param_from_document_dir(\n        document_dir\n    )\n    iterate_source_dir_and_generate_to_target_dir(",
        "type": "code",
        "location": "/document_agi_computer_control/recursive_document_writer.py:38-72"
    },
    "113": {
        "file_id": 11,
        "content": "This code defines functions for scanning a source directory, generating and writing to target directories, and returning an iterator to iterate over the generated files. The `get_source_iterator_and_target_generator_param_from_document_dir` function creates path parameters from a document directory, while the `generate_comment_path` function generates and returns a comment file path for writing. The `scan_code_dir_and_write_to_comment_dir` function uses these functions to scan a code directory and write generated files to a comment directory.",
        "type": "comment"
    },
    "114": {
        "file_id": 11,
        "content": "        param,\n        dirpath_and_fpath_walker,\n        generate_comment_path,\n        construct_llm_and_write_code_comment,\n    )\n    return param\nfrom jinja2 import Environment, FileSystemLoader, Template\n@beartype\nclass SearchIndexData(dict):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.counter = 0\n        self.file_id: Optional[int] = None\n    def insert_filepath_and_summary(self, file_id: int, filepath: str, summary: str):\n        self.file_id = file_id\n        self.insert(\n            content=filepath,\n            type=\"filepath\",\n        )\n        self.insert(\n            content=summary,\n            type=\"summary\",\n        )\n    def insert_code_and_comment(self, code: str, comment: str, location: str):\n        self.insert(content=code, type=\"code\", location=location)\n        self.insert(\n            content=comment,\n            type=\"comment\",\n        )\n    def insert(\n        self,\n        content: str,\n        type: Literal[\"filepath\", \"summary\", \"comment\", \"code\"],\n        location: Optional[str] = None,",
        "type": "code",
        "location": "/document_agi_computer_control/recursive_document_writer.py:73-113"
    },
    "115": {
        "file_id": 11,
        "content": "This code defines a class called SearchIndexData that inherits from dict and has methods to insert file paths, summaries, and code with comments. It also uses Jinja2 for template rendering.",
        "type": "comment"
    },
    "116": {
        "file_id": 11,
        "content": "    ):\n        assert isinstance(self.file_id, int)\n        self[self.counter] = dict(\n            file_id=self.file_id,\n            content=content,\n            type=type,\n            **(dict(location=location) if location else {}),\n        )\n        self.counter += 1\n@beartype\ndef render_document_webpage(\n    document_dir_path: str,\n    param: SourceIteratorAndTargetGeneratorParam,\n    repository_url: str,\n    template_dir: str = \".\",\n    template_filename: str = \"website_template.html.j2\",\n    output_filename: str = \"index.html\",\n    url_prefix: str = \"https://github.com/\",\n):\n    @beartype\n    def load_template() -> Template:\n        # Load the template from file\n        file_loader = FileSystemLoader(\n            template_dir\n        )  # Replace 'path_to_templates_directory' with the actual path\n        env = Environment(loader=file_loader)\n        template = env.get_template(\n            template_filename\n        )  # Replace 'sitemap_template.html' with the actual template file name\n        return template\n    @beartype",
        "type": "code",
        "location": "/document_agi_computer_control/recursive_document_writer.py:114-147"
    },
    "117": {
        "file_id": 11,
        "content": "Function defines a recursive document writer that takes file ID, content, type, and optional location as input, and adds it to the dictionary at the current counter position. The counter is then incremented. Function renders a webpage by loading a template from a specified directory, replacing placeholders with actual data, and saving the result under an output filename. Template URL prefix is also configurable.",
        "type": "comment"
    },
    "118": {
        "file_id": 11,
        "content": "    def write_to_output_path(content: str):\n        output_path = os.path.join(document_dir_path, output_filename)\n        write_file(output_path, content)\n    @beartype\n    def get_template_render_params() -> dict[str, Union[dict, str]]:\n        data = SearchIndexData()\n        file_mapping: dict[int, dict[str, Union[str, int]]] = {}\n        @beartype\n        def strip_path_prefix(path: str):\n            return path[len(param.source_dir_path) :]\n        @beartype\n        def strip_location(location: str):\n            result = parse.parse(CODE_LOCATION_FORMAT, location)\n            assert isinstance(result, parse.Result)\n            stripped_path = strip_path_prefix(result[\"code_path\"])\n            return f\"{stripped_path}:{result['line_start']+1}-{result['line_end']+1}\"\n        @beartype\n        def update_data_by_target_data(\n            target_data: dict, file_id: int, source_relative_path: str\n        ):\n            data.insert_filepath_and_summary(\n                file_id=file_id,\n                filepath=source_relative_path,",
        "type": "code",
        "location": "/document_agi_computer_control/recursive_document_writer.py:148-174"
    },
    "119": {
        "file_id": 11,
        "content": "This code defines a function `write_to_output_path` that writes content to an output path. It also uses decorators `@beartype` for defining helper functions: `strip_path_prefix`, `strip_location`, and `update_data_by_target_data`. These functions are used to extract file path, location, and update data with the target data respectively. The code seems to be part of a larger program that deals with document writing and managing search index data.",
        "type": "comment"
    },
    "120": {
        "file_id": 11,
        "content": "                summary=target_data[\"summary\"],\n            )\n            for detail in target_data[\"details\"]:\n                data.insert_code_and_comment(\n                    code=detail[\"content\"],\n                    comment=detail[\"comment\"],\n                    location=strip_location(detail[\"location\"]),\n                )\n        @beartype\n        def update_data_and_file_mapping(\n            manager: CacheManager, record: dict, file_id: int, source_relative_path: str\n        ):\n            file_mapping[file_id] = dict(\n                filepath=source_relative_path,\n                entry_id=data.counter,\n                language_id=get_language_id_from_filename(source_relative_path),\n            )\n            target_path, _ = manager.get_record_target_path_and_hash(record)\n            target_data = json.loads(read_file(target_path))\n            update_data_by_target_data(target_data, file_id, source_relative_path)\n        def assemble_render_params():\n            partial_repository_url = repository_url.replace(url_prefix, \"\")",
        "type": "code",
        "location": "/document_agi_computer_control/recursive_document_writer.py:175-198"
    },
    "121": {
        "file_id": 11,
        "content": "This code appears to be part of a larger system that deals with document and file management. It includes functions for updating data, inserting code and comment into an existing document, and assembling render parameters. The repository URL is being manipulated to extract a partial path. More context would be helpful in providing a more accurate description of the overall functionality.",
        "type": "comment"
    },
    "122": {
        "file_id": 11,
        "content": "            render_params = dict(\n                datadict=data,\n                repository_url=repository_url,\n                file_mapping=file_mapping,\n                partial_repository_url=partial_repository_url,\n            )\n            return render_params\n        def iterate_source_dir_and_assemble_render_params():\n            # if only have one file, we should return one\n            with CacheContextManager(param.db_path) as manager:\n                source_path_list = [\n                    sp for _, sp in dirpath_and_fpath_walker(param.source_dir_path)\n                ]\n                source_path_list.sort()  # to reduce git folder size\n                for file_id, source_path in enumerate(source_path_list):\n                    source_relative_path = strip_path_prefix(source_path)\n                    record, _ = manager.get_record_by_computing_source_hash(source_path)\n                    if record:\n                        update_data_and_file_mapping(\n                            manager, record, file_id, source_relative_path",
        "type": "code",
        "location": "/document_agi_computer_control/recursive_document_writer.py:199-219"
    },
    "123": {
        "file_id": 11,
        "content": "The code defines a function `iterate_source_dir_and_assemble_render_params()` that iterates through the source directory, retrieves records from the database based on file hashes, and assembles render parameters for each file. It sorts the path list to reduce the size of Git folders and updates data and file mapping using a `CacheContextManager` object.",
        "type": "comment"
    },
    "124": {
        "file_id": 11,
        "content": "                        )\n            return assemble_render_params()\n        return iterate_source_dir_and_assemble_render_params()\n    def strip_quote(s: str):\n        s = s.strip()\n        if s[0] == s[-1]:\n            if s[0] in ['\"', \"'\"]:\n                return s[1:-1].strip()\n        return s.strip()\n    @beartype\n    def write_render_params(render_params: dict):\n        # TODO: mapping source file path to documentation json\n        # TODO: add mode of index to hide search bar and render single file left-right comparison only\n        datadict = render_params[\"datadict\"]\n        metadata = dict()\n        metadata[\"url\"] = dict(\n            full=render_params[\"repository_url\"],\n            partial=render_params[\"partial_repository_url\"],\n        )\n        metadata[\"file_mapping\"] = render_params[\"file_mapping\"]\n        metadata[\"project_name\"] = render_params[\"partial_repository_url\"].split(\"/\")[\n            -1\n        ]\n        split_count = 0\n        # datadict_split = {}\n        datadict = {\n            k: v",
        "type": "code",
        "location": "/document_agi_computer_control/recursive_document_writer.py:220-250"
    },
    "125": {
        "file_id": 11,
        "content": "This code is a part of a larger program that seems to be responsible for generating documentation from source files. It includes a function called `strip_quote` which removes leading and trailing quotes from a string, and another function named `write_render_params`. This latter function takes a dictionary called `render_params`, likely containing the necessary data for rendering documentation. The code also contains variables used to store metadata such as URLs and file mappings.",
        "type": "comment"
    },
    "126": {
        "file_id": 11,
        "content": "            if (v[\"type\"] not in [\"comment\", \"summary\"])\n            else {\n                \"file_id\": v[\"file_id\"],\n                \"content\": strip_quote(v[\"content\"]),\n                \"type\": v[\"type\"],\n            }\n            for k, v in datadict.items()\n        }\n        data_dir = os.path.join(document_dir_path, \"data\")\n        if not os.path.exists(data_dir):\n            os.mkdir(data_dir)\n        for chunk in split_dict_into_chunks(datadict, DATA_SLICE_LENGTH):\n            write_file(\n                os.path.join(data_dir, f\"{split_count}.json\"),\n                json.dumps(chunk, indent=4, ensure_ascii=False),\n            )\n            split_count += 1\n        metadata[\"split_count\"] = split_count\n        write_file(\n            os.path.join(document_dir_path, \"metadata.json\"),\n            json.dumps(metadata, indent=4, ensure_ascii=False),\n        )\n    @beartype\n    def render_template(template: Template):\n        render_params = get_template_render_params()\n        write_render_params(render_params)",
        "type": "code",
        "location": "/document_agi_computer_control/recursive_document_writer.py:251-279"
    },
    "127": {
        "file_id": 11,
        "content": "This code writes document data into separate JSON files, splitting them based on a specified length (DATA_SLICE_LENGTH) and keeps track of the split count in a metadata file. It also defines a function called 'render_template' which gets template render parameters and writes them.",
        "type": "comment"
    },
    "128": {
        "file_id": 11,
        "content": "        # do something else, like writing to files.\n        # ret = template.render(**render_params)\n        # return ret\n    def copy_static_pages():\n        script_base_dir = os.path.split(__file__)[0]\n        static_pages_dir = os.path.join(script_base_dir, \"static_pages\")\n        for fname in [\"index.html\", \"codeview.html\"]:\n            # for fname in os.listdir(static_pages_dir):\n            shutil.copy(os.path.join(static_pages_dir, fname), document_dir_path)\n    def write_gitignore():\n        with open(os.path.join(document_dir_path, \".gitignore\"), \"w+\") as f:\n            f.write(\"!.gitignore\\n!*\\n!*/*\\ncache_db.json\\ncache_tree.json\\n\")\n            # f.write(\"!.gitignore\\n!*\\n!*/*\\ncache_db.json\\n\")\n    def render_to_output_path():\n        template = load_template()\n        render_template(template)\n        copy_static_pages()\n        write_gitignore()\n        # content = render_template(template)\n        # write_to_output_path(content)\n    render_to_output_path()\nimport subprocess\ndef run_subprocess(cli: str):",
        "type": "code",
        "location": "/document_agi_computer_control/recursive_document_writer.py:280-310"
    },
    "129": {
        "file_id": 11,
        "content": "The code defines three functions: `copy_static_pages`, `write_gitignore`, and `render_to_output_path`. The `render_to_output_path` function loads a template, renders it, copies static pages files, writes a .gitignore file, and then executes the `run_subprocess` function with an argument. This code seems to be preparing a project for version control by copying static files, creating a gitignore file, and potentially rendering templates before running a subprocess command.",
        "type": "comment"
    },
    "130": {
        "file_id": 11,
        "content": "    print(\"running:\", cli)\n    excode = subprocess.check_call(cli, shell=True)\n    if excode != 0:\n        exit(excode)\ndef main():\n    (document_dir_path, repository_url) = parse_arguments()\n    project_name = repository_url.split(\"/\")[-1]\n    custom_doc_writer.CUSTOM_DOC_WRITER_PARAMS[\"location_prefix\"] = document_dir_path\n    custom_doc_writer.CUSTOM_DOC_WRITER_PARAMS[\"project_name\"] = project_name\n    param = scan_code_dir_and_write_to_comment_dir(document_dir_path)\n    # not done yet. we have to create the webpage.\n    render_document_webpage(document_dir_path, param, repository_url)\n    run_subprocess(\n        f\"python3 -u tree_markdown_view_folder_hierarchy/main_recursive.py -s '{document_dir_path}'\"\n    )\n    run_subprocess(f\"python3 -u title_generator/main.py -s '{document_dir_path}'\")\n    run_subprocess(f\"python3 -u sitemap_generator/main.py -s '{document_dir_path}'\")\nif __name__ == \"__main__\":\n    main()",
        "type": "code",
        "location": "/document_agi_computer_control/recursive_document_writer.py:311-333"
    },
    "131": {
        "file_id": 11,
        "content": "This code takes a document directory path and repository URL as arguments, initializes custom writer parameters with location prefix and project name, scans the code directory and writes to the comment directory. It then generates a webpage using these parameters, and runs subprocesses for generating tree view, title generator, and sitemap generator for the specified document directory.",
        "type": "comment"
    },
    "132": {
        "file_id": 12,
        "content": "/document_agi_computer_control/sitemap_generator/main.py",
        "type": "filepath"
    },
    "133": {
        "file_id": 12,
        "content": "This code imports libraries for a sitemap generator, sets arguments such as source directory, template paths, and base URL, loads metadata from JSON, and generates sitemap lines using urllib. It also takes items with timestamps and values, renders them into templates, and writes the output to a file.",
        "type": "summary"
    },
    "134": {
        "file_id": 12,
        "content": "import os\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-s\", \"--source_dir\", type=str, required=True)\nargs = parser.parse_args()\n# the only parameter.\nsource_dir = args.source_dir\nassert os.path.exists(source_dir)\nassert os.path.isdir(source_dir)\nassert os.path.isabs(source_dir)\nrepo_name = os.path.split(source_dir)[1]\nif repo_name == \"docs\":\n    repo_name = os.path.split(os.path.dirname(source_dir))[1]\ntemplate_path = os.path.join(os.path.dirname(__file__), \"sitemap.xml.j2\")\n# template_path = \"sitemap_template.html.j2\"\noutput_file_path = os.path.join(source_dir,\"sitemap.xml\")\nbase_url = f\"https://james4ever0.github.io/{repo_name}\"\nfrom jinja2 import Template\ntemplate = Template(open(template_path, \"r\").read())\nimport json\nmetadata = json.loads(open(os.path.join(source_dir, \"metadata.json\"), \"r\").read())\nfile_mapping = metadata[\"file_mapping\"]\n# split_count = metadata[\"split_count\"]\n# project_name = metadata[\"project_name\"]\nimport urllib.parse\nlines = [\n    f\"{base_url}?q={urllib.parse.quote(comp['filepath'])}\" for comp in file_mapping.values()",
        "type": "code",
        "location": "/document_agi_computer_control/sitemap_generator/main.py:1-35"
    },
    "135": {
        "file_id": 12,
        "content": "This code imports necessary libraries and sets up arguments for a sitemap generator. It checks the source directory, retrieves template paths, defines output file path, sets base URL, loads metadata from JSON, and generates sitemap lines using urllib.",
        "type": "comment"
    },
    "136": {
        "file_id": 12,
        "content": "]\n# Data to be rendered\ndatalist = [(item, \"2023-12-28T09:21:02+00:00\", \"1.00\") for item in lines]\n# Render the template with the data\nrendered_template = template.render(datalist=datalist)\n# Write the rendered output to a file\nwith open(output_file_path, \"w\") as output_file:\n    output_file.write(rendered_template)\nprint(\"Template rendered and written to file successfully.\")",
        "type": "code",
        "location": "/document_agi_computer_control/sitemap_generator/main.py:36-47"
    },
    "137": {
        "file_id": 12,
        "content": "This code takes a list of items, along with timestamps and values, renders them into a template, then writes the output to a file.",
        "type": "comment"
    },
    "138": {
        "file_id": 13,
        "content": "/document_agi_computer_control/slice_utils.py",
        "type": "filepath"
    },
    "139": {
        "file_id": 13,
        "content": "This code defines a function `split_dict_into_chunks` that takes a dictionary and chunk size as input, returning a generator yielding dictionaries containing at most specified chunk size. It uses an OrderedDict, itertools, and a generator to achieve this. The provided test function demonstrates how to use the split_dict_into_chunks function with an example dictionary and chunk size.",
        "type": "summary"
    },
    "140": {
        "file_id": 13,
        "content": "from itertools import islice\nfrom typing import OrderedDict\nfrom beartype import beartype\n@beartype\ndef split_dict_into_chunks(dictionary:dict, chunk_size:int):\n    \"\"\"\n    Split a dictionary into chunks of specified size using itertools and a generator function.\n    Args:\n    - dictionary: The input dictionary to be split.\n    - chunk_size: The size of each chunk.\n    Returns:\n    - A generator that yields dictionaries, each containing at most `chunk_size` items.\n    \"\"\"\n    myord = OrderedDict()\n    dictkeys = list(dictionary.keys())\n    dictkeys.sort()\n    for k in dictkeys:\n        myord[k] = dictionary[k]\n    it = iter(myord.items())\n    while True:\n        chunk = dict(islice(it, chunk_size))\n        if not chunk:\n            break\n        yield chunk\ndef test():\n    # Example usage\n    input_dict = {str(i): i for i in range(1000)}  # Example input dictionary\n    chunked_dicts_generator = split_dict_into_chunks(input_dict, 100)  # Use the generator function to split the dictionary\n    # Iterate through the generator to get the chunks",
        "type": "code",
        "location": "/document_agi_computer_control/slice_utils.py:1-32"
    },
    "141": {
        "file_id": 13,
        "content": "This code defines a function `split_dict_into_chunks` that takes a dictionary and chunk size as input and returns a generator that yields dictionaries containing at most the specified chunk size. It uses an OrderedDict, itertools, and a generator to achieve this. The provided test function demonstrates how to use the split_dict_into_chunks function with an example dictionary and chunk size.",
        "type": "comment"
    },
    "142": {
        "file_id": 13,
        "content": "    for chunk in chunked_dicts_generator:\n        print(str(chunk)[:10]+\"...}\")\nif __name__ == \"__main__\":\n    test()",
        "type": "code",
        "location": "/document_agi_computer_control/slice_utils.py:33-37"
    },
    "143": {
        "file_id": 13,
        "content": "The code uses a generator to iterate through chunked dictionaries and prints the first 10 characters of each dictionary, followed by \"...}\" until all chunks are processed. This function is executed only when the script is run directly, not imported as a module.",
        "type": "comment"
    },
    "144": {
        "file_id": 14,
        "content": "/document_agi_computer_control/stdout_redirect_progress/main.py",
        "type": "filepath"
    },
    "145": {
        "file_id": 14,
        "content": "The code imports libraries, creates a progress bar class with logging widget for task status, and executes commands in unbuffered mode. It runs subprocesses asynchronously using asyncio, waits for their completion, retrieves return codes, and raises exceptions if non-zero.",
        "type": "summary"
    },
    "146": {
        "file_id": 14,
        "content": "# redirect stdout to some buffered output window, and show a progress bar below.\n# differentiate between \"cached\" file and \"processed\" file\n# you may retrieve \"cached\" file processing time from somewhere else.\n# if failed to retrieve stored processing time, use average one instead.\n# TODO: add this to recursive document generator.\n# TODO: before that, just use a simple timer for producing total processing time and count files, in size, count and lines.\nimport asyncio\nimport parse\nfrom textual.app import App, ComposeResult\nfrom textual.widgets import Log, ProgressBar\n# import textual\nfrom threading import Lock\nlock = Lock()\nINTERVAL = 0.1\nclass VisualIgnoreApp(App):\n    \"\"\"A Textual app to visualize\"\"\"\n    def __init__(self, error_container: list, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.mylog = Log()\n        self.prog = ProgressBar()\n        self.error_container = error_container\n        # self.prog.styles.width=\"100%\"\n        self.prog.styles.align_horizontal = \"center\"\n        self.prog.update(total=100, progress=0)",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main.py:1-33"
    },
    "147": {
        "file_id": 14,
        "content": "This code imports necessary libraries and defines a class called VisualIgnoreApp, which is an instance of Textual app. It initializes log and progress bar widgets and sets the progress to 0 initially.",
        "type": "comment"
    },
    "148": {
        "file_id": 14,
        "content": "    async def progress(self):\n        locked = lock.acquire(blocking=False)\n        if locked:\n            self.mylog.clear()\n            await main(self.mylog, self.prog, self.error_container)\n            await asyncio.sleep(2)\n            lock.release()\n    def compose(self) -> ComposeResult:\n        \"\"\"Create child widgets for the app.\"\"\"\n        return [self.mylog, self.prog]\n    def on_mount(self) -> None:\n        self.timer = self.set_interval(INTERVAL, self.progress)\n# mylog = textual.widgets.Log(max_lines = ...)\n# mybar = textual.widgets.ProgressBar(total=100, show_eta=...)\n# mylog.write()\n# TODO: run the document processor in a separate process.\n# TODO: parse the data received from the separate process, line by line.\n# TODO: if the data starts with something special, we would read and parse the whole line and update progress\n# this is sick.\n# cmd = [\"python3\", \"test.py\"]\n# cmd = [\"stdbuf\", \"-o0\", \"-e0\", \"bash\", \"-c\", \"python3 test.py 2>&1\"]\ncmd = [\"stdbuf\", \"-o0\", \"-e0\", \"python3\", \"test.py\"]\n# cmd = [\"bash\", \"-c\", \"python3 test.py 2>&1\"]",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main.py:35-62"
    },
    "149": {
        "file_id": 14,
        "content": "Code is creating a progress bar for displaying the status of a task being processed in a separate process. It uses a logging widget to output data received from the separate process and updates the progress based on the data received. The code also includes options for command execution with stdbuf for better error handling.",
        "type": "comment"
    },
    "150": {
        "file_id": 14,
        "content": "line_format = \"PROCESSING PROGRESS: {progress:d}%\"\ndef parse_line(line: str):\n    parsed = parse.parse(line_format, line)\n    if parsed:\n        return parsed[\"progress\"]\n    return None\nasync def read_stderr(proc, error_container):\n    while True:\n        mbyte = await proc.stderr.readline()  # type:ignore\n        # mbyte = await proc.stderr.read(1)  # type:ignore\n        error_container.append(mbyte.decode())\n        if mbyte == b\"\":\n            break\nasync def read_stdout(proc, mylog, prog):\n    # line_position = 0\n    # line_content = \"\"\n    # mtime = []\n    while True:\n        mbyte = await proc.stdout.readline()  # type:ignore\n        # mbyte = await proc.stdout.read(20)  # type:ignore\n        # mbyte = await proc.stdout.read(1)  # type:ignore\n        if mbyte == b\"\":\n            break\n        else:\n            line_content = mbyte.decode(\"utf-8\").rstrip()\n            # print(content)\n            mylog.write_line(line_content)\n            # mylog.refresh()\n            # continue\n            # if mbyte == b\"\\n\":",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main.py:64-99"
    },
    "151": {
        "file_id": 14,
        "content": "Code snippet is responsible for reading stdout and stderr streams from a process. It uses asynchronous reads to efficiently handle incoming data.\nIn the `read_stdout` function, it continuously reads lines from the stdout stream of the process, decodes them as UTF-8, trims trailing whitespace, and writes each line to a log. The loop breaks when no more data is received.\nThe `read_stderr` function follows similar logic for reading and handling data from the stderr stream.",
        "type": "comment"
    },
    "152": {
        "file_id": 14,
        "content": "            #     line_position = 0\n            #     # try to parse line content.\n            #     mylog.write(\"\\n\")\n            # mylog.write_line(line_content)\n            if line_content.startswith(\">>>> \"):\n                #     mtime.append(datetime.datetime.now())\n                mline = line_content[5:]\n                ret = parse_line(mline)\n                if ret is not None:\n                    steps = ret - prog.progress\n                    if steps > 0:\n                        prog.advance(steps)\n                mylog.write_line(\"parsed progress? \" + str(ret))\nasync def main(mylog, prog, error_container):\n    proc = await asyncio.create_subprocess_shell(\n        # proc = await asyncio.create_subprocess_exec(\n        # *cmd, stdout=asyncio.subprocess.PIPE\n        # UNBUFFERED FLAG: -u\n        \"bash -c 'python3 -u test_no_patch.py 2>&1'\",\n        stdout=asyncio.subprocess.PIPE,\n        stderr=asyncio.subprocess.PIPE\n        # \"python3 -u test_no_patch.py\", stdout=asyncio.subprocess.PIPE\n        # \"python3 test.py\", stdout=asyncio.subprocess.PIPE",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main.py:100-124"
    },
    "153": {
        "file_id": 14,
        "content": "The code is executing a subprocess using the `asyncio.create_subprocess_shell` or `asyncio.create_subprocess_exec` function with stdout and stderr piped to asyncio. It is likely that it is running a Python script in unbuffered mode, which means that the output will be written immediately without waiting for a full line of text to be complete. The code is also parsing the output for progress updates and updating a progress bar accordingly. If a progress update is found, it writes \"parsed progress?\" followed by the returned value to the log.",
        "type": "comment"
    },
    "154": {
        "file_id": 14,
        "content": "    )  # how to handle the stderr now? we may merge the altogether.\n    t1 = asyncio.create_task(read_stdout(proc, mylog, prog))\n    t2 = asyncio.create_task(read_stderr(proc, error_container))\n    # task1 = asyncio.create_task(read_stdout(proc, mylog))\n    # task2 = asyncio.create_task(read_stderr(proc))\n    await asyncio.gather(t1, t2)\n    # await asyncio.gather(task1, task2)\n    retcode = await proc.wait()\n    error_container.insert(0, retcode)\n    if retcode != 0:\n        print(f\"Error: subprocess returned {retcode}\")\n    else:\n        print(f\"Success: subprocess returned {retcode}\")\nif __name__ == \"__main__\":\n    error_container = []\n    app = VisualIgnoreApp(error_container)\n    app.run()\n    # breakpoint()\n    retcode = error_container[0]\n    if retcode != 0:\n        raise Exception(\n            \"\\n\".join(\n                [\"Error: subprocess returned\", str(retcode)] + error_container[1:]\n            )\n        )",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main.py:125-152"
    },
    "155": {
        "file_id": 14,
        "content": "This code runs a subprocess and handles its output and error streams asynchronously using asyncio. It waits for both the stdout and stderr tasks to complete, then retrieves the subprocess' return code. If the return code is not 0, it raises an exception with the error message.",
        "type": "comment"
    },
    "156": {
        "file_id": 15,
        "content": "/document_agi_computer_control/stdout_redirect_progress/main_once.py",
        "type": "filepath"
    },
    "157": {
        "file_id": 15,
        "content": "The code redirects stdout, handles cached and processed files, uses asyncio, initializes a progress bar class, tracks progress in separate process with stdbuf, and updates log file. It spawns subprocesses, redirects output to asyncio tasks, uses UNBUFFERED FLAG, prints success/error messages with execution time, displays remaining error text and total time taken if an error occurs.",
        "type": "summary"
    },
    "158": {
        "file_id": 15,
        "content": "# redirect stdout to some buffered output window, and show a progress bar below.\n# differentiate between \"cached\" file and \"processed\" file\n# you may retrieve \"cached\" file processing time from somewhere else.\n# if failed to retrieve stored processing time, use average one instead.\n# TODO: add this to recursive document generator.\n# TODO: before that, just use a simple timer for producing total processing time and count files, in size, count and lines.\nimport asyncio\nimport parse\nfrom textual.app import App, ComposeResult\nfrom textual.widgets import Log, ProgressBar\n# import textual\nfrom threading import Lock\nlock = Lock()\nINTERVAL = 0.1\nimport shutil\nimport textwrap\ndef wrap_text(text):\n    # Get the terminal width\n    terminal_width, _ = shutil.get_terminal_size()\n    tw = terminal_width - 8\n    if tw < 8:\n        tw = terminal_width\n    wrapped_text = textwrap.fill(text, width=tw)\n    return wrapped_text.rstrip()\nclass VisualIgnoreApp(App):\n    \"\"\"A Textual app to visualize\"\"\"\n    def __init__(self, error_container: list, program_args: list[str], *args, **kwargs):",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once.py:1-41"
    },
    "159": {
        "file_id": 15,
        "content": "This code is for redirecting stdout to a buffered output window, displaying a progress bar below. It distinguishes between \"cached\" and \"processed\" files, retrieves file processing time (either stored or average), and plans to integrate it with a recursive document generator. The code uses asyncio, parse, Log, ProgressBar, threading, and shutil modules, as well as Textual framework for creating a text-based user interface app.",
        "type": "comment"
    },
    "160": {
        "file_id": 15,
        "content": "        super().__init__(*args, **kwargs)\n        self.mylog = Log(max_lines=10000)\n        self.prog = ProgressBar()\n        self.program_args = program_args\n        self.error_container = error_container\n        # self.prog.styles.width=\"100%\"\n        self.prog.styles.align_horizontal = \"center\"\n        # self.prog.update(total=100, progress=0)\n    async def progress(self):\n        locked = lock.acquire(blocking=False)\n        if locked:\n            self.mylog.clear()\n            await main(self.mylog, self.prog, self.error_container, self.program_args)\n            self.exit()\n            # lock.release()\n    def compose(self) -> ComposeResult:\n        \"\"\"Create child widgets for the app.\"\"\"\n        return [self.mylog, self.prog]\n    def on_mount(self) -> None:\n        # await self.progress()\n        # self.exit()\n        self.timer = self.set_interval(INTERVAL, self.progress)\n# mylog = textual.widgets.Log(max_lines = ...)\n# mybar = textual.widgets.ProgressBar(total=100, show_eta=...)\n# mylog.write()\n# TODO: run the document processor in a separate process.",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once.py:42-73"
    },
    "161": {
        "file_id": 15,
        "content": "This code initializes a class with log, progress bar, and program arguments. It defines a 'progress' function to update the progress bar and logs, and a 'compose' method to create child widgets. The 'on_mount' method sets an interval to periodically call the 'progress' function. The TODO comment indicates that running the document processor in a separate process is planned for future implementation.",
        "type": "comment"
    },
    "162": {
        "file_id": 15,
        "content": "# TODO: parse the data received from the separate process, line by line.\n# TODO: if the data starts with something special, we would read and parse the whole line and update progress\n# this is sick.\n# cmd = [\"python3\", \"test.py\"]\n# cmd = [\"stdbuf\", \"-o0\", \"-e0\", \"bash\", \"-c\", \"python3 test.py 2>&1\"]\ncmd = [\"stdbuf\", \"-o0\", \"-e0\", \"python3\", \"test.py\"]\n# cmd = [\"bash\", \"-c\", \"python3 test.py 2>&1\"]\nline_format = \"PROCESSING PROGRESS: {progress:d}/{total:d}\"\ndef parse_line(line: str):\n    parsed = parse.parse(line_format, line)\n    if parsed:\n        return parsed[\"progress\"], parsed[\"total\"]\n    return None\nasync def read_stderr(proc, error_container):\n    while True:\n        mbyte = await proc.stderr.readline()  # type:ignore\n        # mbyte = await proc.stderr.read(1)  # type:ignore\n        error_container.append(mbyte.decode())\n        if mbyte == b\"\":\n            break\nasync def read_stdout(proc, mylog, prog):\n    # line_position = 0\n    # line_content = \"\"\n    # mtime = []\n    mtotal_count  =-1\n    while True:",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once.py:74-106"
    },
    "163": {
        "file_id": 15,
        "content": "This code defines a function to parse lines received from a separate process and reads the stderr and stdout of that process asynchronously. It uses \"stdbuf\" command to redirect stdout and stderr to file descriptors 1 and 2 respectively. The line format is defined as \"PROCESSING PROGRESS: {progress:d}/{total:d}\" and is used to parse the progress updates from the separate process.",
        "type": "comment"
    },
    "164": {
        "file_id": 15,
        "content": "        mbyte = await proc.stdout.readline()  # type:ignore\n        # mbyte = await proc.stdout.read(20)  # type:ignore\n        # mbyte = await proc.stdout.read(1)  # type:ignore\n        if mbyte == b\"\":\n            break\n        else:\n            line_content = mbyte.decode(\"utf-8\").rstrip()\n            # print(content)\n            mylog.write_line(wrap_text(line_content))\n            # mylog.refresh()\n            # continue\n            # if mbyte == b\"\\n\":\n            #     line_position = 0\n            #     # try to parse line content.\n            #     mylog.write(\"\\n\")\n            # mylog.write_line(line_content)\n            if line_content.startswith(\">>>> \"):\n                #     mtime.append(datetime.datetime.now())\n                mline = line_content[5:]\n                ret = parse_line(mline)\n                if ret is not None:\n                    ret_total, ret_prog = ret[1], ret[0]\n                    if ret_total != mtotal_count:\n                        mtotal_count = ret_total\n                        prog.update(total=ret_total, progress=ret_prog)",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once.py:107-131"
    },
    "165": {
        "file_id": 15,
        "content": "The code reads a line from the process's stdout, checks if it is an empty string to break the loop, and then decodes the line content. If not empty, it writes the content to mylog with text wrapping. It also handles cases where the line starts with \">>>>\" and updates progress accordingly by parsing the line using `parse_line()` function.",
        "type": "comment"
    },
    "166": {
        "file_id": 15,
        "content": "                        continue\n                    steps = ret_prog - prog.progress\n                    if steps > 0:\n                        prog.advance(steps)\n                    else:\n                        prog.update(total=ret_total, progress=ret_prog)\n                mylog.write_line(\"parsed progress? \" + str(ret))\nasync def main(mylog, prog, error_container, program_args):\n    # proc = await asyncio.create_subprocess_shell(\n    proc = await asyncio.create_subprocess_exec(\n        *program_args,  # stdout=asyncio.subprocess.PIPE\n        # UNBUFFERED FLAG: -u\n        # \"bash -c 'python3 -u test_no_patch.py 2>&1'\",\n        stdout=asyncio.subprocess.PIPE,\n        stderr=asyncio.subprocess.PIPE\n        # \"python3 -u test_no_patch.py\", stdout=asyncio.subprocess.PIPE\n        # \"python3 test.py\", stdout=asyncio.subprocess.PIPE\n    )  # how to handle the stderr now? we may merge the altogether.\n    t1 = asyncio.create_task(read_stdout(proc, mylog, prog))\n    t2 = asyncio.create_task(read_stderr(proc, error_container))",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once.py:132-153"
    },
    "167": {
        "file_id": 15,
        "content": "The code creates a subprocess with the specified command and redirects its stdout and stderr to separate asyncio tasks. The 'read_stdout' task updates a progress bar based on parsed lines, while the 'read_stderr' task handles errors. The UNBUFFERED FLAG (-u) is used to ensure immediate output from the subprocess.",
        "type": "comment"
    },
    "168": {
        "file_id": 15,
        "content": "    # task1 = asyncio.create_task(read_stdout(proc, mylog))\n    # task2 = asyncio.create_task(read_stderr(proc))\n    await asyncio.gather(t1, t2)\n    # await asyncio.gather(task1, task2)\n    retcode = await proc.wait()\n    error_container.insert(0, retcode)\n    if retcode != 0:\n        print(f\"Error: subprocess returned {retcode}\")\n    else:\n        print(f\"Success: subprocess returned {retcode}\")\nimport sys\nimport time\nimport humanize\nif __name__ == \"__main__\":\n    split_ind = sys.argv.index(\"--\")\n    args = sys.argv[split_ind + 1 :]\n    if \"python\" in args or \"python3\" in args:\n        assert \"-u\" in args, \"Python script must be run with -u flag (unbuffered)\"\n    error_container = []\n    app = VisualIgnoreApp(error_container, args)\n    start_time = time.time()\n    app.run()\n    end_time = time.time()\n    total_time = end_time - start_time\n    # breakpoint()\n    retcode = error_container[0]\n    if retcode != 0:\n        raise Exception(\n            \"\\n\".join(\n                [\"Error: subprocess returned\", str(retcode)]",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once.py:154-187"
    },
    "169": {
        "file_id": 15,
        "content": "This code is creating two asynchronous tasks, one for reading stdout and the other for reading stderr of a subprocess. It then awaits both tasks to complete and retrieves the return code from the subprocess. If the return code is not 0, it raises an exception with an error message. Finally, it calculates the total execution time of the script and prints a success or error message based on the return code. The script requires being run with the -u flag for unbuffered Python.",
        "type": "comment"
    },
    "170": {
        "file_id": 15,
        "content": "                + error_container[1:]\n                + [\"total time:\", humanize.naturaltime(total_time).split(\" ago\")[0]]\n            )\n        )\n    else:\n        print(\"exit successfully\")\n        print(\"total time:\", humanize.naturaltime(total_time).split(\" ago\")[0])",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once.py:188-194"
    },
    "171": {
        "file_id": 15,
        "content": "Code snippet checks if there is an error in the output. If there is, it adds a message to display the remaining error text and the total time taken. Otherwise, it simply prints the total time taken.",
        "type": "comment"
    },
    "172": {
        "file_id": 16,
        "content": "/document_agi_computer_control/stdout_redirect_progress/main_once_char_by_char.py",
        "type": "filepath"
    },
    "173": {
        "file_id": 16,
        "content": "The code is a progress bar text app that uses class variables and libraries for distinguishing cached and processed files. It logs updates, monitors output via stderr/stdout streams, and plans to integrate into a recursive document generator. It utilizes asyncio for subprocess running, handles stdout/stderr in separate tasks, checks return codes/errors, calculates total time taken, and displays it in a human-readable format upon successful execution.",
        "type": "summary"
    },
    "174": {
        "file_id": 16,
        "content": "# redirect stdout to some buffered output window, and show a progress bar below.\n# differentiate between \"cached\" file and \"processed\" file\n# you may retrieve \"cached\" file processing time from somewhere else.\n# if failed to retrieve stored processing time, use average one instead.\n# TODO: add this to recursive document generator.\n# TODO: before that, just use a simple timer for producing total processing time and count files, in size, count and lines.\nimport asyncio\nimport parse\nfrom textual.app import App, ComposeResult\nfrom textual.widgets import Log, ProgressBar\n# import textual\nfrom threading import Lock\nlock = Lock()\nINTERVAL = 0.1\nimport shutil\nimport textwrap\ndef wrap_text(text):\n    # Get the terminal width\n    terminal_width, _ = shutil.get_terminal_size()\n    tw = terminal_width - 8\n    if tw < 8:\n        tw = terminal_width\n    wrapped_text = textwrap.fill(text, width=tw)\n    return wrapped_text.rstrip()\nclass VisualIgnoreApp(App):\n    \"\"\"A Textual app to visualize\"\"\"\n    def __init__(self, error_container: list, program_args: list[str], *args, **kwargs):",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once_char_by_char.py:1-41"
    },
    "175": {
        "file_id": 16,
        "content": "This code is implementing a Textual app that redirects stdout to a buffered output window, displaying a progress bar. It differentiates between \"cached\" and \"processed\" files and retrieves the processing time from somewhere else or uses an average if not available. The code imports necessary libraries, defines a `VisualIgnoreApp` class, and includes some TODOs for future additions such as adding it to a recursive document generator and using a simple timer.",
        "type": "comment"
    },
    "176": {
        "file_id": 16,
        "content": "        super().__init__(*args, **kwargs)\n        self.mylog = Log(max_lines=10000)\n        self.prog = ProgressBar()\n        self.program_args = program_args\n        self.error_container = error_container\n        # self.prog.styles.width=\"100%\"\n        self.prog.styles.align_horizontal = \"center\"\n        # self.prog.update(total=100, progress=0)\n    async def progress(self):\n        locked = lock.acquire(blocking=False)\n        if locked:\n            self.mylog.clear()\n            await main(self.mylog, self.prog, self.error_container, self.program_args)\n            self.exit()\n            # lock.release()\n    def compose(self) -> ComposeResult:\n        \"\"\"Create child widgets for the app.\"\"\"\n        return [self.mylog, self.prog]\n    def on_mount(self) -> None:\n        # await self.progress()\n        # self.exit()\n        self.timer = self.set_interval(INTERVAL, self.progress)\n# mylog = textual.widgets.Log(max_lines = ...)\n# mybar = textual.widgets.ProgressBar(total=100, show_eta=...)\n# mylog.write()\n# TODO: run the document processor in a separate process.",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once_char_by_char.py:42-73"
    },
    "177": {
        "file_id": 16,
        "content": "This code initializes class variables and sets up a progress bar for monitoring the execution of a document processor. It also sets up a logger to clear its contents after a certain number of lines, and starts an interval-based function that updates the progress bar periodically until the task is complete. The comment indicates that in future, this should run in a separate process.",
        "type": "comment"
    },
    "178": {
        "file_id": 16,
        "content": "# TODO: parse the data received from the separate process, line by line.\n# TODO: if the data starts with something special, we would read and parse the whole line and update progress\n# this is sick.\n# cmd = [\"python3\", \"test.py\"]\n# cmd = [\"stdbuf\", \"-o0\", \"-e0\", \"bash\", \"-c\", \"python3 test.py 2>&1\"]\ncmd = [\"stdbuf\", \"-o0\", \"-e0\", \"python3\", \"test.py\"]\n# cmd = [\"bash\", \"-c\", \"python3 test.py 2>&1\"]\nline_format = \"PROCESSING PROGRESS: {progress:d}/{total:d}\"\ndef parse_line(line: str):\n    parsed = parse.parse(line_format, line)\n    if parsed:\n        return parsed[\"progress\"], parsed[\"total\"]\n    return None\nasync def read_stderr(proc, error_container):\n    while True:\n        # mbyte = await proc.stderr.readline()  # type:ignore\n        mbyte = await proc.stderr.read(100)  # type:ignore\n        error_container.append(mbyte)\n        if mbyte == b\"\":\n            break\nasync def read_stdout(proc, mylog, prog):\n    line_position = 0\n    line_content = \"\"\n    # mtime = []\n    init = False\n    while True:\n        # mbyte = await proc.stdout.readline()  # type:ignore",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once_char_by_char.py:74-107"
    },
    "179": {
        "file_id": 16,
        "content": "This code appears to be part of a larger system that involves running another process and monitoring its output. It uses the Python subprocess module to execute a separate process (presumably \"test.py\") and reads from its stdout and stderr streams.\n\nThe code defines a function, `parse_line`, which is used to parse lines from the stderr stream in a specific format. It also includes two asynchronous functions: `read_stderr` and `read_stdout`. The former reads data from the stderr stream until it encounters an empty byte string, while the latter reads data from the stdout stream, parsing each line using `parse_line` function.\n\nOverall, this code seems to be a part of a system that manages and monitors the output of another process in a structured manner.",
        "type": "comment"
    },
    "180": {
        "file_id": 16,
        "content": "        # mbyte = await proc.stdout.read(20)  # type:ignore\n        mbyte = await proc.stdout.read(1)  # type:ignore\n        if mbyte == b\"\":\n            break\n        else:\n            # line_content = mbyte.decode(\"utf-8\").rstrip()\n            # # print(content)\n            # mylog.write_line(wrap_text(line_content))\n            # mylog.refresh()\n            # continue\n            if mbyte == b\"\\n\":\n                line_position = 0\n                mylog.write(\"\\n\")\n                # try to parse line content.\n                if line_content.startswith(\">>>> \"):\n                    #     mtime.append(datetime.datetime.now())\n                    mline = line_content[5:]\n                    ret = parse_line(mline)\n                    if ret is not None:\n                        if not init:\n                            prog.update(total=ret[1], progress=0)\n                            init = True\n                        steps = ret[0] - prog.progress\n                        if steps > 0:\n                            prog.advance(steps)",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once_char_by_char.py:108-132"
    },
    "181": {
        "file_id": 16,
        "content": "Reading stdout one character at a time and checking for a newline to progress through the line. If a newline is found, reset line position and attempt to parse line content. If line starts with \">>>\" and there are no init values, update progress bar total value. Calculate steps needed to reach new progress value and advance progress if steps greater than zero.",
        "type": "comment"
    },
    "182": {
        "file_id": 16,
        "content": "                    mylog.write(\"parsed progress? \" + str(ret)+\"\\n\")\n                line_content = \"\"\n            else:\n                line_position +=1\n                line_content += mbyte.decode(\"utf-8\")\n                mylog.write( mbyte.decode(\"utf-8\"))\n            # mylog.write_line(line_content)\nasync def main(mylog, prog, error_container, program_args):\n    # proc = await asyncio.create_subprocess_shell(\n    proc = await asyncio.create_subprocess_exec(\n        *program_args,  # stdout=asyncio.subprocess.PIPE\n        # UNBUFFERED FLAG: -u\n        # \"bash -c 'python3 -u test_no_patch.py 2>&1'\",\n        stdout=asyncio.subprocess.PIPE,\n        stderr=asyncio.subprocess.PIPE\n        # \"python3 -u test_no_patch.py\", stdout=asyncio.subprocess.PIPE\n        # \"python3 test.py\", stdout=asyncio.subprocess.PIPE\n    )  # how to handle the stderr now? we may merge the altogether.\n    t1 = asyncio.create_task(read_stdout(proc, mylog, prog))\n    t2 = asyncio.create_task(read_stderr(proc, error_container))\n    # task1 = asyncio.create_task(read_stdout(proc, mylog))",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once_char_by_char.py:133-155"
    },
    "183": {
        "file_id": 16,
        "content": "The code creates a subprocess using `asyncio.create_subprocess_exec` and waits for its output. It uses separate tasks to handle stdout and stderr. The `main` function is an asynchronous function that takes a logger (mylog), progress object (prog), error container, and program arguments (program_args). It writes the parsed progress to the logger.",
        "type": "comment"
    },
    "184": {
        "file_id": 16,
        "content": "    # task2 = asyncio.create_task(read_stderr(proc))\n    await asyncio.gather(t1, t2)\n    # await asyncio.gather(task1, task2)\n    retcode = await proc.wait()\n    error_container.insert(0, retcode)\n    if retcode != 0:\n        print(f\"Error: subprocess returned {retcode}\")\n    else:\n        print(f\"Success: subprocess returned {retcode}\")\nimport sys\nimport time\nimport humanize\nif __name__ == \"__main__\":\n    split_ind = sys.argv.index(\"--\")\n    args = sys.argv[split_ind + 1 :]\n    if \"python\" in args or \"python3\" in args:\n        assert \"-u\" in args, \"Python script must be run with -u flag (unbuffered)\"\n    error_container = []\n    app = VisualIgnoreApp(error_container, args)\n    start_time = time.time()\n    app.run()\n    end_time = time.time()\n    total_time = end_time - start_time\n    # breakpoint()\n    retcode = error_container[0]\n    if retcode != 0:\n        error_info = b\"\\n\".join(error_container[1:])\n        sys.stderr.buffer.write(error_info)\n        raise Exception(\n            \"\\n\".join(\n                [\"Error: subprocess returned\", str(retcode)]",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once_char_by_char.py:156-190"
    },
    "185": {
        "file_id": 16,
        "content": "This code is part of a Python script that runs another subprocess and waits for its completion. It checks the return code of the subprocess and prints whether it was successful or encountered an error. If there's an error, it writes the error information to stderr. The code also includes time measurements and seems to be part of a larger application called VisualIgnoreApp.",
        "type": "comment"
    },
    "186": {
        "file_id": 16,
        "content": "                + [\"total time:\", humanize.naturaltime(total_time).split(\" ago\")[0]]\n            )\n        )\n    else:\n        print(\"exit successfully\")\n        print(\"total time:\", humanize.naturaltime(total_time).split(\" ago\")[0])",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/main_once_char_by_char.py:191-196"
    },
    "187": {
        "file_id": 16,
        "content": "Code snippet calculates the total time taken by a process and prints it in a human-readable format. If the process exits successfully, it also displays \"exit successfully\" along with the total time taken.",
        "type": "comment"
    },
    "188": {
        "file_id": 17,
        "content": "/document_agi_computer_control/stdout_redirect_progress/test.py",
        "type": "filepath"
    },
    "189": {
        "file_id": 17,
        "content": "This code is overriding the built-in print function to default flush=True. It demonstrates progress bar printing and sleep functionality with a custom print function.",
        "type": "summary"
    },
    "190": {
        "file_id": 17,
        "content": "import builtins\nimport copy\nmyprint = copy.copy(builtins.print)\ndef custom_print(*args, **kwargs):\n    if \"flush\" not in kwargs:\n        kwargs[\"flush\"] = True\n    myprint(*args, **kwargs)\n# Override the built-in print function with the custom function\nbuiltins.print = custom_print\n# Now, when you use print, it will default to flush=True\nimport time\n# for _ in range(200):\n#     print(\">>>> PROCESSING PROGRESS: 30%\")\nprint(\"Hello, world\")\nSLEEP = 0.2\n# time.sleep(SLEEP)\nfor i in range(10000):\n    print(\n        f\">>>> PROCESSING PROGRESS: {i}%\"\n    )  # problem is here. how to set flush=True this as default?\n    print(\"hello world\")\n    time.sleep(SLEEP)",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/test.py:1-32"
    },
    "191": {
        "file_id": 17,
        "content": "This code is overriding the built-in print function to default flush=True. It demonstrates progress bar printing and sleep functionality with a custom print function.",
        "type": "comment"
    },
    "192": {
        "file_id": 18,
        "content": "/document_agi_computer_control/stdout_redirect_progress/test_no_patch.py",
        "type": "filepath"
    },
    "193": {
        "file_id": 18,
        "content": "This code simulates a progress bar with intermittent \"hello world\" printing, adjustable sleep time using SLEEP variable. The issue is setting flush=True as the default in print statements.",
        "type": "summary"
    },
    "194": {
        "file_id": 18,
        "content": "import time\n# for _ in range(200):\n#     print(\">>>> PROCESSING PROGRESS: 30%\")\nprint(\"Hello, world\")\nSLEEP = 0.1\n# SLEEP = 0.01\n# SLEEP = 1\n# time.sleep(SLEEP)\ntotal = 100\nfor i in range(total):\n    print(\n        f\">>>> PROCESSING PROGRESS: {i+1}/{total}\"\n    )  # problem is here. how to set flush=True this as default?\n    print(\"hello world\")\n    time.sleep(SLEEP)",
        "type": "code",
        "location": "/document_agi_computer_control/stdout_redirect_progress/test_no_patch.py:1-18"
    },
    "195": {
        "file_id": 18,
        "content": "This code simulates a progress bar with intermittent \"hello world\" printing, adjustable sleep time using SLEEP variable. The issue is setting flush=True as the default in print statements.",
        "type": "comment"
    },
    "196": {
        "file_id": 19,
        "content": "/document_agi_computer_control/title_generator/main.py",
        "type": "filepath"
    },
    "197": {
        "file_id": 19,
        "content": "This code generates file titles from metadata, sets up a TinyDB database, updates data, and uses a Language Model to generate titles for content based on input comment and path. It processes each file, splits summaries into chunks of 300 elements, writes them to JSON files with indices, stores the count of split titles in a metadata file, and prints generation process completion.",
        "type": "summary"
    },
    "198": {
        "file_id": 19,
        "content": "# generate title\n# create /cache_title.json, /metadata_title.json, /data/titles/<number>.json\n# hash by comment, cache by path identifier and comment hash\n# identify those identical comments (file that only has one segment), only give title to file not segment\n# only display title if exists\nimport os\nimport argparse\nfrom re import L\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-s\", \"--source_dir\", type=str, required=True)\nargs = parser.parse_args()\n# the only parameter.\nsource_dir = args.source_dir\nassert os.path.exists(source_dir)\nassert os.path.isdir(source_dir)\nassert os.path.isabs(source_dir)\nimport json\nimport sys\nsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"../\"))\nfrom llm import llm_context\nfrom slice_utils import split_dict_into_chunks\nmetadata = json.loads(open(os.path.join(source_dir, \"metadata.json\"), \"r\").read())\nfile_mapping = metadata[\"file_mapping\"]\nsplit_count = metadata[\"split_count\"]\nproject_name = metadata[\"project_name\"]\ndata = {}\nfor i in range(split_count):",
        "type": "code",
        "location": "/document_agi_computer_control/title_generator/main.py:1-41"
    },
    "199": {
        "file_id": 19,
        "content": "This code generates titles for files in a given source directory. It uses the metadata from \"metadata.json\" to determine file mapping, split count, and project name. The code loads necessary modules and checks if the source directory exists and is an absolute path.",
        "type": "comment"
    }
}