{
    "300": {
        "file_id": 28,
        "content": "FILE_RETRIEVER_OBJECTIVE_PROMPT = \"\"\"Write a detailed technical explanation of what this code does.\nFocus on the high-level purpose of the code and how it may be used in the larger project.\n\"\"\"\nFOLDER_RETRIEVER_OBJECTIVE_PROMPT = \"\"\"Write a technical explanation of what the code in this file does and how it might fit into the larger project or work with other parts of the project.\nGive examples of how this code might be used. Include code examples where appropriate.\n\"\"\"\nFILE_PROMPT = (\n    f\"\"\"\n{FILE_RETRIEVER_OBJECTIVE_PROMPT.strip()}\nInclude code examples where appropriate. Keep you response between 100 and 300 words.\nDO NOT RETURN MORE THAN 300 WORDS.\nOutput should be in markdown format.\nDo not just list the methods and classes in this file.\n\"\"\",\n)\nFOLDER_PROMPT = (\n    f\"\"\"\n{FOLDER_RETRIEVER_OBJECTIVE_PROMPT.strip()}\nBe concise. Include any information that may be relevant to a developer who is curious about this code.\nKeep you response under 400 words. Output should be in markdown format.\nDo not just list the files and folders in this folder.",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/prompts.py:1-23"
    },
    "301": {
        "file_id": 28,
        "content": "This code contains prompts for generating technical explanations of the code and its usage in the larger project. The FILE_PROMPT provides a detailed objective prompt with an upper limit of 300 words, while the FOLDER_PROMPT offers concise information under 400 words about how the code fits into the project. The output should be in markdown format and tailored to developers who may be curious about the code.",
        "type": "comment"
    },
    "302": {
        "file_id": 28,
        "content": "\"\"\",\n)\nTARGET_AUDIENCE = \"smart developer\"\ndef generateFileSummaryContextQueriesPrompt(\n    schema: str,\n    contentType: str,\n    projectName: str,\n    filePath: str,\n    summary: str,\n    fileRetrieverObjectivePrompt=FILE_RETRIEVER_OBJECTIVE_PROMPT,\n):\n    prompt = f\"\"\"You are acting as a {contentType} documentation expert for a project called {projectName}.\nBelow is the summary from a file located at `{filePath}`. \nYour objective is:\n{fileRetrieverObjectivePrompt}\nFile summary:\n{summary}\nGenerate 3 to 5 queries (NO MORE THAN FIVE) to help you retrieve relevant content to achieve your objective.\nResponse pydantic schema:\n{schema}\nRespond the context queries strictly to the schema, in JSON format:\n\"\"\"\n    return prompt\ndef generateFileSummaryPrompt(\n    contentType: str,\n    projectName: str,\n    filePath: str,\n    summary: str,\n    contextQAPairs: list[tuple[str, str]],\n    filePrompt=FILE_PROMPT,\n):\n    context = \"\\n\".join(\n        [f\"Context to query '{query}':\\n{answer}\\n\" for query, answer in contextQAPairs]\n    )",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/prompts.py:24-70"
    },
    "303": {
        "file_id": 28,
        "content": "This code defines two functions, `generateFileSummaryContextQueriesPrompt` and `generateFileSummaryPrompt`, which generate prompts for generating queries based on a given file summary. The prompts include the file's context and schema information, instructing users to generate 3-5 relevant queries within the specified schema format.",
        "type": "comment"
    },
    "304": {
        "file_id": 28,
        "content": "    prompt = f\"\"\"You are acting as a {contentType} documentation expert for a project called {projectName}.\nBelow is the summary and relevant context of {contentType} from a file located at `{filePath}`. \n{filePrompt}\nDo not say \"this file is a part of the {projectName} project\".\nFile summary:\n{summary}\n{context}\nResponse the document in Markdown:\n\"\"\"\n    return prompt\ndef generateFileQuestionsPrompt(\n    schema: str,\n    contentType: str,\n    projectName: str,\n    filePath: str,\n    summary: str,\n    targetAudience: str = TARGET_AUDIENCE,\n):\n    prompt = f\"\"\"You are acting as a {contentType} documentation expert for a project called {projectName}.\nBelow is the {contentType} from a file located at `{filePath}`. \nWhat are 3 questions that a {targetAudience} might have about this {contentType}?\nFile summary:\n{summary}\nResponse pydantic schema:\n{schema}\nRespond the reader questions strictly to the schema, in JSON format:\n\"\"\"\n    return prompt\ndef generateFileAnswerPrompt(\n    contentType: str,\n    projectName: str,\n    filePath: str,",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/prompts.py:71-115"
    },
    "305": {
        "file_id": 28,
        "content": "This code generates prompts for a documentation expert to answer questions about a specific file content. The first function, `generateFileQuestionsPrompt`, asks the user to provide 3 questions that a target audience might have about the given content type from a file. The second function, `generateFileAnswerPrompt`, prompts the user to respond to these reader questions strictly within a provided Pydantic schema and in JSON format.",
        "type": "comment"
    },
    "306": {
        "file_id": 28,
        "content": "    summary: str,\n    question: str,\n    context: str,\n    targetAudience: str = TARGET_AUDIENCE,\n):\n    prompt = f\"\"\"You are acting as a {contentType} documentation expert for a project called {projectName}.\nBelow is the {contentType} from a file located at `{filePath}`.\nFile summary:\n{summary}\nThis is a question that a {targetAudience} might have about this {contentType}:\n{question}\nContext about the question:\n{context}\nAnswer the question in 1-2 sentences. Output should be in markdown format.\nRespond the answer to the question in markdown format:\n\"\"\"\n    return prompt\ndef generateFolderSummaryContextQueriesPrompt(\n    schema: str,\n    contentType: str,\n    projectName: str,\n    folderPath: str,\n    summary: str,\n    folderRetrieverObjectivePrompt=FOLDER_RETRIEVER_OBJECTIVE_PROMPT,\n):\n    prompt = f\"\"\"You are acting as a {contentType} documentation expert for a project called {projectName}.\nYou are currently documenting the folder located at `{folderPath}`. \nYour objective is:\n{folderRetrieverObjectivePrompt}\nFolder summary:",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/prompts.py:116-158"
    },
    "307": {
        "file_id": 28,
        "content": "Function generates a prompt for documentation experts to describe a folder's content and answer related questions in markdown format. The function takes parameters such as schema, content type, project name, folder path, summary, and optional folder retriever objective prompt.",
        "type": "comment"
    },
    "308": {
        "file_id": 28,
        "content": "{summary}\nGenerate 3 to 5 queries (NO MORE THAN FIVE) to help you retrieve relevant content to achieve your objective.\nResponse pydantic schema:\n{schema}\nRespond the context queries strictly to the schema, in JSON format:\n\"\"\"\n    return prompt\ndef generateFolderSummaryPrompt(\n    contentType: str,\n    projectName: str,\n    filePath: str,\n    summary: str,\n    contextQAPairs: list[tuple[str, str]],\n    folderPrompt=FOLDER_PROMPT,\n):\n    context = \"\\n\".join(\n        [f\"Context to query '{query}':\\n{answer}\\n\" for query, answer in contextQAPairs]\n    )\n    prompt = f\"\"\"You are acting as a {contentType} documentation expert for a project called {projectName}.\nBelow is the summary and relevant context of {contentType} from a folder located at `{filePath}`. \n{folderPrompt}\nDo not say \"this file is a part of the {projectName} project\".\nFolder summary:\n{summary}\n{context}\nResponse the document in Markdown:\n\"\"\"\n    return prompt\ndef generateCondensePrompt(\n    chat_history: str, question: str\n):  # in order to query context for next question, we generate another query'",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/prompts.py:160-202"
    },
    "309": {
        "file_id": 28,
        "content": "The code defines two functions: `generateFolderSummaryPrompt` and `generateCondensePrompt`. The first function takes parameters such as content type, project name, file path, summary, context pairs, and a folder prompt. It generates a prompt by joining the context queries, summary, and relevant context for a document in Markdown format. The second function takes a chat history and a question as input and generates a prompt to query context for the next question.",
        "type": "comment"
    },
    "310": {
        "file_id": 28,
        "content": "    prompt = f\"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\nChat History:\n{chat_history}\nFollow Up Input: {question}\nRespond the standalone question:\n\"\"\"\n    return prompt\ndef generateQAPrompt(\n    contentType: str,\n    projectName: str,\n    question: str,\n    context: str,\n    targetAudience: str = TARGET_AUDIENCE,\n):\n    prompt = f\"\"\"You are an AI assistant for a software project called {projectName}. You are trained on all the {contentType} that makes up this project.\nYou are given the following extracted parts of a technical summary of files in a {contentType} and a question. \nProvide a conversational answer with hyperlinks back to GitHub.\nYou should only use hyperlinks that are explicitly listed in the context. Do NOT make up a hyperlink that is not listed.\nInclude lots of {contentType} examples and links to the {contentType} examples, where appropriate.\nAssume the reader is a {targetAudience} but is not deeply familiar with {projectName}.",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/prompts.py:203-225"
    },
    "311": {
        "file_id": 28,
        "content": "This code defines a function `generateQAPrompt` that generates a prompt for an AI assistant to answer questions related to a software project. The prompt specifies the content type, project name, and target audience, and instructs the assistant to provide conversational answers with GitHub hyperlinks, using only explicitly listed links.",
        "type": "comment"
    },
    "312": {
        "file_id": 28,
        "content": "Assume the reader does not know anything about how the project is strucuted or which folders/files are provided in the context.\nDo not reference the context in your answer. Instead use the context to inform your answer.\nIf you don't know the answer, just say \"Hmm, I'm not sure.\" Don't try to make up an answer.\nIf the question is not about the {projectName}, politely inform them that you are tuned to only answer questions about the {projectName}.\nYour answer should be at least 100 words and no more than 300 words.\nDo not include information that is not directly relevant to the question, even if the context includes it.\nAlways include a list of reference links from the context. Links should ONLY come from the context.\nQuestion: {question}\nContext:\n{context}\nAnswer the document in Markdown:\n\"\"\"\n    return prompt\ndef generateRecentChatHistorySummaryPrompt(query, answer):\n    prompt = f\"\"\"You are a professional chat history summarizer. You will produce a chat history summary in 50 words, that both focus ",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/prompts.py:226-245"
    },
    "313": {
        "file_id": 28,
        "content": "This function generates a summary prompt for recent chat history, taking query and answer as input. It returns the generated prompt.",
        "type": "comment"
    },
    "314": {
        "file_id": 28,
        "content": "on the user prompt and the bot response. DO NOT include bloat words such as 'Both the prompt and the response include'. Capture key details and factors, be insightful. Make your response fluent and coherent, just like a comprehensive summary over the whole chat.\nUser:\n{query}\nBot:\n{answer}\nRespond a chat history summary in 50 words:\n\"\"\"\n    return prompt\ndef generateChatHistorySummaryPrompt(last_chat_history: str, recent_chat_history: str):\n    prompt = f\"\"\"You are a professional chat history summarizer. You will produce a chat history summary in 50 words, that both focus on the last chat history and the recent chat history. DO NOT include bloat words such as 'Both history include'. Capture key details and factors, be insightful. Make your response fluent and coherent, just like a comprehensive summary over the whole chat.\nLast chat history:\n{last_chat_history}\nRecent chat history:\n{recent_chat_history}\nRespond a chat history summary in 50 words:\n\"\"\"\n    return prompt\ndef generateFolderSummaryPrompt():\n    prompt = \"\"\"\"\"\"",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/prompts.py:245-276"
    },
    "315": {
        "file_id": 28,
        "content": "This code defines a function generateChatHistorySummaryPrompt, which creates a prompt for summarizing chat history in 50 words. The prompt emphasizes capturing key details and factors while being insightful and coherent without using bloat words. It takes two parameters - last_chat_history and recent_chat_history - to focus on the summary.",
        "type": "comment"
    },
    "316": {
        "file_id": 28,
        "content": "    return prompt",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/prompts.py:277-277"
    },
    "317": {
        "file_id": 28,
        "content": "This line of code returns the prompt that was previously defined or inputted, allowing for its usage in further stages of the program.",
        "type": "comment"
    },
    "318": {
        "file_id": 29,
        "content": "/document_agi_computer_control/vectorstore_embedding_chat_rag/test.py",
        "type": "filepath"
    },
    "319": {
        "file_id": 29,
        "content": "The code imports OllamaEmbeddings from Langchain and initializes an embedding model. The comments indicate that the model is used for querying and document embedding, but LlamaIndex with its cache could be a potential alternative. The code then embeds a query and some documents using the initialized model and prints the types and shapes of the resulting embeddings.",
        "type": "summary"
    },
    "320": {
        "file_id": 29,
        "content": "# create embedding with ollama\n# or use llamaindex instead? it has cache.\n# since small local index is enough, we do not need huge vector search database engine.\n# if that is the case, we just swap the adapter.\nimport rich\nfrom langchain.embeddings import OllamaEmbeddings\nollama_emb = OllamaEmbeddings(\n    model=\"openhermes2.5-mistral:latest\",\n    # model=\"llama:7b\",\n)\nquery = \"hello world\"\ndocuments = [\"hello again\", \"bye world\"]\nq_emb = ollama_emb.embed_query(query)\nd_emb = ollama_emb.embed_documents(documents)\nimport numpy as np\nrich.print(type(q_emb), np.array(q_emb).shape)  # list\nrich.print(type(d_emb), np.array(d_emb).shape)\n# <class 'list'>\n# (4096,)\n# <class 'list'>\n# (2, 4096)",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/test.py:1-26"
    },
    "321": {
        "file_id": 29,
        "content": "The code imports OllamaEmbeddings from Langchain and initializes an embedding model. The comments indicate that the model is used for querying and document embedding, but LlamaIndex with its cache could be a potential alternative. The code then embeds a query and some documents using the initialized model and prints the types and shapes of the resulting embeddings.",
        "type": "comment"
    },
    "322": {
        "file_id": 30,
        "content": "/document_agi_computer_control/vectorstore_embedding_chat_rag/test_hnswlib.py",
        "type": "filepath"
    },
    "323": {
        "file_id": 30,
        "content": "The code imports hnswlib, initializes an HNSW index with L2 space and specified dimension, adds random embeddings to the index, saves it as \"hnsw_index.bin\", performs k-nearest neighbor search using two random queries, and prints the shapes of the results.",
        "type": "summary"
    },
    "324": {
        "file_id": 30,
        "content": "import hnswlib\ndim = 128  # Example dimension of the embeddings\nnum_elements = 10000  # Example number of elements\np = hnswlib.Index(space=\"l2\", dim=dim)  # L2 space with the specified dimension\np.init_index(\n    max_elements=num_elements, ef_construction=200, M=16\n)  # Initialize the index\nimport numpy as np\nembeddings = np.random.rand(\n    num_elements, dim\n)  # Example embeddings, replace with your actual embeddings\np.add_items(embeddings)\np.save_index(\"hnsw_index.bin\")\nindices, distances = p.knn_query(np.random.rand(2, dim), k=3)\nimport rich\nrich.print(indices.shape, distances.shape)\n# (2, 3)",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/test_hnswlib.py:1-24"
    },
    "325": {
        "file_id": 30,
        "content": "The code imports hnswlib, initializes an HNSW index with L2 space and specified dimension, adds random embeddings to the index, saves it as \"hnsw_index.bin\", performs k-nearest neighbor search using two random queries, and prints the shapes of the results.",
        "type": "comment"
    },
    "326": {
        "file_id": 31,
        "content": "/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py",
        "type": "filepath"
    },
    "327": {
        "file_id": 31,
        "content": "This code initializes a document index, checks for hash matches, embeds documents if needed, and uses embeddings to search comments. It retrieves relevant results based on user queries and constructs context variable for response generation using llm_context function.",
        "type": "summary"
    },
    "328": {
        "file_id": 31,
        "content": "# responsible for turning everything into embeddings.\n# responsible for caching, index wirings.\n# code chunk with title, chunk description with title\n# code document chunks, folder document chunks\n# latest code chunks -> embeddings\n# folder/file summary hash -> latest document chunks -> document chunk hash -> embeddings\nimport os\nimport argparse\nos.environ[\"OPENAI_API_KEY\"] = \"any\"\nos.environ[\"OPENAI_API_BASE\"] = \"http://0.0.0.0:8000\"\nos.environ[\"BETTER_EXCEPTIONS\"] = \"1\"\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom pydantic import BaseModel\nfrom tinydb import TinyDB\nclass FileSummary(BaseModel):\n    file_hash: str\n    summary: str\nclass FolderSummary(BaseModel):  # this is not generated. do it now.\n    folder_hash: str\n    summary: str\ntextSpliter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=100,\n)\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-s\", \"--source_dir\", type=str, required=True)\nargs = parser.parse_args()\n# the only parameter.\nsource_dir = args.source_dir",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py:1-44"
    },
    "329": {
        "file_id": 31,
        "content": "This code initializes a RecursiveCharacterTextSplitter with specified chunk size and overlap, parses arguments from the command line using argparse, and sets source_dir as required input. This code is responsible for turning everything into embeddings by splitting text into chunks and generating file and folder summaries. It also handles caching, index wiring, and stores document chunks in folders and files.",
        "type": "comment"
    },
    "330": {
        "file_id": 31,
        "content": "assert os.path.exists(source_dir)\nassert os.path.isdir(source_dir)\nassert os.path.isabs(source_dir)\nimport json\nimport sys\nsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"../\"))\nfrom llm import llm_context\ncache_path_base = os.path.join(source_dir, \"vector_cache\")\ndocument_cache_path_bash = os.path.join(cache_path_base, \"document\")\nif not os.path.exists(cache_path_base):\n    os.mkdir(cache_path_base)\nif not os.path.exists(document_cache_path_bash):\n    os.mkdir(document_cache_path_bash)\nfolder_summary_db = TinyDB(os.path.join(cache_path_base, \"folder_summaries.json\"))\nmetadata = json.loads(open(os.path.join(source_dir, \"metadata.json\"), \"r\").read())\ntitle_metadata = json.loads(\n    open(os.path.join(source_dir, \"metadata_title.json\"), \"r\").read()\n)\nfile_mapping = metadata[\"file_mapping\"]\nsplit_count = metadata[\"split_count\"]\nproject_name = metadata[\"project_name\"]\ntitle_split_count = title_metadata[\"split_count\"]\ndata = {}\ntitle_data = {}\n# file_title_indices = []\nfile_summary_indices = [v[\"entry_id\"] + 1 for v in file_mapping.values()]",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py:46-82"
    },
    "331": {
        "file_id": 31,
        "content": "This code sets up the necessary paths and databases for vector indexing. It checks if directories exist, creates them if not, loads metadata from files, and initializes file and title data dictionaries. The file summary indices are populated while potentially skipping a title-based approach.",
        "type": "comment"
    },
    "332": {
        "file_id": 31,
        "content": "for i in range(split_count):\n    new_data = json.loads(open(os.path.join(source_dir, f\"data/{i}.json\"), \"r\").read())\n    data.update(new_data)\nfor i in range(title_split_count):\n    new_data = json.loads(\n        open(os.path.join(source_dir, f\"data/titles/{i}.json\"), \"r\").read()\n    )\n    title_data.update(new_data)\n# breakpoint()\ndef strip_quote(s: str):\n    if s[0] == s[-1]:\n        if s[0] in ['\"', \"'\"]:\n            return s[1:-1].strip()\n    return s.strip().strip(\".\")\nfile_summary_dict = {}\nfile_hash_dict = {}\nfile_chunk_comment_dict = {}\nfile_chunk_code_dict = {}\nimport hashlib\ndef hash_doc(enc: str):\n    hash_object = hashlib.md5(enc.encode())\n    return hash_object.hexdigest()\ncode_and_comment_list = []\nfor k, v in data.items():\n    if v[\"type\"] == \"code\":\n        code_elem = v\n        comment_elem = data[str(int(k) + 1)]\n        code_content = v[\"content\"]\n        comment_content = comment_elem[\"content\"]\n        location = v[\"location\"]\n        code_and_comment_list.append(\n            dict(code=code_content, comment=comment_content, location=location)",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py:84-128"
    },
    "333": {
        "file_id": 31,
        "content": "The code reads JSON files, updates data and title_data dictionaries accordingly, and defines a function to strip quotes from strings. It also calculates hash values for documents, and initializes empty dictionaries for file summaries, hashes, chunk comments, and code. Then it iterates through the data dictionary, appending code elements along with their corresponding comments and locations into a list called 'code_and_comment_list'.",
        "type": "comment"
    },
    "334": {
        "file_id": 31,
        "content": "        )\n        # chunk_hash = hash_doc(code_content)\nos.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\nfrom langchain.embeddings import OllamaEmbeddings\nollama_emb = OllamaEmbeddings(\n    model=\"openhermes2.5-mistral:latest\",\n    # model=\"llama:7b\",\n)\nfrom docarray import BaseDoc\nfrom docarray.index import HnswDocumentIndex  # type: ignore\nimport numpy as np\nfrom docarray.typing import NdArray\nclass CodeCommentChunk(BaseDoc):\n    code: str\n    comment: str\n    location: str\n    chunk_hash: str\n    embedding: NdArray[4096]  # type:ignore\nclass FileDocumentChunk(BaseDoc):\n    file_hash: str\n    chunk: str\n    embedding: NdArray[4096]  # type:ignore\nclass FolderDocumentChunk(BaseDoc):\n    folder_hash: str\n    chunk: str\n    embedding: NdArray[4096]  # type:ignore\n# create a Document Index\ncomment_index = HnswDocumentIndex[CodeCommentChunk](\n    work_dir=os.path.join(cache_path_base, \"comment\")\n)\nfile_document_index = HnswDocumentIndex[FileDocumentChunk](\n    work_dir=os.path.join(document_cache_path_bash, \"file\")",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py:129-174"
    },
    "335": {
        "file_id": 31,
        "content": "This code is creating a Document Index using HnswDocumentIndex for CodeCommentChunk, FileDocumentChunk, and FolderDocumentChunk types. It uses OllamaEmbeddings with \"openhermes2.5-mistral:latest\" model to get the document embeddings. The indexes are saved in respective directories under cache_path_base and document_cache_path_bash for future retrieval and search operations.",
        "type": "comment"
    },
    "336": {
        "file_id": 31,
        "content": ")\nfolder_document_index = HnswDocumentIndex[FolderDocumentChunk](\n    work_dir=os.path.join(document_cache_path_bash, \"folder\")\n)\ncomment_index_ids = []\nimport progressbar, random, time\nfor it in progressbar.progressbar(code_and_comment_list, prefix=\"code and comments:\"):\n    chunk_hash = hash_doc(it[\"code\"])\n    comment_index._sqlite_cursor.execute(\n        \"SELECT location, doc_id FROM docs WHERE chunk_hash = ?\", (chunk_hash,)\n    )\n    rows = comment_index._sqlite_cursor.fetchall()\n    if len(rows) > 0:\n        cached = False\n        for row in rows:\n            if row[0] == it[\"location\"]:\n                cached = True\n                doc_id = row[1]\n                comment_index_ids.append(doc_id)\n                break\n        if cached:\n            print(\"document cached:\", str(it)[:50] + \"...}\")\n            continue\n    code_and_comment = f\"\"\"Code:\n{it['code']}\nComment:\n{it['comment']}\n\"\"\"\n    while True:\n        embed_list = ollama_emb.embed_query(code_and_comment)\n        if embed_list is not None:\n            break",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py:175-212"
    },
    "337": {
        "file_id": 31,
        "content": "This code chunk is initializing a folder document index using the HnswDocumentIndex class and connecting to a SQLite database. It then iterates through a list of code and comment pairs, checking if each document's hash exists in the database. If it does, the corresponding document ID is stored and the document is considered cached. Otherwise, it embeds the code and comment pair using the Ollama embedder.",
        "type": "comment"
    },
    "338": {
        "file_id": 31,
        "content": "        else:\n            print(\"waiting for ollama service to be idle\")\n            time.sleep(random.random())\n    embed = np.array(embed_list)  # it returns None. how comes?\n    # during LLM completion api?\n    # print(code_and_comment)\n    # print(embed.shape)\n    # breakpoint()\n    docObject = CodeCommentChunk(\n        **it, chunk_hash=chunk_hash, embedding=embed\n    )  # type:ignore\n    doc_id = comment_index._to_hashed_id(docObject.id)\n    comment_index_ids.append(doc_id)\n    comment_index.index(docObject)\n# comment_index._sqlite_cursor.execute(\"SELECT doc_id FROM docs WHERE text LIKE 'hello%'\")\n# rows = comment_index._sqlite_cursor.fetchall()\n# # print(rows)\n# hashed_ids = set(it[0] for it in rows)\n# # hashed_ids = set(str(it[0]) for it in rows)\n# print(hashed_ids)\ndef print_and_return(content: str):\n    return content + \"\\n\"\nif __name__ == \"__main__\":\n    # query for code & embedding index\n    init_prompt = \"\"\"You are a helpful assistant who can answer questions based on relevant context about a specific code project. Please answer the user query according to the context.",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py:213-244"
    },
    "339": {
        "file_id": 31,
        "content": "The code is initializing a print statement with the text \"waiting for ollama service to be idle\" and then entering a sleep state for a random duration. It then converts a list of embeddings into a numpy array and creates a CodeCommentChunk object, assigning it an ID and embedding value. The function appends the hashed ID to the comment_index_ids list and indexes the CodeCommentChunk object in the comment_index. Finally, it prints the rows of documents where the text column matches 'hello%' from the SQLite cursor and returns a content string for printing and returning.",
        "type": "comment"
    },
    "340": {
        "file_id": 31,
        "content": "Assume the reader does not know anything about how the project is strucuted or which folders/files are provided in the context.\nDo not reference the context in your answer. Instead use the context to inform your answer.\nIf you don't know the answer, just say \"Hmm, I'm not sure.\" Don't try to make up an answer.\nYour answer should be at least 100 words and no more than 300 words.\nDo not include information that is not directly relevant to the question, even if the context includes it.\n\"\"\"\n    while True:\n        query = input(\"query for code & embedding index:\\n\")\n        ans = comment_index._search_and_filter(\n            np.array(ollama_emb.embed_query(query)).reshape(1, -1),\n            limit=3,\n            # limit=10,\n            search_field=\"embedding\",\n            hashed_ids=set(comment_index_ids),\n        )\n        print(\"ans:\", ans)\n        context = \"\"\n        for it in ans.documents[0]:\n            location = it.location\n            file_location = location.split(\":\")[0]\n            context += print_and_return(\"-\" * 10)",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py:245-267"
    },
    "341": {
        "file_id": 31,
        "content": "This code appears to be a part of an interactive application that allows the user to input queries and retrieve relevant documents based on their similarity in an embedding index. The embedded query is used to search for matches in the comment index, with a limit of 3 results (originally set to 10). The code then prints the answers and context by accessing the location information from the returned documents.",
        "type": "comment"
    },
    "342": {
        "file_id": 31,
        "content": "            context += print_and_return(\"Location:\")\n            context += print_and_return(location)\n            # context += print_and_return(\"Title:\")\n            # context += print_and_return(title_data.get(location, title_data[file_location]))\n            context += print_and_return(\"Comment:\")\n            context += print_and_return(it.comment)\n            context += print_and_return(\"Code:\")\n            context += print_and_return(it.code)\n        with llm_context(init_prompt, temperature=0.2) as model:\n            model.run(\n                f\"Context:\\n{context}\\nUser query: {query}\\nRespond in Markdown format:\\n\"\n            )",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/vectorindex.py:268-280"
    },
    "343": {
        "file_id": 31,
        "content": "This code constructs a context variable by appending different prompts and their corresponding data, such as location, title, comment, and code. Then it uses the llm_context function with an init_prompt and a temperature of 0.2 to generate a response in Markdown format based on the constructed context and a user query.",
        "type": "comment"
    },
    "344": {
        "file_id": 32,
        "content": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/async_utils.py",
        "type": "filepath"
    },
    "345": {
        "file_id": 32,
        "content": "This code defines three asynchronous functions: `read_lines`, `run_command`, and `main`. The `read_lines` function reads lines from a stream until there are no more lines, yielding each line. The `run_command` function creates a subprocess with the given command and its stdout is connected to an instance of `read_lines`. It then waits for the process to complete. Finally, the `main` function runs the command asynchronously using `run_command`.",
        "type": "summary"
    },
    "346": {
        "file_id": 32,
        "content": "import asyncio\nasync def read_lines(stream:asyncio.StreamReader):\n    while True:\n        line = await stream.readline()\n        # print(\"line\")\n        if not line:\n            break\n        else: \n            yield line\nasync def run_command(command:list[str]):\n    process = await asyncio.create_subprocess_exec(\n        *command,\n        stdout=asyncio.subprocess.PIPE,\n        # stderr=asyncio.subprocess.PIPE\n    )\n    f = [it for it in read_lines(process.stdout)]\n    # stderr_reader = asyncio.StreamReader()\n    # Read lines from stdout and stderr concurrently\n    # await asyncio.gather(\n    #     read_lines(stdout_reader),\n    #     read_lines(stderr_reader)\n    # )\n    # Wait for the process to complete\n    await process.wait()\nasync def main():\n    command = [\"ls\", \"-l\"]\n    # Run the command asynchronously\n    await run_command(command)\nif __name__ == \"__main__\":\n    # Run the event loop\n    asyncio.run(main())",
        "type": "code",
        "location": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/async_utils.py:1-37"
    },
    "347": {
        "file_id": 32,
        "content": "This code defines three asynchronous functions: `read_lines`, `run_command`, and `main`. The `read_lines` function reads lines from a stream until there are no more lines, yielding each line. The `run_command` function creates a subprocess with the given command and its stdout is connected to an instance of `read_lines`. It then waits for the process to complete. Finally, the `main` function runs the command asynchronously using `run_command`.",
        "type": "comment"
    },
    "348": {
        "file_id": 33,
        "content": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/display_tree_structure.py",
        "type": "filepath"
    },
    "349": {
        "file_id": 33,
        "content": "The script uses the \"rich\" library to display tree structures and handles command line arguments, file/directory operations. The function recursively traverses a dictionary, managing directories and updating counts. It labels tree structures with size, line count, estimates time from lines, reads file names, and yields line counts for non-error files. The code iterates through the dictionary, calculating sizes and processing time, and prints total and selected file counts, line counts by suffix, and error count.",
        "type": "summary"
    },
    "350": {
        "file_id": 33,
        "content": "from rich.text import Text\nfrom rich.console import Console\nimport datetime\n# color from:\nfrom rich.color import ANSI_COLOR_NAMES\nfrom collections import defaultdict\nconsole = Console()\nfrom rich.tree import Tree\nfrom rich import print\nimport json\nimport os\nimport humanize\nerror_map = defaultdict(list)\ncached_verified = []\ndef patch_missing_files(path, basemap, color, processor=lambda x: x):\n    subpath, filename = dirsplit(path)\n    # breakpoint()\n    if basemap.get(path) is None:\n        subtree = patch_missing_files(subpath + \"/\", basemap, color)\n        subsubtree = subtree.add(processor(filename), style=color, guide_style=color)\n        # print(filename)\n        basemap[path] = subsubtree\n        return subsubtree\n    else:\n        return basemap.get(path)\ndef size_to_readable_string(size: int):\n    return humanize.naturalsize(size)\nGREY = \"bright_black\"\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--full\", help=\"full tree\", type=str, required=True)\nparser.add_argument(\"--selected\", help=\"selected tree\", type=str, required=True)",
        "type": "code",
        "location": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/display_tree_structure.py:1-44"
    },
    "351": {
        "file_id": 33,
        "content": "This code is a Python script that uses the \"rich\" library for creating and displaying a tree structure. It allows you to patch missing files, convert file sizes to readable strings, and handle command line arguments using \"argparse\". The script takes two input parameters: \"--full\" for the full tree and \"--selected\" for the selected tree.",
        "type": "comment"
    },
    "352": {
        "file_id": 33,
        "content": "parser.add_argument(\"--basepath\", help=\"path to the base\", type=str, required=True)\nargs = parser.parse_args()\nfull_json = args.full\nselected_json = args.selected\nbasepath = args.basepath\nassert os.path.isabs(basepath)\nbasepath = os.path.abspath(basepath)\ntree_data = json.load(open(full_json))\nselected_json = json.load(open(selected_json))  # could be different.\n# cached_paths = [\n#     \"/media/root/Toshiba XG3/works/prometheous/document_agi_computer_control/recursive_document_writer.py\",\n#     \"/media/root/Toshiba XG3/works/prometheous/document_agi_computer_control/code_view_with_path_argument_and_anchor/code_view_demo.py\",\n# ]\ncached_paths = []\nfor p in cached_paths:\n    assert os.path.isabs(p)\ncached_paths = [os.path.abspath(p) for p in cached_paths]\nfor p in cached_paths:\n    assert os.path.commonprefix([p, basepath]) == basepath\nsize_map = {}\nselected_keys = []\nexisting_keys = []\n# Add the tree contents recursively\ndef add_tree_contents(parent, contents, basedir=\".\", basemap={}):\n    for item in contents:",
        "type": "code",
        "location": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/display_tree_structure.py:45-78"
    },
    "353": {
        "file_id": 33,
        "content": "This code is parsing command-line arguments, loading JSON files, ensuring paths are absolute, and checking common prefixes. It then defines a function to recursively add tree contents and initializes variables for selected keys and existing keys.",
        "type": "comment"
    },
    "354": {
        "file_id": 33,
        "content": "        if item[\"type\"] == \"directory\":\n            # subtree = parent.add(f\"[bold]{item['name']}\")\n            # subtree = parent.add(\n            #     Text.assemble((item[\"name\"], \"bold\")),\n            #     style=GREY,\n            #     guide_style=GREY,\n            # )\n            subtree = patch_missing_files(\n                os.path.join(basedir, item[\"name\"] + \"/\"),\n                basemap,\n                GREY,\n                lambda x: Text.assemble((x, \"bold\")),\n            )\n            existing_keys.append(os.path.join(basedir, item[\"name\"] + \"/\"))\n            # basemap[os.path.join(basedir, item[\"name\"] + \"/\")] = subtree\n            dirfs = 0\n            for fs in add_tree_contents(\n                subtree,\n                item.get(\"contents\", []),\n                os.path.join(basedir, item[\"name\"]),\n                basemap,\n            ):\n                dirfs += fs\n                yield fs\n            size_map[os.path.join(basedir, item[\"name\"] + \"/\")] = dirfs\n        else:  # file\n            # subtree = parent.add(item['name'])",
        "type": "code",
        "location": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/display_tree_structure.py:79-105"
    },
    "355": {
        "file_id": 33,
        "content": "If item type is directory:\n- Create a subtree for the directory name.\n- Add missing files to the subtree.\n- Append the directory path to existing keys.\n- Add the subtree to the base map.\n- Count and yield the number of files in the subtree.\n- Store the total file size in size_map.\nElse if item type is file:\n- Add the file name as a subtree.\n- Append the file path to existing keys.\n- Yield 1 (as there's only one file).",
        "type": "comment"
    },
    "356": {
        "file_id": 33,
        "content": "            # if item['name'] == \"devcontainer.json\": breakpoint()\n            existing_keys.append(os.path.join(basedir, item[\"name\"]))\n            filesize = os.path.getsize(\n                os.path.join(basepath, os.path.join(basedir, item[\"name\"]))\n            )\n            size_map[os.path.join(basedir, item[\"name\"])] = filesize\n            filesize_human = size_to_readable_string(filesize)\n            # subtree = patch_missing_files(os.path.join(basedir, item[\"name\"]),basemap, GREY, lambda x: f\"[{filesize_human}] \" + x)\n            subtree = parent.add(f\"x <{filesize_human}> \" + item[\"name\"], style=GREY)\n            basemap[os.path.join(basedir, item[\"name\"])] = subtree\n            yield filesize\ndef dirsplit(path):\n    if path.endswith(\"/\"):\n        path = path[:-1]\n    return os.path.split(path)\ndef set_path_to_white(path, basemap):\n    subtree = patch_missing_files(path, basemap, \"white\")\n    subtree.style = \"white\"\n    subtree.guide_style = \"white\"\n    return subtree\nselected_dirs = []\nline_map = {}",
        "type": "code",
        "location": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/display_tree_structure.py:106-134"
    },
    "357": {
        "file_id": 33,
        "content": "This code is creating a visual file selector by ignoring specific rules. It breaks when it encounters the \"devcontainer.json\" file, appends existing file paths to a list, calculates their sizes and converts them to readable strings, adds subtrees to a tree structure with gray color and file size information, and handles directories by setting their style to white. It also maintains two dictionaries: one for storing file sizes and another for mapping file paths to their corresponding subtrees in the tree structure.",
        "type": "comment"
    },
    "358": {
        "file_id": 33,
        "content": "# can have missing files.\ndef iterate_all_keys(contents, basemap, basedir=\".\"):\n    for item in contents:\n        if item[\"type\"] == \"directory\":\n            subpaths = item.get(\"contents\", [])\n            if subpaths:\n                dirlc = 0\n                cached_count = 0\n                # total_lc = 0\n                for lc in iterate_all_keys(\n                    subpaths, basemap, os.path.join(basedir, item[\"name\"])\n                ):\n                    # total_lc +=1\n                    if lc == -3:\n                        cached_count += 1\n                        continue\n                    dirlc += lc\n                    yield lc\n                if dirlc != 0:\n                    selected_dirs.append(os.path.join(basedir, item[\"name\"] + \"/\"))\n                    set_path_to_white(\n                        os.path.join(basedir, item[\"name\"] + \"/\"), basemap\n                    )\n                    line_map[os.path.join(basedir, item[\"name\"] + \"/\")] = dirlc\n                elif len(subpaths) == cached_count:",
        "type": "code",
        "location": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/display_tree_structure.py:137-161"
    },
    "359": {
        "file_id": 33,
        "content": "This function iterates through all keys in the \"contents\" dictionary, checking for directories and recursively calling itself to handle their contents. It keeps track of the total count of files and directories (lc) and yields each file or directory encountered. If a directory is found, it adds it to the selected_dirs list and updates the line_map with the count of files in that directory.",
        "type": "comment"
    },
    "360": {
        "file_id": 33,
        "content": "                    # elif total_lc == cached_count:\n                    subtree = set_path_to_white(\n                        os.path.join(basedir, item[\"name\"] + \"/\"), basemap\n                    )\n                    subtree.label = f\"[Cached] \" + item[\"name\"]\n                    cached_verified.append(os.path.join(basedir, item[\"name\"] + \"/\"))\n        else:  # file\n            # breakpoint()\n            selected_keys.append(os.path.join(basedir, item[\"name\"]))\n            linecount = read_file_and_get_line_count(\n                os.path.join(basepath, os.path.join(basedir, item[\"name\"]))\n            )\n            line_map[os.path.join(basedir, item[\"name\"])] = linecount\n            subtree = set_path_to_white(os.path.join(basedir, item[\"name\"]), basemap)\n            error = True\n            if linecount == 0:\n                label = \"Empty\"\n            elif linecount == -1:\n                label = \"Missing\"\n            elif linecount == -2:\n                label = \"Error\"\n            elif linecount == -3:",
        "type": "code",
        "location": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/display_tree_structure.py:162-184"
    },
    "361": {
        "file_id": 33,
        "content": "The code is checking the line count of a file and assigning a corresponding label to its tree structure representation. If the line count is 0, it's marked as \"Empty\", if it's -1, it's marked as \"Missing\", if it's -2, it's marked as \"Error\", and any other value is ignored. It also keeps track of cached directories and appends selected file paths to the `selected_keys` list.",
        "type": "comment"
    },
    "362": {
        "file_id": 33,
        "content": "                label = \"Cached\"\n                error = False\n                cached_verified.append(os.path.join(basedir, item[\"name\"]))\n            else:\n                label = f\"{linecount} L\"\n                error = False\n            if error:\n                error_map[label].append(os.path.join(basedir, item[\"name\"]))\n            else:\n                yield linecount\n            subtree.label = f\"[{label}] \" + item[\"name\"]\ndef read_file_and_get_line_count(filepath: str):\n    filepath = os.path.abspath(filepath)\n    if not os.path.exists(filepath):\n        return -1\n    if filepath in cached_paths:\n        return -3\n    try:\n        with open(filepath, \"r\") as f:\n            lines = f.readlines()\n            return len(lines)\n    except:\n        return -2\nselected_keys = []\ndef get_selected_keys(tree_data, basemap):\n    iterate_all_keys(tree_data[0].get(\"contents\", []), basemap)\n    return selected_keys\ntree = Tree(\".\")\n# tree = Tree(\"agi_computer_control\", style=GREY, guide_style=GREY)\nroot = tree_data[0]  # Assuming the first item in the JSON is the root directory",
        "type": "code",
        "location": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/display_tree_structure.py:185-222"
    },
    "363": {
        "file_id": 33,
        "content": "Line 184-221: Reads file names from the tree structure and verifies if they are cached or not. If not cached, it reads the line count of each file using the read_file_and_get_line_count function and appends to either cache list or error list. The yield statement returns line counts for non-error files.\nLine 185: Adds \"Cached\" label if item is in cached_paths, sets error flag as False, and appends file path to cached_verified list.\nLine 193: Adds the linecount label if it's not already an error file, sets error flag as False, and yields the linecount.\nLines 204-217: Reads the line count of the given filepath, handles non-existent files and cached files. Returns -1 for non-existing files, -3 for cached files.\nLine 219: Initializes selected_keys list, to be used in get_selected_keys function.\nLines 220-221: Recursively iterates through all keys in the tree structure and appends unique keys to selected_keys list.",
        "type": "comment"
    },
    "364": {
        "file_id": 33,
        "content": "mymap = {\"./\": tree}\nexisting_keys.append(\"./\")\ntotal_size = sum(add_tree_contents(tree, root.get(\"contents\", []), basemap=mymap))\nnonexist_keys = [k for k in mymap.keys() if k not in existing_keys]\nfor key in nonexist_keys:\n    it = mymap.get(key, None)\n    if it is not None:\n        parent, child = dirsplit(key)\n        parent_it = mymap.get(parent + \"/\", None)\n        if parent_it is not None:\n            parent_it.children.remove(it)\n        del mymap[key]\ntotal_lines = sum(iterate_all_keys(selected_json[0].get(\"contents\", []), mymap))\n# Print the tree\nsize_map[\"./\"] = total_size\nif total_lines != 0:\n    selected_dirs.append(\"./\")\n    line_map[\"./\"] = total_lines\ntree.label = Text.assemble(\n    (\n        (\n            f\"[{total_lines} L] \"\n            if total_lines != 0\n            else f\"x <{size_to_readable_string(total_size)}> \"\n        )\n        + tree.label,\n        \"magenta\",\n    )\n)\ndef estimate_time_from_lines(line_count: int):\n    seconds = (line_count / 10) * 100\n    return humanize.naturaltime(datetime.timedelta(seconds=seconds)).split(\" ago\")[0]",
        "type": "code",
        "location": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/display_tree_structure.py:223-261"
    },
    "365": {
        "file_id": 33,
        "content": "This code updates the map with the tree structure and its contents, removes nonexistent keys, calculates the total size and lines for existing directories, labels the tree with size and line count, and estimates time from lines.",
        "type": "comment"
    },
    "366": {
        "file_id": 33,
        "content": "for k, v in mymap.items():\n    if k.endswith(\"/\"):\n        _, name = dirsplit(k)\n        if k in cached_verified:\n            continue\n        elif k in selected_dirs:\n            v.label = f\"[{line_map[k]} L] \" + name\n            # v.label = f\"[{estimate_time_from_lines(line_map[k])}] \"+ name\n        else:\n            v.label = f\"x <{size_to_readable_string(size_map[k])}> \" + name\nconsole = Console()\nconsole.print(tree)\n# total_size = sum(size_map.values())\nselected_size  = 0\nfor k in selected_keys:\n    # try:\n    s =  size_map[k] \n    selected_size +=s\n    # except KeyError:\n    #     print(\"key\", k ,\"not found\")\n    #     breakpoint()\n# make mapping between displayed tree and actual tree\nprint(\n    dict(\n        total=size_to_readable_string(total_size),\n        selected=size_to_readable_string(selected_size),\n    )\n)\n# total_lines = sum(line_map.values())\nprocessing_time = estimate_time_from_lines(total_lines)\nprint(dict(selected_lines=humanize.intword(total_lines) + \" lines\", processing_time=processing_time))\ntotal_size_by_suffix = defaultdict(int)",
        "type": "code",
        "location": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/display_tree_structure.py:264-300"
    },
    "367": {
        "file_id": 33,
        "content": "Iterating through the dictionary mymap, setting label for directories in the tree based on their presence in cached_verified or selected_dirs, calculating selected size and total size of files, estimating processing time from lines, and storing it all in dictionaries for printing.",
        "type": "comment"
    },
    "368": {
        "file_id": 33,
        "content": "filecount_by_suffix = defaultdict(int)\nfor k, v in size_map.items():\n    suffix = os.path.split(k)[1].split(\".\")[-1]\n    if suffix == \"\":\n        suffix = \"<no suffix>\"\n    total_size_by_suffix[suffix] += v\n    filecount_by_suffix[suffix] += 1\nlines_by_suffix = defaultdict(int)\nselected_filecount_by_suffix = defaultdict(int)\nfor k in selected_keys:\n    suffix = os.path.split(k)[1].split(\".\")[-1]\n    if suffix == \"\":\n        suffix = \"<no suffix>\"\n    selected_filecount_by_suffix[suffix] += 1\n    v = line_map[k]\n    lines_by_suffix[suffix] += v\nprint(\n    dict(\n        total={k: size_to_readable_string(v) for k, v in total_size_by_suffix.items()},\n        # total=set(os.path.split(it)[1].split(\".\")[-1] for it in size_map.keys()),\n        selected={\n            k: humanize.intword(v) + \" lines\" for k, v in lines_by_suffix.items()\n        },\n    )\n)\nprint(dict(total=filecount_by_suffix, selected=selected_filecount_by_suffix))\nprint(\"error:\", {k: len(v) for k, v in error_map.items()})\nprint(\"error map:\", error_map)\n# print(mymap)",
        "type": "code",
        "location": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/display_tree_structure.py:301-329"
    },
    "369": {
        "file_id": 33,
        "content": "The code calculates and prints the total file count and line count by file suffix for all files (total) and selected files (selected). It also prints the number of errors encountered.",
        "type": "comment"
    },
    "370": {
        "file_id": 34,
        "content": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/file_copy_by_fd.py",
        "type": "filepath"
    },
    "371": {
        "file_id": 34,
        "content": "The code processes command line arguments, generates file lists using fd, iterates over paths to copy files into a target directory. It uses argparse for args handling, os and shutil for operations, subprocess for shell commands, and assertions for checking absolute paths. The process checks if the file exists and is a file, gets relative path, joins with target directory, creates new directories as needed, and copies using shutil.copy2().",
        "type": "summary"
    },
    "372": {
        "file_id": 34,
        "content": "import os\nimport shutil\nimport argparse\nparser = argparse.ArgumentParser()\n# parser.add_argument(\n#     \"-f\", \"--filelist\", help=\"path to filelist, generated by fd\", type=str, required=True\n# )\nparser.add_argument(\n    \"-b\", \"--basedir\", help=\"common prefix of filepaths\", type=str, required=True\n)\nparser.add_argument(\n    \"-t\",\n    \"--targetdir\",\n    help=\"target directory to copy files into\",\n    type=str,\n    required=True,\n)\nargs = parser.parse_args()\n# filelist = args.filelist\n# filepaths = open(filelist).read().split(\"\\n\")\nbasedir = args.basedir  # common prefix of filepaths\ncommand = [\"bash\", \"-c\", f\"cd '{basedir}' && fd -S '+1b'\"]\nassert os.path.isabs(basedir)\ntargetdir = args.targetdir  # target directory to copy files into\nassert os.path.isabs(targetdir)\nimport subprocess\nmoutput = subprocess.check_output(command, encoding='utf-8')\nfilepaths = moutput.split(\"\\n\")\nfor fp0 in filepaths:\n    fp0 = fp0.strip()\n    if fp0:\n        if fp0.endswith(\"/\"): continue\n        fp = os.path.join(basedir, fp0)\n        assert os.path.isabs(fp)",
        "type": "code",
        "location": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/file_copy_by_fd.py:1-41"
    },
    "373": {
        "file_id": 34,
        "content": "The code is parsing command line arguments, generating a file list using fd command, and then iterating over the file paths to copy them into a target directory. It uses argparse for handling command-line arguments, os and shutil libraries for file operations, subprocess for running shell commands, and assertions for checking file paths are absolute.",
        "type": "comment"
    },
    "374": {
        "file_id": 34,
        "content": "        assert os.path.exists(fp)\n        assert os.path.isfile(fp)\n        rel = os.path.relpath(fp, basedir)\n        new_path = os.path.join(targetdir, rel)\n        new_dir = os.path.dirname(new_path)\n        if not os.path.exists(new_dir):\n            os.makedirs(new_dir)\n        print(fp,\"->\", new_path)\n        # exit()\n        # breakpoint()\n        shutil.copy2(fp, new_path)",
        "type": "code",
        "location": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/file_copy_by_fd.py:42-52"
    },
    "375": {
        "file_id": 34,
        "content": "Checking if file exists and is a file, getting relative path, joining with target directory, creating new directory if needed, and copying the file using shutil.copy2().",
        "type": "comment"
    },
    "376": {
        "file_id": 35,
        "content": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/main.py",
        "type": "filepath"
    },
    "377": {
        "file_id": 35,
        "content": "The code creates a command-line tool for comparing files using the \"tree\" command, asynchronously runs commands, includes visualization for ignored files, features a progress bar and GUI updates, and manages UI elements while processing data or files. The code also contains two functions: one to toggle dark mode and another to exit the application.",
        "type": "summary"
    },
    "378": {
        "file_id": 35,
        "content": "from textual.app import App, ComposeResult\nfrom textual.widgets import Header, Footer, RichLog, Label\nfrom rich.text import Text\nfrom textual.timer import Timer\nfrom threading import Lock\nimport subprocess\n# from tempfile import TemporaryDirectory\nfrom jinja2 import Template\nfrom argparse import ArgumentParser\nfrom beartype import beartype\nfrom datetime import datetime\n# import os\nINTERVAL = 5\nimport asyncio\nasync def run_command(command:str):\n    process = await asyncio.create_subprocess_shell(\n        command,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE\n    )\n    stdout, stderr = await process.communicate()\n    return stdout.decode().strip(), stderr.decode().strip()\nscript_template_str = \"\"\"\ncd \"{{diffpath}}\"\nfd --no-ignore --hidden | tree --fromfile > \"{{tempdir}}/all_tree.txt\"\nfd | tree --fromfile > \"{{tempdir}}/selected_tree.txt\"\ndiff -y \"{{tempdir}}/all_tree.txt\" \"{{tempdir}}/selected_tree.txt\" > \"{{tempdir}}/diff_tree.txt\"\ncat \"{{tempdir}}/diff_tree.txt\"\n\"\"\"\n# tree output in json\n# load tree json, set selected & unselected properties",
        "type": "code",
        "location": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/main.py:1-36"
    },
    "379": {
        "file_id": 35,
        "content": "Code imports necessary modules and defines variables for a command-line tool that generates differences between selected and all files in a directory using the \"tree\" command. It also includes functions to run commands asynchronously and define a script template for generating the diff report.",
        "type": "comment"
    },
    "380": {
        "file_id": 35,
        "content": "# count file size\n# render tree json\nscript_template = Template(script_template_str)\nRELATIVE_TEMP_DIR_SCRIPT_PATH = \"script.sh\"\ndef parse_args():\n    parser = ArgumentParser()\n    parser.add_argument(\"-d\", \"--diffpath\", help=\"Path to visualize ignored files\")\n    args = parser.parse_args()\n    return args.diffpath\n@beartype\ndef render_script_template(diffpath: str, tempdir: str) -> str:\n    return script_template.render(diffpath=diffpath, tempdir=tempdir)\nprocessingLock = Lock()\nclass VisualIgnoreApp(App):\n    \"\"\"A Textual app to visualize ignore files.\"\"\"\n    BINDINGS = [(\"d\", \"toggle_dark\", \"Toggle dark mode\"), (\"e\", \"exit\", \"Exit\")]\n    timer: Timer\n    def __init__(self, diffpath, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.header = Header()\n        self.diffpath = diffpath\n        self.treeview = RichLog(auto_scroll=False)\n        self.footer = Footer()\n        # self.counter = 0\n        self.label = Label(Text.assemble((\"ETA:\", \"bold\")), expand=True)\n        self.label.styles.background = \"red\"",
        "type": "code",
        "location": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/main.py:37-74"
    },
    "381": {
        "file_id": 35,
        "content": "This code defines a class `VisualIgnoreApp` that visualizes ignored files. It takes a `diffpath` as input and renders a script template using the `render_script_template` function. The class also has a header, treeview, footer, and a label for displaying progress. A lock `processingLock` is used for synchronization.",
        "type": "comment"
    },
    "382": {
        "file_id": 35,
        "content": "        # self.label.styles.border = ('solid','red')\n        # self.label.styles.height = 3\n        self.label.styles.height = 1\n        # self.label.styles.dock = 'bottom'\n    async def progress(self):\n        locked = processingLock.acquire(blocking=False)\n        if locked: # taking forever. bad.\n            cont, _= await run_command(\n            # diff_content = subprocess.check_output(\n                f'python3 run_simple.py -d \"{self.diffpath}\"'\n                # [\"python3\", \"run_simple.py\", \"-d\", self.diffpath]\n            )\n            # cont = diff_content.decode()\n            has_error = False\n            # TODO: you may outsource this part to external process as well, emit as last line.\n            for it in cont.split(\"\\n\"):\n                if it.startswith(\"{\"):\n                    if \"processing_time\" in it and \"selected_lines\" in it:\n                        self.label.renderable = \"ETA: \"+it + \" \"+ datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                    if it.endswith(\"}\"):\n                        if \"Error\" in it or \"Empty\" in it or \"Missing\" in it:",
        "type": "code",
        "location": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/main.py:75-96"
    },
    "383": {
        "file_id": 35,
        "content": "Creating a progress bar for the file processing task by running external Python script and updating label with ETA information.",
        "type": "comment"
    },
    "384": {
        "file_id": 35,
        "content": "                            has_error  = True\n            if has_error:\n                self.label.renderable += \" [Error]\"\n            self.label.refresh()\n            # with TemporaryDirectory() as tempdir:\n            #     content = render_script_template(self.diffpath, tempdir)\n            #     script_path = os.path.join(tempdir, RELATIVE_TEMP_DIR_SCRIPT_PATH)\n            #     with open(script_path, \"w+\") as f:\n            #         f.write(content)\n            #     diff_content = subprocess.check_output(['bash', script_path])\n            # self.treeview.call_later\n            self.treeview.clear()\n            self.treeview.write(cont)  # newline by default.\n            processingLock.release()\n        # self.counter += 1\n    def compose(self) -> ComposeResult:\n        \"\"\"Create child widgets for the app.\"\"\"\n        return [self.header, self.treeview, self.label, self.footer]\n    def on_mount(self) -> None:\n        self.timer = self.set_interval(INTERVAL, self.progress)\n    def action_toggle_dark(self) -> None:",
        "type": "code",
        "location": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/main.py:97-124"
    },
    "385": {
        "file_id": 35,
        "content": "This code appears to be part of a larger application with multiple classes and methods. It defines a class, presumably for a GUI-based user interface, with methods like \"compose,\" \"on_mount,\" and \"action_toggle_dark.\" \n\nThe code block seems to handle an error condition, update the GUI label, clear the treeview and write new content, and potentially execute a script. It also has methods that define child widgets for the app, start an interval timer, and toggle dark mode. \n\nOverall, this code appears to be part of a more complex program with interactive user interface elements and functionality to process data or files.",
        "type": "comment"
    },
    "386": {
        "file_id": 35,
        "content": "        \"\"\"An action to toggle dark mode.\"\"\"\n        self.dark = not self.dark\n    def action_exit(self):\n        \"\"\"An action to exit the app.\"\"\"\n        self.exit()\ndef main():\n    diffpath = parse_args()\n    app = VisualIgnoreApp(diffpath)\n    app.run()\nif __name__ == \"__main__\":\n    main()",
        "type": "code",
        "location": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/main.py:125-140"
    },
    "387": {
        "file_id": 35,
        "content": "The code contains two functions: \"action_toggle_darkmode\" and \"action_exit\". The first function toggles the dark mode on or off, while the second one exits the application. The main function is responsible for parsing arguments, creating an instance of the VisualIgnoreApp class, and running the app.",
        "type": "comment"
    },
    "388": {
        "file_id": 36,
        "content": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/main_pyjom.py",
        "type": "filepath"
    },
    "389": {
        "file_id": 36,
        "content": "The code handles file operations, utilizes Tree widgets and UTF-8 encoding, manages errors, performs asynchronous reads and updates labels, calculates processing time, refreshes UI controls, compares selections, stops processes when needed, manages non-selected paths, performs file/directory checks, error handling, statistics updating, debug writing, monitors changes, handles GUI state & widgets, sets progress intervals, enables dark mode and exits the application.",
        "type": "summary"
    },
    "390": {
        "file_id": 36,
        "content": "# this version is for pyjom, our ultimate challenge.\n# TODO: show file extension counts \n# TODO: click extension name or error count for iteratively jumping to the next file with extension or error\n# TODO: type \"R\" to refresh the tree\n# TODO: filter empty files using fd\n# TODO: visualize unselected files by calling fd -u\n# TODO: add visualization of tree files.\n# TODO: add action to restart the processing thread\n# TODO: mark if file is not utf-8 encoded (as binary?) even if not selected\n# TODO: exit with error if previous error counters are not zeros.\n# to find empty files:\n# fd -S \"-1b\"\n# import sys\n# filter out empty files:\n# fd -S \"+1b\"\nimport humanize\nimport numpy\nfrom textual.app import App, ComposeResult\nfrom textual.widgets import Header, Footer, Tree, Label\nfrom rich.text import Text\nfrom textual.timer import Timer\nfrom threading import Lock\n# from tempfile import TemporaryDirectory\nfrom jinja2 import Template\nfrom argparse import ArgumentParser\nfrom beartype import beartype\nfrom datetime import datetime, timedelta",
        "type": "code",
        "location": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/main_pyjom.py:1-35"
    },
    "391": {
        "file_id": 36,
        "content": "This code is for a pyjom application, which seems to involve file management and visualization using Tree widgets. The TODO list suggests additional features like showing extension counts, filtering empty files, and adding actions for restarting processing. It also handles UTF-8 encoding and error counters.",
        "type": "comment"
    },
    "392": {
        "file_id": 36,
        "content": "import os\ncached_paths = []\nIGNORE_RULE_FILES = (\".gitignore\", \".fdignore\", \".ignore\") # TODO: set fd to respect .gitignore even if without .git folders\nDOCS_FOLDER_NAME = \"docs\"\nINTERVAL = 0.1\nSLEEP=7\nimport asyncio\ndef format_timedelta(td):\n    hours, remainder = divmod(td.seconds, 3600)\n    minutes, seconds = divmod(remainder, 60)\n    return f\"{hours}:{minutes}:{seconds}\"\ndef estimate_time_from_lines(line_count: int):\n    seconds = (line_count / 35) * 60\n    return seconds\ndef naturaltime(seconds):\n    return humanize.naturaltime(timedelta(seconds=seconds)).split(\" ago\")[0]\ndef estimate_time_from_filesize(filesize: int):\n    seconds = (filesize / 1000) * 60\n    return seconds\nscript_template_str = \"\"\"\ncd \"{{diffpath}}\"\nfd --no-ignore --hidden | tree --fromfile > \"{{tempdir}}/all_tree.txt\"\nfd | tree --fromfile > \"{{tempdir}}/selected_tree.txt\"\ndiff -y \"{{tempdir}}/all_tree.txt\" \"{{tempdir}}/selected_tree.txt\" > \"{{tempdir}}/diff_tree.txt\"\ncat \"{{tempdir}}/diff_tree.txt\"\n\"\"\"\n# tree output in json\n# load tree json, set selected & unselected properties",
        "type": "code",
        "location": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/main_pyjom.py:36-71"
    },
    "393": {
        "file_id": 36,
        "content": "This code defines functions for estimating time based on line count or file size, and includes a script template for comparing a selected folder with all files using the fd command. It also utilizes the humanize module to convert time values into natural language strings.",
        "type": "comment"
    },
    "394": {
        "file_id": 36,
        "content": "# count file size\n# render tree json\nscript_template = Template(script_template_str)\nRELATIVE_TEMP_DIR_SCRIPT_PATH = \"script.sh\"\nimport aiofiles\ndef expand_parent(elem):\n    elem.expand()\n    if not elem.is_root:\n        expand_parent(elem.parent)\nasync def read_file_and_get_line_count(filepath: str):\n    filepath = os.path.abspath(filepath)\n    if not os.path.exists(filepath):\n        return -1\n    if filepath in cached_paths:\n        return -3\n    try:\n        readable = False\n        async with aiofiles.open(filepath, mode='r', encoding='utf-8') as f:\n            _ = await f.readline()\n            readable = True\n        if readable:\n            lc = 0\n            # use 'cat' & 'wc -l'\n            cmd = ['wc', '-l', filepath]\n            p = await asyncio.create_subprocess_exec(*cmd, stdout=asyncio.subprocess.PIPE)\n            line = await p.stdout.read()\n            decline = line.decode().strip()\n            # with open(\"lc.txt\", 'w+') as f:\n            #     f.write(decline)\n            #     exit()\n            #     # sys.exit()",
        "type": "code",
        "location": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/main_pyjom.py:72-107"
    },
    "395": {
        "file_id": 36,
        "content": "This code reads a file and gets its line count. It checks if the file exists and if it is already in the cache to avoid unnecessary operations. If the file exists, it uses the 'cat' and 'wc -l' commands to get the line count. The code also includes error handling for cases where the file doesn't exist or has been cached before.",
        "type": "comment"
    },
    "396": {
        "file_id": 36,
        "content": "            lc = decline.split(' ')[0]\n            lc = int(lc)\n            await p.wait()\n            return lc if lc else 1\n    except:\n        return -2\nfrom collections import defaultdict\n# def patch_missing_files(path, basemap, expand=False, ):\ndef patch_missing_files(path, basemap, expand=False, processor=lambda x: x):\n    subpath, filename = dirsplit(path)\n    # breakpoint()\n    if basemap.get(path) is None:\n        subtree, _, _ = patch_missing_files(subpath + \"/\", basemap, processor = processor)\n        # renderable = Text.assemble((processor(filename), color))\n        if path.endswith(\"/\"):\n            subsubtree = subtree.add(processor(filename), expand=expand)\n        else:\n            subsubtree = subtree.add_leaf(processor(filename))\n        # subsubtree = subtree.add(processor(filename), expanded=expanded,style=color, guide_style=color)\n        # print(filename)\n        basemap[path] = subsubtree\n        return subsubtree, filename, False\n    else:\n        return basemap.get(path), filename, True",
        "type": "code",
        "location": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/main_pyjom.py:108-132"
    },
    "397": {
        "file_id": 36,
        "content": "Function `patch_missing_files` recursively searches for missing files in a directory and its subdirectories, creating placeholders if necessary. It takes a path, base map, and processor function as input and returns the updated base map, filename, and a flag indicating whether the file was found or not. If the file is found, it doesn't create a placeholder. The processor function can be used to modify filenames according to specific rules.",
        "type": "comment"
    },
    "398": {
        "file_id": 36,
        "content": "async def get_file_size(filename):\n    try:\n        async with aiofiles.open(filename, mode='rb') as file:\n            file_size = os.fstat(file.fileno()).st_size\n            return file_size\n    except:\n        return -1\ndef parse_args():\n    parser = ArgumentParser()\n    parser.add_argument(\"-d\", \"--diffpath\", help=\"Path to visualize ignored files\")\n    parser.add_argument(\"-s\", \"--skip\", help=\"Skip visualization\", action=\"store_true\")\n    args = parser.parse_args()\n    if args.skip: \n        print(\"Skipping visualization\")\n        exit(0)\n    return args.diffpath\ndef dirsplit(path):\n    if path.endswith(\"/\"):\n        path = path[:-1]\n    return os.path.split(path)\ndef iterate_parent_dirs(path):\n    parts = path.split(\"/\")\n    for i in range(1, len(parts)):\n        yield \"/\".join(parts[:i])+\"/\", parts[i-1]\n@beartype\ndef render_script_template(diffpath: str, tempdir: str) -> str:\n    return script_template.render(diffpath=diffpath, tempdir=tempdir)\nprocessingLock = Lock()\nclass VisualIgnoreApp(App):\n    \"\"\"A Textual app to visualize ignore files.\"\"\"",
        "type": "code",
        "location": "/document_agi_computer_control/visual_file_selector_by_ignore_rules/main_pyjom.py:134-172"
    },
    "399": {
        "file_id": 36,
        "content": "The code defines a function `get_file_size` that reads the file size asynchronously. It also includes `parse_args`, which handles command line arguments, and `dirsplit` that splits paths into directories and filenames. The `iterate_parent_dirs` function yields parent directory paths, and there's a lock object `processingLock`. Finally, the `VisualIgnoreApp` class inherits from `Textual App` and is used for visualizing ignore files.",
        "type": "comment"
    }
}