{
    "200": {
        "file_id": 19,
        "content": "    data.update(new_data)\ndef strip_quote(s: str):\n    if s[0] == s[-1]:\n        if s[0] in ['\"', \"'\"]:\n            return s[1:-1].strip()\n    return s.strip().strip(\".\")\nfrom tinydb import TinyDB, Query\ncache_title = TinyDB(os.path.join(source_dir, \"cache_title.json\"))\ntitle_split_dir = os.path.join(source_dir, \"data/titles\")\nmetadata_title_path = os.path.join(source_dir, \"metadata_title.json\")\nimport shutil\nif not os.path.exists(title_split_dir):\n    os.makedirs(title_split_dir)\nelse:\n    shutil.rmtree(title_split_dir)\n    os.makedirs(title_split_dir)\nif not os.path.isdir(title_split_dir):\n    raise Exception(\n        f\"'{title_split_dir}' (where splited titles stored) must be a directory\"\n    )\n# structure:\n# [filepath] [summary] [code] [comment] ...\ntitle_data = {}\nfile_mapping_detail = {}\ndata_count = len(data.keys())\nimport hashlib\ndef hash_key(summary: str):\n    enc = summary.strip()\n    if enc:\n        # Generate a hash for the given summary\n        hash_object = hashlib.md5(enc.encode())\n        return hash_object.hexdigest()",
        "type": "code",
        "location": "/document_agi_computer_control/title_generator/main.py:42-87"
    },
    "201": {
        "file_id": 19,
        "content": "The code initializes a TinyDB for storing cache data, checks and creates the title_split directory if it does not exist, defines a function to strip quotes, and then hashes a given summary using MD5.",
        "type": "comment"
    },
    "202": {
        "file_id": 19,
        "content": "def ask_llm_for_title(path: str, comment: str):\n    init_prompt = \"\"\"You are a professional title writer. You can write a concise, conclusive and meaningful title within 3 to 7 words. You will be given a piece of content, a path that refers to the content and produce a single title.\n\"\"\"\n    with llm_context(init_prompt) as model:\n        prompt = f\"\"\"Content:\n{comment}\nPath of the content: {path}\nTitle within 3 to 7 words (do not quote the title, just write it out):\n\"\"\"\n        ret = model.run(prompt).strip()\n        ret = strip_quote(ret)\n    return ret\ndef generate_title_and_update_to_result(\n    path: str, comment: str, result_dict: dict[str, str]\n):\n    comment_hash = hash_key(comment)\n    doc = cache_title.get((Query().hash == comment_hash) and (Query().path == path))\n    if doc:\n        mtitle = doc[\"title\"]\n    else:\n        mtitle = ask_llm_for_title(path, comment)\n        cache_title.upsert(\n            dict(path=path, hash=comment_hash, title=mtitle), cond=Query().path == path\n        )\n    result_dict[path] = mtitle",
        "type": "code",
        "location": "/document_agi_computer_control/title_generator/main.py:90-118"
    },
    "203": {
        "file_id": 19,
        "content": "The code defines a function `ask_llm_for_title` that utilizes LLM (Language Model) to generate titles for given content and its path. It also includes the `generate_title_and_update_to_result` function, which retrieves an existing title from the cache or generates a new one using `ask_llm_for_title`, and updates it in the result dictionary.",
        "type": "comment"
    },
    "204": {
        "file_id": 19,
        "content": "for k, v in file_mapping.items():\n    # end_id is exclusive.\n    if str(int(k) + 1) in file_mapping.keys():\n        end_id = int(file_mapping[str(int(k) + 1)][\"entry_id\"])\n    else:\n        end_id = data_count\n    file_mapping_detail[k] = {\n        \"filepath\": v[\"filepath\"],\n        \"span\": {\"start\": int(v[\"entry_id\"]), \"end\": end_id},\n    }\nfile_count = len(file_mapping.keys())\nprint(f\"\\n>>>> PROCESSING PROGRESS: 0/{file_count}\")\nfor i in range(file_count):\n    try:\n        it = file_mapping_detail[str(i)]\n        start, end = it[\"span\"][\"start\"], it[\"span\"][\"end\"]\n        split_count = (end - start - 2) / 2\n        split_count = int(split_count)\n        # generate for file summary title first.\n        generate_title_and_update_to_result(\n            data[str(start)][\"content\"], data[str(start + 1)][\"content\"], title_data\n        )\n        if split_count == 1:  # only generate for file summary\n            continue\n        else:\n            # generate for splits\n            for j in range(split_count):\n                generate_title_and_update_to_result(",
        "type": "code",
        "location": "/document_agi_computer_control/title_generator/main.py:121-149"
    },
    "205": {
        "file_id": 19,
        "content": "The code iterates through a file_mapping dictionary, creates a file_mapping_detail dictionary with start and end IDs for each file, calculates the number of splits needed based on end - start - 2, generates a title for the first entry and updates the result, and if there is only one split, it continues to the next iteration. Otherwise, it generates titles for all splits.",
        "type": "comment"
    },
    "206": {
        "file_id": 19,
        "content": "                    data[str(start + 2 + j * 2)][\"location\"],\n                    data[str(start + 3 + j * 2)][\"content\"],\n                    title_data,\n                )\n    finally:\n        print(f\"\\n>>>> PROCESSING PROGRESS: {i+1}/{file_count}\")\n# split and store file summaries.\nprint(\"Spliting and storing titles...\")\ntitle_split_count = 0\nimport json\nfor i, chunk in enumerate(split_dict_into_chunks(title_data, 300)):\n    title_split_count += 1\n    with open(os.path.join(title_split_dir, f\"{i}.json\"), \"w+\") as f:\n        f.write(json.dumps(chunk, indent=4, ensure_ascii=False))\nprint(\"Storing title metadata...\")\nwith open(metadata_title_path, \"w+\") as f:\n    f.write(json.dumps(dict(split_count=title_split_count)))\nprint(\"Finished title generation.\")",
        "type": "code",
        "location": "/document_agi_computer_control/title_generator/main.py:150-170"
    },
    "207": {
        "file_id": 19,
        "content": "This code processes and stores file summaries, splits them into chunks of 300, and writes each chunk to a separate JSON file. It also keeps track of the total split count in a metadata file. The progress is printed at regular intervals.",
        "type": "comment"
    },
    "208": {
        "file_id": 20,
        "content": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/high_level.py",
        "type": "filepath"
    },
    "209": {
        "file_id": 20,
        "content": "The code initializes a TinyDB, creates functions for brief generation based on summaries, generates hashes for summaries, and updates file and directory briefs using the generated hashes. It then iterates through file summaries and directories, updating their briefs if necessary.",
        "type": "summary"
    },
    "210": {
        "file_id": 20,
        "content": "import os\nimport hashlib\nfrom tinydb import TinyDB, Query\n# Initialize TinyDB\ndb = TinyDB('briefs_db.json')\ndef generate_file_summary_brief(filepath, summary):\n    # Generate a brief for the file based on its summary\n    # ...\ndef generate_directory_summary_brief(directory_path, children_briefs):\n    # Generate a brief for the directory based on its direct children's briefs\n    # ...\ndef hash_summary(summary):\n    # Generate a hash for the given summary\n    hash_object = hashlib.md5(summary.encode())\n    return hash_object.hexdigest()\ndef update_file_briefing(filepath, summary):\n    # Check if a matching briefing exists for the hash of the summary\n    # If not, update the briefing for the file\n    # ...\ndef update_directory_briefing(directory_path, children_briefs):\n    # Concatenate and sort the briefs of direct children before hashing\n    # Check if a matching briefing exists for the hash of the concatenated children briefs\n    # If not, update the briefing for the directory\n    # ...\n# Iterate through file summaries and update briefs",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/high_level.py:1-32"
    },
    "211": {
        "file_id": 20,
        "content": "This code initializes a TinyDB, defines functions to generate briefs for files and directories based on their summaries, generates hashes for summaries, updates file and directory briefings using the generated hashes. It then iterates through file summaries, updating briefs if necessary.",
        "type": "comment"
    },
    "212": {
        "file_id": 20,
        "content": "for filepath, summary in file_summaries.items():\n    update_file_briefing(filepath, summary)\n# Iterate through directories and their direct children to update briefs\nfor directory_path, children_briefs in directory_children_briefs.items():\n    update_directory_briefing(directory_path, children_briefs)",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/high_level.py:33-38"
    },
    "213": {
        "file_id": 20,
        "content": "Iterates through file summaries and updates briefs, followed by directories and their direct children's briefs.",
        "type": "comment"
    },
    "214": {
        "file_id": 21,
        "content": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main.py",
        "type": "filepath"
    },
    "215": {
        "file_id": 21,
        "content": "This Python script generates a filesystem hierarchy in markdown format from JSON data, using AI-generated summaries for directories and file_mapping for files. It also offers optional enhancements to generate tree.json and tree.html, with Jinja2 templates for rendering an organized visualization of project structure in HTML.",
        "type": "summary"
    },
    "216": {
        "file_id": 21,
        "content": "# demo logic to generate filesystem hierarchy in markdown\n# TODO: diff and line markers shifts based reprocessing: just process the changed part instead of the whole file again\n# TODO: calculate code duplication percent across directories, prefer files by timestamp or size\n# TODO: show the total stage progress like [Stage 1/4], [Stage 2/4]\n# TODO: generate sitemap\n# TODO: modify all titles in all pages to contain full project name and project description (more informative titles)\n# TODO: print progress info during directory brief generation process\n# TODO: provide a brief view to file chunks.\n# TODO: provide an AST view (language specific) to file chunks.\n# TODO: make our prompt into json to formalize the input structure, and parse the output as json\n# language specific shall be built on language agnostic\nimport os\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-s\", \"--source_dir\", type=str, required=True)\nargs = parser.parse_args()\n# the only parameter.\nsource_dir = args.source_dir",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main.py:1-25"
    },
    "217": {
        "file_id": 21,
        "content": "This code defines a Python script that generates a filesystem hierarchy in markdown format. It accepts the source directory as an argument and includes several TODOs for future enhancements such as diff handling, code duplication calculation, progress display, file chunks view, AST view, and language-specific features.",
        "type": "comment"
    },
    "218": {
        "file_id": 21,
        "content": "assert os.path.exists(source_dir)\nassert os.path.isdir(source_dir)\nassert os.path.isabs(source_dir)\nfrom collections import defaultdict\nimport json\nimport urllib.parse\nimport sys\nsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"../\"))\nfrom llm import llm_context\nmetadata = json.loads(open(os.path.join(source_dir, \"metadata.json\"), \"r\").read())\nfile_mapping = metadata[\"file_mapping\"]\nsplit_count = metadata[\"split_count\"]\nproject_name = metadata[\"project_name\"]\ndata = {}\nfor i in range(split_count):\n    new_data = json.loads(open(os.path.join(source_dir, f\"data/{i}.json\"), \"r\").read())\n    data.update(new_data)\ndef strip_quote(s: str):\n    if s[0] == s[-1]:\n        if s[0] in ['\"', \"'\"]:\n            return s[1:-1].strip()\n    return s.strip().strip('.')\n# read metadata.json & data/*.json\n# create and read some cache_tree.json, which you may want to include in .gitignore\n# produce tree.json\n# copy tree.html\nimport html.entities\nhtml5_escapes = html.entities.html5\nhtml_escape_mapping = {}\nfor k,v in html5_escapes.items():",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main.py:27-67"
    },
    "219": {
        "file_id": 21,
        "content": "The code reads metadata and data from JSON files, creates a cache_tree.json file, produces tree.json, and copies the content of tree.html. It also includes functions for stripping quotes and mapping HTML5 escapes.",
        "type": "comment"
    },
    "220": {
        "file_id": 21,
        "content": "    if k.endswith(\";\"):  html_escape_mapping[v] = \"&\"+k\ndef html_escape(s: str):\n    ret = \"\"\n    for elem in s:\n        if elem in html_escape_mapping.keys():\n            ret += html_escape_mapping[elem]\n        else:\n            ret += elem\n    return ret\nimport hashlib\ndef hash_key(summary: str):\n    enc = summary.strip()\n    if enc:\n        # Generate a hash for the given summary\n        hash_object = hashlib.md5(enc.encode())\n        return hash_object.hexdigest()\nimport tinydb\ncache_tree = tinydb.TinyDB(os.path.join(source_dir, \"cache_tree.json\"))\ndef generate_file_summary_brief(filepath, summary):\n    # Generate a brief for the file based on its summary\n    stripped_summary = summary.strip()\n    if stripped_summary:\n        prompt = f\"\"\"\nFilepath: {filepath}\nSummary:\n{stripped_summary}\nBrief in 7 words (do not quote your brief, just write it out):\n\"\"\"\n        mhash = hash_key(prompt)\n        rec = cache_tree.get(\n            (tinydb.Query().hash == mhash) and (tinydb.Query().path == filepath)\n        )\n        if rec:",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main.py:68-112"
    },
    "221": {
        "file_id": 21,
        "content": "This code snippet generates a brief for a file based on its summary. It checks if the summary is not empty and creates a hash of the prompt using the filepath and stripped summary. The code then retrieves a record from the cache tree using the generated hash and file path, updating it with a new brief if it exists.",
        "type": "comment"
    },
    "222": {
        "file_id": 21,
        "content": "            return rec[\"brief\"]\n        else:\n            init_prompt = \"\"\"You are a professional brief writer. You can turn long summaries into a single short, concise, conclusive and meaningful brief within 7 words. You will be given a filepath, a summary of the file and produce a concise brief that best describes the file.\"\"\"\n            with llm_context(init_prompt) as model:\n                mbrief = strip_quote(model.run(prompt).strip())\n            mdoc = dict(path=filepath, hash=mhash, brief=mbrief)\n            cache_tree.upsert(mdoc, cond=tinydb.Query().path == filepath)\n            return mbrief\n    return \"\"\ndef generate_tree_repesentation(\n    directory_path: str,\n    childrens_mapping: dict[str, set[str]],\n    file_briefs: dict[str, str],\n    directory_briefs: dict[str, str],\n    indent=0,\n    briefs=[],\n):\n    childrens = list(childrens_mapping[directory_path])\n    childrens.sort()\n    if directory_path == \"/\":\n        name = project_name\n    else:\n        name = directory_path.strip(\"/\").split(\"/\")[-1]",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main.py:113-137"
    },
    "223": {
        "file_id": 21,
        "content": "This code snippet generates a tree representation of a file or directory structure. It first checks if a brief is available for the file, and if not, it uses an AI model to generate one. Then, it builds the tree representation using the provided parameters and returns the generated briefs.",
        "type": "comment"
    },
    "224": {
        "file_id": 21,
        "content": "    mbrief, show = directory_briefs[directory_path]\n    mbrief = strip_quote(mbrief)\n    briefs.append(\n        \" \" * indent * 4\n        + f'- <span hierarchy=\"{indent}\" class=\"expanded\" onclick=\"toggleVisibility(this)\" ><strong class=\"directory\" id=\"{directory_path}\"><code>{html_escape(name)}</code></strong>'\n        + (\"\" if not show else f\" <em>{mbrief}</em>\")\n        + \"</span>\"\n        # \" \" * indent * 4 + f\"- **`{name}`**\" + (\"\" if not show else f\" <em>{mbrief}</em>\")\n    )\n    for child in childrens:\n        child_name = child.strip(\"/\").split(\"/\")[-1]\n        if child.endswith(\"/\"):\n            # mbrief, show= directory_briefs[child]\n            # briefs.append(\n            #     \" \" * (indent + 1) * 4\n            #     + f\"- **`{child_name}`**\"+(\"\" if not show else f\" *{mbrief}*\")\n            # )\n            generate_tree_repesentation(\n                child,\n                childrens_mapping,\n                file_briefs,\n                directory_briefs,\n                indent + 1,\n                briefs,",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main.py:138-162"
    },
    "225": {
        "file_id": 21,
        "content": "This function generates a hierarchical representation of file and directory structure. It uses recursion to handle nested directories, appends the names and brief descriptions to a 'briefs' list, and handles HTML formatting for display.",
        "type": "comment"
    },
    "226": {
        "file_id": 21,
        "content": "            )\n        else:\n            child_link = f\"index.html?q={urllib.parse.quote(child)}\"\n            briefs.append(\n                \" \" * (indent + 1) * 4\n                + f'- <a href=\"{child_link}\" id=\"{child}\"><code>{html_escape(child_name)}</code></a> <em>{strip_quote(file_briefs[child])}</em>'\n            )\n    return briefs\ndef generate_directory_summary_brief(\n    directory_path,\n    childrens_mapping: dict[str, set[str]],\n    file_briefs: dict[str, str],\n    directory_briefs={},\n):\n    # Generate a brief for the directory based on its direct children's briefs\n    childrens = list(childrens_mapping[directory_path])\n    if len(childrens) == 0:\n        raise Exception(f\"Directory '{directory_path}' has no children\")\n    if len(childrens) == 1:\n        if childrens[0].endswith(\"/\"):\n            generate_directory_summary_brief(\n                childrens[0], childrens_mapping, file_briefs, directory_briefs\n            )\n            mbrief = directory_briefs[childrens[0]][0]\n        else:\n            mbrief = file_briefs[childrens[0]]",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main.py:163-191"
    },
    "227": {
        "file_id": 21,
        "content": "This function generates a brief for a directory based on its direct children's briefs. If the directory has no children, it raises an exception. If it has only one child, it recursively calls itself to generate a brief for the child directory or file.",
        "type": "comment"
    },
    "228": {
        "file_id": 21,
        "content": "        directory_briefs[directory_path] = (mbrief, False)\n    else:\n        subprompt_parts = []\n        children_briefs = {}\n        for child in childrens:\n            if child.endswith(\"/\"):\n                generate_directory_summary_brief(\n                    child, childrens_mapping, file_briefs, directory_briefs\n                )\n                cbrief = directory_briefs[child][0]\n            else:\n                cbrief = file_briefs[child]\n            children_briefs[child] = cbrief\n        candidates = list(children_briefs.items())\n        candidates.sort(key=lambda x: x[0])\n        for k, v in candidates:\n            if not k.endswith(\"/\"):\n                mark = \"file\"\n            else:\n                mark = \"directory\"\n            relpath = os.path.relpath(k, directory_path)\n            it = f\"Brief for {mark} '{relpath}': {v}\"\n            subprompt_parts.append(it)\n        subprompt = \"\\n\".join(subprompt_parts)\n        prompt = f\"\"\"\n{subprompt}\nBrief for directory '{directory_path}' in 7 words (do not quote your brief, just write it out):",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main.py:192-219"
    },
    "229": {
        "file_id": 21,
        "content": "This code is generating a summary brief for each directory and file in the given folder hierarchy. It checks if the item is a directory or a file, and then sorts them based on their names. The brief includes the type of item (directory or file) and its relative path, and prompts the user to provide a 7-word brief description for the directory.",
        "type": "comment"
    },
    "230": {
        "file_id": 21,
        "content": "\"\"\"\n        mhash = hash_key(prompt)\n        rec = cache_tree.get(\n            (tinydb.Query().hash == mhash) and (tinydb.Query().path == directory_path)\n        )\n        if rec:\n            mbrief = rec[\"brief\"]\n        else:\n            init_prompt = \"\"\"You are a professional brief writer. You can turn a list of briefs into a single short, concise, conclusive and meaningful brief within 7 words. You will be given a list of briefs and relative paths of the directory children and produce a concise brief that best describes the directory.\"\"\"\n            with llm_context(init_prompt) as model:\n                mbrief = strip_quote(model.run(prompt).strip())\n            mdoc = dict(path=directory_path, hash=mhash, brief=mbrief)\n            cache_tree.upsert(mdoc, cond=tinydb.Query().path == directory_path)\n        directory_briefs[directory_path] = (mbrief, True)\n    return directory_briefs\nfile_summaries = {\n    v[\"filepath\"]: data[str(v[\"entry_id\"] + 1)][\"content\"]\n    for v in file_mapping.values()\n}\n# print(file_summaries)",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main.py:220-242"
    },
    "231": {
        "file_id": 21,
        "content": "This code retrieves a brief description for a directory based on its path and hash value using an LLM (Language Model) context. If the record already exists in the cache_tree, it fetches the brief from there; otherwise, it prompts the LLM to generate a brief and stores it in the cache_tree before adding it to the directory_briefs dictionary. It also generates file summaries for the files in the given file_mapping.",
        "type": "comment"
    },
    "232": {
        "file_id": 21,
        "content": "# file_briefs = {k: generate_file_summary_brief(k, v) for k, v in file_summaries.items()}\nfile_briefs = {}\nitems_count = len(file_summaries.keys())\nprint(f\"\\n>>>> PROCESSING PROGRESS: 0/{items_count}\")\ncounter = 0\nfor k, v in file_summaries.items():\n    file_briefs[k] = generate_file_summary_brief(k, v)\n    counter += 1\n    print(f\"\\n>>>> PROCESSING PROGRESS: {counter}/{items_count}\")\nchildrens_mapping = defaultdict(set)\nfor k in file_summaries.keys():\n    print(k)\n    split_k = k.split(\"/\")\n    print(split_k)  # [dir1, dir2, ... filename]\n    # add \"/\" to the right and left of dir.\n    for i in range(len(split_k) - 1):\n        parent = \"/\".join(split_k[: i + 1]) + \"/\"\n        child = parent + split_k[i + 1]\n        if i != len(split_k) - 2:  # is directory:\n            child += \"/\"\n        print({\"i\": i, \"parent\": parent, \"child\": child, \"k\": k})\n        childrens_mapping[parent].add(child)\n# breakpoint()\ndirectory_briefs = generate_directory_summary_brief(\"/\", childrens_mapping, file_briefs)\n# now, let's generate the representation.",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main.py:244-273"
    },
    "233": {
        "file_id": 21,
        "content": "The code generates a directory hierarchy brief by iterating through file summaries, creating a mapping of child directories and files, and generating directory and file briefs using the mapping. It then uses these briefs to generate the final representation of the folder hierarchy.",
        "type": "comment"
    },
    "234": {
        "file_id": 21,
        "content": "briefs = generate_tree_repesentation(\n    \"/\", childrens_mapping, file_briefs, directory_briefs\n)\n# briefs.insert(0,\"# Project Structure:\")\nbriefs.insert(\n    0,\n    f'## Project Structure<span hierarchy=\"0\" class=\"partial-repository-url\"> of: {metadata[\"url\"][\"partial\"]}</span><div style=\"float: right;\"><a href=\"tree.html?full=true\"><i class=\"bi bi-arrow-down-right-circle\"></i></a><a href=\"index.html\"><i class=\"bi bi-search\"></i></a></div>',\n)\nprint(\"=\" * 40)\nprint(\"\\n\".join(briefs))\n### building\n# render README.md into index.html\nimport markdown\nfrom jinja2 import Template\n# Markdown content\nmarkdown_content = \"\\n\".join(briefs)\n# Convert Markdown to HTML\nhtml_content = markdown.markdown(markdown_content)\ntemplate_path = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"tree.html.j2\")\ncss_path = os.path.join(\n    os.path.abspath(os.path.dirname(__file__)), \"github-markdown.css\"\n)\ntemplate = Template(open(template_path, \"r\").read())\n# Render the template with the data\nrendered_template = template.render(content=html_content)",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main.py:274-303"
    },
    "235": {
        "file_id": 21,
        "content": "This code generates a tree representation of a project structure and then converts it into Markdown format. The generated Markdown content is used to create HTML using Jinja2 templates, and the final result is written to \"tree.html\" file. It helps display the project hierarchy in an organized manner for better understanding.",
        "type": "comment"
    },
    "236": {
        "file_id": 21,
        "content": "print(\"Template rendered.\")\ntree_fname = \"tree.html\"\n# Write the template content to a file\nwith open(os.path.join(source_dir, tree_fname), \"w+\", encoding=\"utf-8\") as file:\n    file.write(rendered_template)\nimport shutil\nshutil.copy(css_path, source_dir)\nprint(\n    f\"Markdown converted to HTML and written to {os.path.join(source_dir, tree_fname)}\"\n)",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main.py:305-318"
    },
    "237": {
        "file_id": 21,
        "content": "This code renders a template, writes it to a file along with copied CSS, and converts Markdown to HTML.",
        "type": "comment"
    },
    "238": {
        "file_id": 22,
        "content": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main_recursive.py",
        "type": "filepath"
    },
    "239": {
        "file_id": 22,
        "content": "The script generates a markdown filesystem hierarchy, supports multiple languages, and has TODO functionality. It uses metadata.json and data/*.json for structure, retrieves file info from LLM model, stores hashes in TinyDB, and creates a searchable tree representation with HTML briefs.",
        "type": "summary"
    },
    "240": {
        "file_id": 22,
        "content": "# demo logic to generate filesystem hierarchy in markdown\n# TODO: diff and line markers shifts based reprocessing: just process the changed part instead of the whole file again\n# TODO: calculate code duplication percent across directories, prefer files by timestamp or size\n# TODO: show the total stage progress like [Stage 1/4], [Stage 2/4]\n# TODO: generate sitemap\n# TODO: modify all titles in all pages to contain full project name and project description (more informative titles)\n# TODO: print progress info during directory brief generation process\n# TODO: provide a brief view to file chunks.\n# TODO: provide an AST view (language specific) to file chunks.\n# TODO: make our prompt into json to formalize the input structure, and parse the output as json\n# language specific shall be built on language agnostic\nimport os\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-s\", \"--source_dir\", type=str, required=True)\nargs = parser.parse_args()\n# the only parameter.\nsource_dir = args.source_dir",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main_recursive.py:1-25"
    },
    "241": {
        "file_id": 22,
        "content": "The code is a Python script that generates a filesystem hierarchy in markdown format. It requires the source directory as input and provides various TODO features, including diff and line marker shift-based reprocessing, calculating code duplication percent across directories, generating sitemaps, modifying titles with project information, printing progress info during directory brief generation, providing file chunks' brief view and AST view (language specific), and formalizing the input/output structure using JSON. The language-specific functionality is built upon a language-agnostic base.",
        "type": "comment"
    },
    "242": {
        "file_id": 22,
        "content": "assert os.path.exists(source_dir)\nassert os.path.isdir(source_dir)\nassert os.path.isabs(source_dir)\nfrom collections import defaultdict\nimport json\nimport urllib.parse\nimport sys\nsys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), \"../\"))\nfrom llm import llm_context\nmetadata = json.loads(open(os.path.join(source_dir, \"metadata.json\"), \"r\").read())\nfile_mapping = metadata[\"file_mapping\"]\nsplit_count = metadata[\"split_count\"]\nproject_name = metadata[\"project_name\"]\ndata = {}\nfor i in range(split_count):\n    new_data = json.loads(open(os.path.join(source_dir, f\"data/{i}.json\"), \"r\").read())\n    data.update(new_data)\ndef strip_quote(s: str):\n    if s[0] == s[-1]:\n        if s[0] in ['\"', \"'\"]:\n            return s[1:-1].strip()\n    return s.strip()\n# read metadata.json & data/*.json\n# create and read some cache_tree.json, which you may want to include in .gitignore\n# produce tree.json\n# copy tree.html\nimport html.entities\nhtml5_escapes = html.entities.html5\nhtml_escape_mapping = {}\nfor k,v in html5_escapes.items():",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main_recursive.py:27-67"
    },
    "243": {
        "file_id": 22,
        "content": "Code reads metadata.json and data/*.json, creates cache_tree.json, produces tree.json, and copies tree.html. It also defines a strip_quote function to remove surrounding quotes from strings if present.",
        "type": "comment"
    },
    "244": {
        "file_id": 22,
        "content": "    if k.endswith(\";\"):  html_escape_mapping[v] = \"&\"+k\ndef html_escape(s: str):\n    ret = \"\"\n    for elem in s:\n        if elem in html_escape_mapping.keys():\n            ret += html_escape_mapping[elem]\n        else:\n            ret += elem\n    return ret\nimport hashlib\ndef hash_key(summary: str):\n    enc = summary.strip()\n    if enc:\n        # Generate a hash for the given summary\n        hash_object = hashlib.md5(enc.encode())\n        return hash_object.hexdigest()\nimport tinydb\ncache_tree = tinydb.TinyDB(os.path.join(source_dir, \"cache_tree.json\"))\ndef generate_file_summary_brief(filepath, summary):\n    # Generate a brief for the file based on its summary\n    stripped_summary = summary.strip()\n    if stripped_summary:\n        prompt = f\"\"\"\nFilepath: {filepath}\nSummary:\n{stripped_summary}\nBrief in 7 words (do not quote your brief, just write it out):\n\"\"\"\n        mhash = hash_key(prompt)\n        rec = cache_tree.get(\n            (tinydb.Query().hash == mhash) and (tinydb.Query().path == filepath)\n        )\n        if rec:",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main_recursive.py:68-112"
    },
    "245": {
        "file_id": 22,
        "content": "This code segment checks if a string ends with ';', creates an HTML-escaped version of the string, generates a hash for the summary, and generates a brief for the file based on its summary. It uses the TinyDB to store hashed summaries and prompts, retrieving previous records from cache if they exist. The function also performs HTML escaping for elements found in the keys of the html_escape_mapping dictionary.",
        "type": "comment"
    },
    "246": {
        "file_id": 22,
        "content": "            return rec[\"brief\"]\n        else:\n            init_prompt = \"\"\"You are a professional brief writer. You can turn long summaries into a single short brief within 7 words. You will be given a filepath, a summary of the file and produce a concise brief that best describes the file.\n\"\"\"\n            with llm_context(init_prompt) as model:\n                mbrief = strip_quote(model.run(prompt).strip())\n            mdoc = dict(path=filepath, hash=mhash, brief=mbrief)\n            cache_tree.upsert(mdoc, cond=tinydb.Query().path == filepath)\n            return mbrief\n    return \"\"\ndef generate_tree_repesentation(\n    directory_path: str,\n    childrens_mapping: dict[str, set[str]],\n    file_briefs: dict[str, str],\n    directory_briefs: dict[str, str],\n    indent=0,\n    briefs=[],\n):\n    childrens = list(childrens_mapping[directory_path])\n    childrens.sort(key=lambda x: x.lower())\n    if directory_path == \"/\":\n        name = project_name\n    else:\n        name = directory_path.strip(\"/\").split(\"/\")[-1]\n    mbrief, show = directory_briefs[directory_path]",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main_recursive.py:113-139"
    },
    "247": {
        "file_id": 22,
        "content": "The code recursively generates a tree representation of the directory structure and assigns brief descriptions to each file or directory. It retrieves file hashes, produces concise briefs using an LLM model, and sorts the directories alphabetically. The function takes in a directory path, children mappings, file and directory briefs, and an indent level as parameters.",
        "type": "comment"
    },
    "248": {
        "file_id": 22,
        "content": "    mbrief = strip_quote(mbrief)\n    briefs.append(\n        \" \" * indent * 4\n        + f'- <span hierarchy=\"{indent}\" class=\"expanded\" onclick=\"toggleVisibility(this)\" ><strong class=\"directory\" id=\"{directory_path}\"><code>{html_escape(name)}</code></strong>'\n        + (\"\" if not show else f\" <em>{mbrief}</em>\")\n        + \"</span>\"\n        # \" \" * indent * 4 + f\"- **`{name}`**\" + (\"\" if not show else f\" <em>{mbrief}</em>\")\n    )\n    for child in childrens:\n        child_name = child.strip(\"/\").split(\"/\")[-1]\n        if child.endswith(\"/\"):\n            # mbrief, show= directory_briefs[child]\n            # briefs.append(\n            #     \" \" * (indent + 1) * 4\n            #     + f\"- **`{child_name}`**\"+(\"\" if not show else f\" *{mbrief}*\")\n            # )\n            generate_tree_repesentation(\n                child,\n                childrens_mapping,\n                file_briefs,\n                directory_briefs,\n                indent + 1,\n                briefs,\n            )\n        else:\n            child_link = f\"index.html?q={urllib.parse.quote(child)}\"",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main_recursive.py:140-166"
    },
    "249": {
        "file_id": 22,
        "content": "This code is generating a tree representation of a file/directory hierarchy. It uses recursion to traverse the directory structure, adding each directory and its files as nodes in the tree. The 'briefs' list holds the HTML markup for each node, including folder names, optional brief descriptions, and expand/collapse buttons. It handles folders with nested directories by recursively calling itself, and includes a search functionality linking to individual files or directories.",
        "type": "comment"
    },
    "250": {
        "file_id": 22,
        "content": "            briefs.append(\n                \" \" * (indent + 1) * 4\n                + f'- <a class=\"file_link\" href=\"{child_link}\" id=\"{child}\"><code>{html_escape(child_name)}</code></a> <em>{strip_quote(file_briefs[child])}</em>'\n            )\n    return briefs\ndef comment_summarizer(summary_model, comments: list[str],directory_path:str) -> str:\n    def combine_comments(comment1: str, comment2: str):\n        summary_query = f\"\"\"\n{comment1}\n{comment2}\nBrief for directory '{directory_path}' in 7 words (do not quote your brief, just write it out):\n\"\"\"\n        ret = summary_model.run(summary_query)\n        return ret\n    def recursive_combine(comments_list: list[str]):\n        if len(comments_list) == 0:\n            raise Exception(\"No comments to combine\")\n        elif len(comments_list) == 1:\n            return comments_list[0]\n        elif len(comments_list) % 2 == 0:\n            combined = [\n                combine_comments(comments_list[i], comments_list[i + 1])\n                for i in range(0, len(comments_list), 2)",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main_recursive.py:167-197"
    },
    "251": {
        "file_id": 22,
        "content": "This code defines a function `comment_summarizer` that takes a summary model, list of comments, and a directory path as input. It uses the `combine_comments` function to recursively combine comments into a single brief for the given directory. The brief should be 7 words long and is generated using the provided summary model. The code also defines the `combine_comments` function which combines two comments into a single brief and generates a query for the summary model. If there are no comments or only one comment, it returns the comment(s) as is.",
        "type": "comment"
    },
    "252": {
        "file_id": 22,
        "content": "            ]\n        else:\n            combined = [\n                combine_comments(comments_list[i], comments_list[i + 1])\n                for i in range(0, len(comments_list) - 1, 2)\n            ]\n            combined += [comments_list[-1]]\n        return recursive_combine(combined)\n    summary = recursive_combine(comments)\n    del summary_model\n    return summary\ndef generate_directory_summary_brief(\n    directory_path,\n    childrens_mapping: dict[str, set[str]],\n    file_briefs: dict[str, str],\n    directory_briefs={},\n):\n    # Generate a brief for the directory based on its direct children's briefs\n    childrens = list(childrens_mapping[directory_path])\n    if len(childrens) == 0:\n        raise Exception(f\"Directory '{directory_path}' has no children\")\n    if len(childrens) == 1:\n        if childrens[0].endswith(\"/\"):\n            generate_directory_summary_brief(\n                childrens[0], childrens_mapping, file_briefs, directory_briefs\n            )\n            mbrief = directory_briefs[childrens[0]][0]",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main_recursive.py:198-227"
    },
    "253": {
        "file_id": 22,
        "content": "This code defines a function `generate_directory_summary_brief` that takes a directory path, childrens mapping, file briefs, and directory briefs as input. It generates a brief for the given directory based on its direct children's briefs. If the directory has no children or only one child which is another directory, it recursively calls itself to generate the directory's brief.",
        "type": "comment"
    },
    "254": {
        "file_id": 22,
        "content": "        else:\n            mbrief = file_briefs[childrens[0]]\n        directory_briefs[directory_path] = (mbrief, False)\n    else:\n        subprompt_parts = []\n        children_briefs = {}\n        for child in childrens:\n            if child.endswith(\"/\"):\n                generate_directory_summary_brief(\n                    child, childrens_mapping, file_briefs, directory_briefs\n                )\n                cbrief = directory_briefs[child][0]\n            else:\n                cbrief = file_briefs[child]\n            children_briefs[child] = cbrief\n        candidates = list(children_briefs.items())\n        candidates.sort(key=lambda x: x[0].lower())\n        for k, v in candidates:\n            if not k.endswith(\"/\"):\n                mark = \"file\"\n            else:\n                mark = \"directory\"\n            relpath = os.path.relpath(k, directory_path)\n            it = f\"Brief for {mark} '{relpath}': {v}\"\n            subprompt_parts.append(it)\n        subprompt = \"\\n\".join(subprompt_parts)\n        prompt = f\"\"\"",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main_recursive.py:228-254"
    },
    "255": {
        "file_id": 22,
        "content": "This code generates a directory summary brief and file briefs for a given path. If the path is a directory, it recursively calls itself to process its child directories and files. It then creates a prompt with brief details for each item in the path, indicating whether they are directories or files. The prompt is formatted using markdown.",
        "type": "comment"
    },
    "256": {
        "file_id": 22,
        "content": "{subprompt}\nBrief for directory '{directory_path}' in 7 words (do not quote your brief, just write it out):\n\"\"\"\n        mhash = hash_key(prompt)\n        rec = cache_tree.get(\n            (tinydb.Query().hash == mhash) and (tinydb.Query().path == directory_path)\n        )\n        if rec:\n            mbrief = rec[\"brief\"]\n        else:\n            # TODO: use recursive summarization.\n            init_prompt = \"\"\"You are a professional brief summarizer. You can produce a single short brief within 7 words. You will be given a pair of briefs and produce a concise brief that best describes the directory.\n\"\"\"\n            with llm_context(init_prompt) as model:\n                ret = comment_summarizer(model, subprompt_parts,directory_path)\n                mbrief = strip_quote(ret.strip())\n                # mbrief = strip_quote(model.run(prompt).strip())\n            mdoc = dict(path=directory_path, hash=mhash, brief=mbrief)\n            cache_tree.upsert(mdoc, cond=tinydb.Query().path == directory_path)\n        directory_briefs[directory_path] = (mbrief, True)",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main_recursive.py:255-276"
    },
    "257": {
        "file_id": 22,
        "content": "This code checks if a brief for the given directory exists in the cache. If it does, it retrieves the existing brief. If not, it initializes a summarizer model and generates a brief using recursive summarization. The newly generated brief is stored in the cache for future use.",
        "type": "comment"
    },
    "258": {
        "file_id": 22,
        "content": "    return directory_briefs\nfile_summaries = {\n    v[\"filepath\"]: data[str(v[\"entry_id\"] + 1)][\"content\"]\n    for v in file_mapping.values()\n}\n# print(file_summaries)\n# file_briefs = {k: generate_file_summary_brief(k, v) for k, v in file_summaries.items()}\nfile_briefs = {}\nitems_count = len(file_summaries.keys())\nprint(f\"\\n>>>> PROCESSING PROGRESS: 0/{items_count}\")\ncounter = 0\nfor k, v in file_summaries.items():\n    file_briefs[k] = generate_file_summary_brief(k, v)\n    counter += 1\n    print(f\"\\n>>>> PROCESSING PROGRESS: {counter}/{items_count}\")\nchildrens_mapping = defaultdict(set)\nfor k in file_summaries.keys():\n    print(k)\n    split_k = k.split(\"/\")\n    print(split_k)  # [dir1, dir2, ... filename]\n    # add \"/\" to the right and left of dir.\n    for i in range(len(split_k) - 1):\n        parent = \"/\".join(split_k[: i + 1]) + \"/\"\n        child = parent + split_k[i + 1]\n        if i != len(split_k) - 2:  # is directory:\n            child += \"/\"\n        print({\"i\": i, \"parent\": parent, \"child\": child, \"k\": k})\n        childrens_mapping[parent].add(child)",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main_recursive.py:277-309"
    },
    "259": {
        "file_id": 22,
        "content": "Code fetches file summaries, generates briefs for each file, and creates a children's mapping by iterating through the file summaries. It splits file paths into directories and filenames, adds slashes to create parent and child paths, and stores them in a mapping dictionary. This helps to understand the folder hierarchy.",
        "type": "comment"
    },
    "260": {
        "file_id": 22,
        "content": "# breakpoint()\ndirectory_briefs = generate_directory_summary_brief(\"/\", childrens_mapping, file_briefs)\n# now, let's generate the representation.\nbriefs = generate_tree_repesentation(\n    \"/\", childrens_mapping, file_briefs, directory_briefs\n)\n# briefs.insert(0,\"# Project Structure:\")\nbriefs.insert(\n    0,\n    f'## Project structure<span hierarchy=\"0\" class=\"partial-repository-url\"> of: {metadata[\"url\"][\"partial\"]}</span><div style=\"float: right;\"><a title=\"Document index\" style=\"margin:3.5px;\" href=\"index.html\"><i class=\"bi bi-search\"></i></a><a title=\"Feeling lucky\" style=\"margin:3.5px;\" id=\"feeling-lucky\" href=\"#\"><i class=\"bi bi-dice-3\"></i></a><a title=\"Expand tree\" style=\"margin:3.5px;\" href=\"tree.html?full=true\" id=\"expand-tree\"><i class=\"bi bi-caret-down-square\"></i></a></div>',\n)\nprint(\"=\" * 40)\nprint(\"\\n\".join(briefs))\n### building\n# render README.md into index.html\nimport markdown\nfrom jinja2 import Template\n# Markdown content\nmarkdown_content = \"\\n\".join(briefs)\n# Convert Markdown to HTML\nhtml_content = markdown.markdown(markdown_content)",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main_recursive.py:311-337"
    },
    "261": {
        "file_id": 22,
        "content": "Generating and printing the tree representation of project structure in HTML format for easier navigation.",
        "type": "comment"
    },
    "262": {
        "file_id": 22,
        "content": "template_path = os.path.join(os.path.abspath(os.path.dirname(__file__)), \"tree.html.j2\")\ncss_path = os.path.join(\n    os.path.abspath(os.path.dirname(__file__)), \"github-markdown.css\"\n)\ntemplate = Template(open(template_path, \"r\").read())\n# Render the template with the data\nrendered_template = template.render(content=html_content, project_name=metadata[\"url\"][\"partial\"])\nprint(\"Template rendered.\")\ntree_fname = \"tree.html\"\n# Write the template content to a file\nwith open(os.path.join(source_dir, tree_fname), \"w+\", encoding=\"utf-8\") as file:\n    file.write(rendered_template)\nimport shutil\nshutil.copy(css_path, source_dir)\nprint(\n    f\"Markdown converted to HTML and written to {os.path.join(source_dir, tree_fname)}\"\n)",
        "type": "code",
        "location": "/document_agi_computer_control/tree_markdown_view_folder_hierarchy/main_recursive.py:339-360"
    },
    "263": {
        "file_id": 22,
        "content": "This code renders a template with provided data, writes the rendered result to a file, and copies an external CSS file to the source directory.",
        "type": "comment"
    },
    "264": {
        "file_id": 23,
        "content": "/document_agi_computer_control/vectorstore_embedding_chat_rag/chat.py",
        "type": "filepath"
    },
    "265": {
        "file_id": 23,
        "content": "This code imports necessary modules and appends the directory of the current file to the system path, allowing it to access and interact with the \"main\" module for document-based chat functionality.",
        "type": "summary"
    },
    "266": {
        "file_id": 23,
        "content": "# chat with the document.\nimport sys, os\nsys.path.append(os.path.dirname(__file__))\nimport main",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/chat.py:1-5"
    },
    "267": {
        "file_id": 23,
        "content": "This code imports necessary modules and appends the directory of the current file to the system path, allowing it to access and interact with the \"main\" module for document-based chat functionality.",
        "type": "comment"
    },
    "268": {
        "file_id": 24,
        "content": "/document_agi_computer_control/vectorstore_embedding_chat_rag/docarray_test.py",
        "type": "filepath"
    },
    "269": {
        "file_id": 24,
        "content": "This code initializes a document index and performs text search for similar documents using HnswLibrary. It also includes functions for text hashing and creating TextDoc objects to represent document structure, searches for nearest neighbors with 10 result limit, and prints results/scores.",
        "type": "summary"
    },
    "270": {
        "file_id": 24,
        "content": "import os\nimport hashlib\nos.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\nfrom langchain.embeddings import OllamaEmbeddings\nollama_emb = OllamaEmbeddings(\n    model=\"openhermes2.5-mistral:latest\",\n    # model=\"llama:7b\",\n)\ndef hash_doc(enc: str):\n    hash_object = hashlib.md5(enc.encode())\n    return hash_object.hexdigest()\ncache_path = \"./docarray_cache\"\nfrom docarray import BaseDoc\nfrom docarray.index import HnswDocumentIndex\nimport numpy as np\nfrom docarray.typing import NdArray\nclass TextDoc(BaseDoc):\n    text: str\n    text_hash: str\n    embedding: NdArray[4096]\nimport rich\n# create a Document Index\nindex = HnswDocumentIndex[TextDoc](work_dir=cache_path)\n# index your data\ndocs = [\"hello again\", \"bye world\"]\nquery = \"hello world\"\n# find similar Documents\nfor it in docs:\n    docHash = hash_doc(it)\n    index._sqlite_cursor.execute(\n        \"SELECT text FROM docs WHERE text_hash = ?\", (docHash,)\n    )\n    rows = index._sqlite_cursor.fetchall()\n    if len(rows) > 0:\n        cached = False\n        for row in rows:",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/docarray_test.py:1-51"
    },
    "271": {
        "file_id": 24,
        "content": "This code imports necessary libraries and initializes a document index using an HnswDocumentIndex from the docarray library. It then indexes data, in this case, a list of strings, and allows for finding similar documents based on a query. The code also includes functions for hashing text and creating a TextDoc class representing the structure of each document.",
        "type": "comment"
    },
    "272": {
        "file_id": 24,
        "content": "            if row[0] == it:\n                cached = True\n                break\n        if cached:\n            print(\"document cached:\", it)\n            continue\n    # result = index.text_search(docHash, search_field=\"text_hash\", limit=1)\n    # if result.count == 1:\n    #     if result.documents[0].text_hash == docHash:\n    #         print(\"document cached:\", it)\n    #         continue\n    embed = np.array(ollama_emb.embed_query(it))\n    docObject = TextDoc(text=it, text_hash=docHash, embedding=embed)\n    index.index(docObject)\n# breakpoint()\nindex._sqlite_cursor.execute(\"SELECT doc_id FROM docs WHERE text LIKE 'hello%'\")\n# index._sqlite_cursor.execute(\"SELECT doc_id FROM docs\")\n# [(423764937781332251,), (955323081996155123,)]\n# index._sqlite_cursor.execute(\"SELECT * FROM docs\") # field: doc_id\nrows = index._sqlite_cursor.fetchall()\n# print(rows)\nhashed_ids = set(it[0] for it in rows)\n# hashed_ids = set(str(it[0]) for it in rows)\n# print(hashed_ids)\nans = index._search_and_filter(\n    np.array(ollama_emb.embed_query(query)).reshape(1, -1),",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/docarray_test.py:52-81"
    },
    "273": {
        "file_id": 24,
        "content": "The code is performing text search and indexing operations on a document store. It first checks if the document is cached, and if so, it continues without further processing. If not, it embeds the query and creates a TextDoc object with the embedded representation of the document's text. The document is then indexed in the document store using the provided index object. Next, the code executes SQLite queries to fetch doc_ids from the database. It converts these doc_ids into a set and then performs a search and filter operation on the index using an embedded representation of the query.",
        "type": "comment"
    },
    "274": {
        "file_id": 24,
        "content": "    limit=10,\n    search_field=\"embedding\",\n    hashed_ids=hashed_ids,\n)\nrich.print(\"ans:\", ans)\n# breakpoint()\n# hnswlib ids: [955323081996155123, 423764937781332251]\nresults, scores = index.find(\n    ollama_emb.embed_query(query), limit=10, search_field=\"embedding\"\n)\nrich.print(results, scores)",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/docarray_test.py:82-94"
    },
    "275": {
        "file_id": 24,
        "content": "This code is searching for the nearest neighbors using HnswLibrary in the index, passing an embedded query and limiting the results to 10. It then prints the results and scores obtained from the search. The breakpoint can be used for debugging purposes if needed.",
        "type": "comment"
    },
    "276": {
        "file_id": 25,
        "content": "/document_agi_computer_control/vectorstore_embedding_chat_rag/llamaindex_test.py",
        "type": "filepath"
    },
    "277": {
        "file_id": 25,
        "content": "The code imports necessary libraries, sets up a vector store index and embeddings model, initializes an OpenLM model, and constructs a VectorStoreIndex using either the vector store or docstore. The vector store index is used as a retriever to retrieve \"hello\" and print the answer in a rich format.",
        "type": "summary"
    },
    "278": {
        "file_id": 25,
        "content": "from llama_index import Document\nfrom llama_index.embeddings import OllamaEmbedding  # OpenAIEmbedding\nfrom llama_index.text_splitter import SentenceSplitter\nfrom llama_index.extractors import TitleExtractor\nfrom llama_index.ingestion import IngestionPipeline\nfrom llama_index.llms import Ollama\nfrom llama_index.storage.docstore import SimpleDocumentStore\nimport os\nos.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\nfrom llama_index import VectorStoreIndex, ServiceContext, LLMPredictor\nfrom llama_index.vector_stores import DocArrayHnswVectorStore\n# from llama_index.vector_stores import SimpleVectorStore\n# from llama_index.ingestion.cache import SimpleCache\nvector_store = DocArrayHnswVectorStore(\"storage\",dim = 4096)\n# create the pipeline with transformations\n# if os.path.exists(\"./storage\"):\n#     vector_store = SimpleVectorStore.from_persist_dir()\n# else:\n#     vector_store = SimpleVectorStore(stores_text=True)\n# vector_store.stores_text=True\nembed = OllamaEmbedding(model_name=\"openhermes2.5-mistral:latest\")",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/llamaindex_test.py:1-29"
    },
    "279": {
        "file_id": 25,
        "content": "This code imports necessary libraries and sets up a vector store for storing and retrieving documents using the llama_index package. The code utilizes DocArrayHnswVectorStore for efficient storage and retrieval, and also creates an OpenAIEmbedding model to generate embeddings for documents.",
        "type": "comment"
    },
    "280": {
        "file_id": 25,
        "content": "llm = Ollama(model=\"openhermes2.5-mistral:latest\")\npipeline = IngestionPipeline.construct(\n    transformations=[\n        SentenceSplitter(chunk_size=25, chunk_overlap=0),\n        TitleExtractor(llm=llm),\n        embed,\n    ],\n    docstore=SimpleDocumentStore(),  # do it if you want load/persist to work properly.\n    vector_store=vector_store,\n    validate_arguments=False,\n)\nloadpath = \"./pipeline_storage\"\nif os.path.exists(loadpath):\n    pipeline.load(persist_dir=loadpath)\n# run the pipeline\nnodes = pipeline.run(documents=[Document.example()])  # return newly added nodes.\nimport rich\n# rich.print(nodes)\n# rich.print(pipeline.documents)\n# rich.print(pipeline.docstore.docs)\npipeline.persist(persist_dir=loadpath)  # not persisting document.\nserv_cont = ServiceContext.from_defaults(\n    llm_predictor=LLMPredictor(llm),\n    embed_model=embed,\n)\n# vsindex = VectorStoreIndex.from_vector_store(vectore_store)\nvsindex = VectorStoreIndex.from_vector_store(vector_store, service_context=serv_cont)\n# vsindex = VectorStoreIndex.from_documents(pipeline.docstore.docs.values(),service_context=serv_cont)",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/llamaindex_test.py:30-60"
    },
    "281": {
        "file_id": 25,
        "content": "This code initializes an OpenLM model, creates a pipeline for text processing with transformations like sentence splitting and title extraction, checks if a load path exists and loads the saved pipeline if so. It then runs the pipeline on example documents, prints the resulting nodes, persists the pipeline, creates a service context using the loaded LLM and embed models, and finally constructs a VectorStoreIndex from either the vector store or the documents in the docstore.",
        "type": "comment"
    },
    "282": {
        "file_id": 25,
        "content": "# this will do RAG. However do you have qualified prompt?\n# engine = vsindex.as_query_engine()\nengine = vsindex.as_retriever()\nans = engine.retrieve(\"hello\")\n# ans = engine.query(\"something interesting in the document\")\nrich.print(ans)",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/llamaindex_test.py:62-68"
    },
    "283": {
        "file_id": 25,
        "content": "This code initializes the vector store index (vsindex) as a retriever rather than a query engine, retrieves \"hello\" using the retriever, and prints the answer in a rich format. A qualified prompt is missing to enhance the functionality of the query or retrieve function.",
        "type": "comment"
    },
    "284": {
        "file_id": 26,
        "content": "/document_agi_computer_control/vectorstore_embedding_chat_rag/llamaindex_vector.py",
        "type": "filepath"
    },
    "285": {
        "file_id": 26,
        "content": "The code imports necessary modules and defines the embedding and language model. It then creates a service context with the defined LLM predictor and embed model. Documents are created and assigned to the variable 'documents'. A VectorStoreIndex is constructed from the documents using the previously defined service context, storing the index in the variable 'index'.",
        "type": "summary"
    },
    "286": {
        "file_id": 26,
        "content": "from llama_index import VectorStoreIndex, LLMPredictor\nfrom llama_index import Document\nfrom llama_index.llms import Ollama\nfrom llama_index.embeddings import OllamaEmbedding\nfrom llama_index import ServiceContext\nembed = OllamaEmbedding(model_name=\"openhermes2.5-mistral:latest\")\nllm = Ollama(model=\"openhermes2.5-mistral:latest\")\nserv_cont = ServiceContext.from_defaults(\n    llm_predictor=LLMPredictor(llm),\n    embed_model=embed,\n)\ndocuments = [Document.example()]\n# print(documents)\nindex = VectorStoreIndex.from_documents(documents, service_context=serv_cont)",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/llamaindex_vector.py:1-16"
    },
    "287": {
        "file_id": 26,
        "content": "The code imports necessary modules and defines the embedding and language model. It then creates a service context with the defined LLM predictor and embed model. Documents are created and assigned to the variable 'documents'. A VectorStoreIndex is constructed from the documents using the previously defined service context, storing the index in the variable 'index'.",
        "type": "comment"
    },
    "288": {
        "file_id": 27,
        "content": "/document_agi_computer_control/vectorstore_embedding_chat_rag/main.py",
        "type": "filepath"
    },
    "289": {
        "file_id": 27,
        "content": "The code imports modules, defines classes for queries and prompts, generates data processing prompts, assembles questions and answers, creates a formatted document, and handles chat history. Else block generates chat history summary and condense prompts with new user question and context.",
        "type": "summary"
    },
    "290": {
        "file_id": 27,
        "content": "import sys, os\nsys.path.append(os.path.dirname(__file__))\nimport prompts as P\nimport vectorindex as V\nfrom pydantic import BaseModel\nclass ReaderQuestions(BaseModel):\n    questions: list[str]\nclass ContextQueries(BaseModel):\n    queries: list[str]\nprompt = P.generateFileSummaryContextQueriesPrompt(\n    schema, contentType, projectName, filePath, summary\n)\nres = ...\ncontextQueries = ContextQueries.parse_raw(res)\ncontextQAPairs = []\nfor q in contextQueries:\n    context = ...\n    contextQAPairs.append((q, context))\nprompt = P.generateFileSummaryPrompt(\n    contentType, projectName, filePath, summary, contextQAPairs\n)\ndocumentSummary = ...  # file summary\nprompt = P.generateFileQuestionsPrompt(\n    schema, contentType, projectName, filePath, summary\n)\nres = ...\nquestions = ReaderQuestions.parse_raw(res)\nprint(questions)\nQAPairs = []\nfor q in questions:\n    context = ...\n    prompt = P.generateFileAnswerPrompt(\n        contentType,\n        projectNAme,\n        filePath,\n        summary,\n        q,\n        context,\n    )\n    res = ...",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/main.py:1-52"
    },
    "291": {
        "file_id": 27,
        "content": "This code imports necessary modules, defines classes for reader questions and context queries, generates prompts for generating file summary context queries, parses raw data into objects, appends context-question pairs to a list, generates prompts for file summary and file questions, and finally prints the reader's questions.",
        "type": "comment"
    },
    "292": {
        "file_id": 27,
        "content": "    # assemble questions and answers.\n    QAPairs.append(q, res)\n# assemble document\ndocumentQA = [\"## Questions:\\n\"] + [\n    f\"{i+1}. {q}\\n\\n{a}\\n\" for i, (q, a) in enumerate(QAPairs)\n]\ndocumentQA = \"\\n\".join(documentQA)\ndocument = f\"{documentSummary}\\n---\\n{documentQA}\"\n################################\nprompt = P.generateFolderSummaryContextQueriesPrompt(\n    schema,\n    contextType,\n    projectName,\n    folderPath,\n    summary,\n)\nres = ...\nqueries = ContextQueries.parse_raw(res)\ncontextQAPairs = []\nfor q in queries:\n    context = ...\n    contextQAPairs.append((q, context))\nprompt = P.generateFolderSummaryPrompt(\n    contentType, projectName, filePath, summary, contextQAPairs\n)\ndocument = ...\n################################\nlast_chat_history = None\ncontext = ...  # generate from first question\nprompt = P.generateQAPrompt(contentType, projectName, question, context)\nres = ...\nprompt = P.generateRecentChatHistorySummaryPrompts(question, res)\nrecent_chat_history = ...\nif last_chat_history is None:\n    chat_history = recent_chat_history",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/main.py:53-98"
    },
    "293": {
        "file_id": 27,
        "content": "The code assembles questions and answers into pairs, creates a document with the formatted questions and answers, generates a prompt for a folder summary context queries, and then generates prompts for QA and recent chat history. If there is no last chat history, it assigns the recent chat history to the current one.",
        "type": "comment"
    },
    "294": {
        "file_id": 27,
        "content": "else:\n    prompt = P.generateChatHistorySummaryPrompt(last_chat_history, recent_chat_history)\n    chat_history = ...\nlast_chat_history = chat_history\nquestion = ...  # new question from user\nprompt = P.generateCondensePrompt(chat_history, question)\nstandalone_question = ...\ncontext = ...",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/main.py:99-108"
    },
    "295": {
        "file_id": 27,
        "content": "Else block generates a chat history summary prompt and updates last_chat_history. It then generates a condense prompt using the updated chat_history and a new user question, while also creating standalone_question and context.",
        "type": "comment"
    },
    "296": {
        "file_id": 28,
        "content": "/document_agi_computer_control/vectorstore_embedding_chat_rag/prompts.py",
        "type": "filepath"
    },
    "297": {
        "file_id": 28,
        "content": "This code generates prompts for creating questions and answers about file content using Pydantic schema, JSON format, and markdown. It includes functions to gather context for software project-related questions and returns previously defined prompts.",
        "type": "summary"
    },
    "298": {
        "file_id": 28,
        "content": "FILE_RETRIEVER_OBJECTIVE_PROMPT = \"\"\"Write a detailed technical explanation of what this code does.\nFocus on the high-level purpose of the code and how it may be used in the larger project.\n\"\"\"\nFOLDER_RETRIEVER_OBJECTIVE_PROMPT = \"\"\"Write a technical explanation of what the code in this file does and how it might fit into the larger project or work with other parts of the project.\nGive examples of how this code might be used. Include code examples where appropriate.\n\"\"\"\nFILE_PROMPT = (\n    f\"\"\"\n{FILE_RETRIEVER_OBJECTIVE_PROMPT.strip()}\nInclude code examples where appropriate. Keep you response between 100 and 300 words.\nDO NOT RETURN MORE THAN 300 WORDS.\nOutput should be in markdown format.\nDo not just list the methods and classes in this file.\n\"\"\",\n)\nFOLDER_PROMPT = (\n    f\"\"\"\n{FOLDER_RETRIEVER_OBJECTIVE_PROMPT.strip()}\nBe concise. Include any information that may be relevant to a developer who is curious about this code.\nKeep you response under 400 words. Output should be in markdown format.\nDo not just list the files and folders in this folder.",
        "type": "code",
        "location": "/document_agi_computer_control/vectorstore_embedding_chat_rag/prompts.py:1-23"
    },
    "299": {
        "file_id": 28,
        "content": "This code contains prompts for generating technical explanations of the code and its usage in the larger project. The FILE_PROMPT provides a detailed objective prompt with an upper limit of 300 words, while the FOLDER_PROMPT offers concise information under 400 words about how the code fits into the project. The output should be in markdown format and tailored to developers who may be curious about the code.",
        "type": "comment"
    }
}